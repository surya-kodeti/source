{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"create-shared-react-context_docs/create-shared-react-context/","title":"Index","text":"<p>A memoized wrapper around React.createContext. Used by One App as a dependency to pass React context between Holocron modules without external usage.</p> <p> </p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#hiring","title":"\ud83d\udc69\u200d\ud83d\udcbb Hiring \ud83d\udc68\u200d\ud83d\udcbb","text":"<p>Want to get paid for your contributions to <code>create-shared-react-context</code>?</p> <p>Send your resume to oneamex.careers@aexp.com</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#table-of-contents","title":"\ud83d\udcd6 Table of Contents","text":"<ul> <li>Features</li> <li>Usage</li> <li>API</li> <li>Git Hooks</li> <li>Contributing</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#features","title":"\u2728 Features","text":"<ul> <li>Memoized wrapper around React.createContext</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#usage","title":"\ud83e\udd39\u200d Usage","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/#installing","title":"Installing","text":"<pre><code>$ npm install --save create-shared-react-context\n</code></pre>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#example","title":"Example","text":"<pre><code>import createSharedReactContext from 'create-shared-react-context';\n\nconst context = createSharedReactContext({}, 'someIdentifier');\n</code></pre>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#api","title":"\ud83c\udf9b\ufe0f API","text":"<p><code>createSharedReactContext</code> takes two arguments: <code>defaultValue</code> and <code>key</code>.</p> <p><code>defaultValue</code> is the same default value that would be used in <code>createContext</code>.</p> <p><code>key</code> is the identifier used to ensure that any subsequent call of <code>createSharedReactContext</code> will return any previous created context with the same identifier.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#motivation","title":"Motivation","text":"<p>In a modular application, sharing functionality between bundled modules with their own dependencies can be done in a myriad of ways. If one wanted to be able to pass functionality using React  Context, but each module has their own instance of the dependency creating that context, the Provider used at the top would fail to pass context down to the Consumer, as the reference would be different. You could declare that dependency as an external, so that the reference used by each module would be shared, but sometimes a dependency may not require a separate instance to function.</p> <p></p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#git-hooks","title":"\ud83c\udfa3 Git Hooks","text":"<p>These commands will be automatically run during normal git operations like committing code.</p> <p><code>pre-commit</code></p> <p>This hook runs <code>npm test</code> before allowing a commit to be checked in.</p> <p><code>commit-msg</code></p> <p>This hook verifies that your commit message matches the One Amex conventions. See the commit message section in the contribution guidelines.</p> <p></p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#contributing","title":"\ud83c\udfc6 Contributing","text":"<p>See contributing guidelines</p> <p></p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#license","title":"\ud83d\udddd\ufe0f License","text":"<p>Any contributions made under this project will be governed by the Apache License 2.0.</p> <p></p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/#code-of-conduct","title":"\ud83d\udde3\ufe0f Code of Conduct","text":"<p>This project adheres to the American Express Community Guidelines. By participating, you are expected to honor these guidelines.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/","title":"CHANGELOG","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#111-2025-01-15","title":"1.1.1 (2025-01-15)","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>interop: fix export and use commonJS (#53) (1777b68)</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#110-2025-01-13","title":"1.1.0 (2025-01-13)","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#features","title":"Features","text":"<ul> <li>typescript: convert to typescript (967de32)</li> <li>typescript: convert to typescript (3721d02)</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#105-2022-04-05","title":"1.0.5 (2022-04-05)","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>labeler: fix globing (882f0d9)</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#104-2021-12-01","title":"1.0.4 (2021-12-01)","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>peerDep: less restrictive peerDep version for react (b3b3b86)</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#102-2020-05-04","title":"1.0.2 (2020-05-04)","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>index: refactor destructuring to ensure IE11 usage (898d27f)</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CHANGELOG/#reverts","title":"Reverts","text":"<ul> <li>Revert \"chore(release): 1.0.2 revert\" (ec186e2)</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/","title":"CODE OF CONDUCT","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/#american-express-open-source-community-guidelines","title":"American Express Open Source Community Guidelines","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/#last-modified-january-29-2016","title":"Last Modified: January 29, 2016","text":"<p>Welcome to the American Express Open Source Community on GitHub! These American Express Community Guidelines outline our expectations for Github participating members within the American Express community, as well as steps for reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all and expect our community Guidelines to be honored.</p> <p>IMPORTANT REMINDER:</p> <p>When you visit American Express on any third party sites such as GitHub your activity there is subject to that site\u2019s then current terms of use., along with their privacy and data security practices and policies. The Github platform is not affiliated with us and may have practices and policies that are different than are our own. Please note, American Express is not responsible for, and does not control, the GitHub site\u2019s terms of use, privacy and data security practices and policies. You should, therefore, always exercise caution when posting, sharing or otherwise taking any action on that site and, of course, on the Internet in general. Our open source community strives to: - Be friendly and patient. - Be welcoming: We strive to be a community that welcomes and supports people of all backgrounds and identities. This includes, but is not limited to members of any race, ethnicity, culture, national origin, color, immigration status, social and economic class, educational level, sex, sexual orientation, gender identity and expression, age, size, family status, political belief, religion, and mental and physical ability. - Be considerate: Your work will be used by other people, and you in turn will depend on the work of others. Any decision you take will affect users and colleagues, and you should take those consequences into account when making decisions. Remember that we're a world-wide community, so you might not be communicating in someone else's primary language. - Be respectful: Not all of us will agree all the time, but disagreement is no excuse for poor behavior and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It\u2019s important to remember that a community where people feel uncomfortable or threatened is not a productive one. - Be careful in the words that we choose: We are a community of professionals, and we conduct ourselves professionally. Be kind to others. Do not insult or put down other participants. Harassment and other exclusionary behavior aren't acceptable. - Try to understand why we disagree: Disagreements, both social and technical, happen all the time. It is important that we resolve disagreements and differing views constructively. Remember that we\u2019re all different people. The strength of our community comes from its diversity, people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn\u2019t mean that they\u2019re wrong. Don\u2019t forget that it is human to err and blaming each other doesn\u2019t get us anywhere. Instead, focus on helping to resolve issues and learning from mistakes.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/#definitions","title":"Definitions","text":"<p>Harassment includes, but is not limited to: - Offensive comments related to gender, gender identity and expression, sexual orientation, disability, mental illness, neuro(a)typicality, physical appearance, body size, race, age, regional discrimination, political or religious affiliation - Unwelcome comments regarding a person\u2019s lifestyle choices and practices, including those related to food, health, parenting, drugs, and employment - Deliberate misgendering. This includes deadnaming or persistently using a pronoun that does not correctly reflect a person's gender identity. You must address people by the name they give you when not addressing them by their username or handle - Physical contact and simulated physical contact (eg, textual descriptions like \u201chug\u201d or \u201cbackrub\u201d) without consent or after a request to stop - Threats of violence, both physical and psychological - Incitement of violence towards any individual, including encouraging a person to commit suicide or to engage in self-harm - Deliberate intimidation - Stalking or following - Harassing photography or recording, including logging online activity for harassment purposes - Sustained disruption of discussion - Unwelcome sexual attention, including gratuitous or off-topic sexual images or behaviour - Pattern of inappropriate social contact, such as requesting/assuming inappropriate levels of intimacy with others - Continued one-on-one communication after requests to cease - Deliberate \u201couting\u201d of any aspect of a person\u2019s identity without their consent except as necessary to protect others from intentional abuse - Publication of non-harassing private communication</p> <p>Our open source community prioritizes marginalized people\u2019s safety over privileged people\u2019s comfort. We will not act on complaints regarding: - \u2018Reverse\u2019 -isms, including \u2018reverse racism,\u2019 \u2018reverse sexism,\u2019 and \u2018cisphobia\u2019 - Reasonable communication of boundaries, such as \u201cleave me alone,\u201d \u201cgo away,\u201d or \u201cI\u2019m not discussing this with you\u201d - Refusal to explain or debate social justice concepts - Communicating in a \u2018tone\u2019 you don\u2019t find congenial - Criticizing racist, sexist, cissexist, or otherwise oppressive behavior or assumptions</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/#diversity-statement","title":"Diversity Statement","text":"<p>We encourage everyone to participate and are committed to building a community for all. Although we will fail at times, we seek to treat everyone both as fairly and equally as possible. Whenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, it is our responsibility to listen carefully and respectfully, and do our best to right the wrong.</p> <p>Although this list cannot be exhaustive, we explicitly honor diversity in age, gender, gender identity or expression, culture, ethnicity, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/#reporting-issues","title":"Reporting Issues","text":"<p>If you experience or witness unacceptable behavior\u2014or have any other concerns\u2014please report it by contacting us at opensource@aexp.com. All reports will be handled with discretion. In your report please include: - Your contact information. - Names (real, nicknames, or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g. a mailing list archive or a public IRC logger), please include a link. - Any additional information that may be helpful.</p> <p>After filing a report, a representative of our community will contact you personally, review the incident, follow up with any additional questions, and make a decision as to how to respond. If the person who is harassing you is part of the response team, they will recuse themselves from handling your incident. If the complaint originates from a member of the response team, it will be handled by a different member of the response team. We will respect confidentiality requests for the purpose of protecting victims of abuse.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/#removal-of-posts","title":"Removal of Posts","text":"<p>We will not review every comment or post, but we reserve the right to remove any that violates these Guidelines or that, in our sole discretion, we otherwise consider objectionable and we may ban offenders from our community.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/#suspensionterminationreporting-to-authority","title":"Suspension/Termination/Reporting to Authority","text":"<p>In certain instances, we may suspend, terminate or ban certain repeat offenders and/or those committing significant violations of these Guidelines. When appropriate, we may also, on our own or as required by the GitHub terms of use, be required to refer and/or work with GitHub and/or the appropriate authorities to review and/or pursue certain violations.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CODE_OF_CONDUCT/#attribution-acknowledgements","title":"Attribution &amp; Acknowledgements","text":"<p>These Guidelines have been adapted from the Code of Conduct of the TODO group. They are subject to revision by American Express and may be revised from time to time.</p> <p>Thank you for your participation!</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/","title":"Contributing to create-shared-react-context","text":"<p>\u2728 Thank you for taking the time to contribute to this project \u2728</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#table-of-contents","title":"\ud83d\udcd6 Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Developing</li> <li>Submitting a new feature</li> <li>Reporting bugs</li> <li>Contributing</li> <li>Coding conventions</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to the American Express Code of Conduct. By contributing, you are expected to honor these guidelines.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#developing","title":"Developing","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#installation","title":"Installation","text":"<ol> <li>Fork the repository <code>create-shared-react-context</code> to your GitHub account.</li> <li> <p>Afterwards run the following commands in your terminal</p> <pre><code>$ git clone https://github.com/&lt;your-github-username&gt;/create-shared-react-context\n$ cd create-shared-react-context\n</code></pre> </li> </ol> <p>replace <code>your-github-username</code> with your github username</p> <ol> <li> <p>Install the dependencies by running</p> <pre><code>$ npm install\n</code></pre> </li> <li> <p>You can now run any of these scripts from the root folder.</p> </li> </ol>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#running-tests","title":"Running tests","text":"<ul> <li><code>npm run lint</code></li> </ul> <p>Verifies that your code matches the American Express code style defined in <code>eslint-config-amex</code>.</p> <ul> <li><code>npm test</code></li> </ul> <p>Runs unit tests and verifies the format of all commit messages on the current branch.</p> <ul> <li><code>npm posttest</code></li> </ul> <p>Runs linting on the current branch, checks that the commits follow conventional commits and verifies that the <code>package-lock.json</code> file includes public NPM registry URLs.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#submitting-a-new-feature","title":"Submitting a new feature","text":"<p>When submitting a new feature request or enhancement of an existing feature please review the following:</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#is-your-feature-request-related-to-a-problem","title":"Is your feature request related to a problem","text":"<p>Please provide a clear and concise description of what you want and what your use case is.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#provide-an-example","title":"Provide an example","text":"<p>Please include a snippets of the code of the new feature.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#describe-the-suggested-enhancement","title":"Describe the suggested enhancement","text":"<p>A clear and concise description of the enhancement to be added include a step-by-step guide if applicable. Add any other context or screenshots or animated GIFs about the feature request</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#describe-alternatives-youve-considered","title":"Describe alternatives you've considered","text":"<p>A clear and concise description of any alternative solutions or features you've considered.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#reporting-bugs","title":"Reporting bugs","text":"<p>All issues are submitted within GitHub issues. Please check this before submitting a new issue.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#describe-the-bug","title":"Describe the bug","text":"<p>A clear and concise description of what the bug is.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#provide-step-by-step-guide-on-how-to-reproduce-the-bug","title":"Provide step-by-step guide on how to reproduce the bug","text":"<p>Steps to reproduce the behavior, please provide code snippets or a link to repository</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#expected-behavior","title":"Expected behavior","text":"<p>Please provide a description of the expected behavior</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#screenshots","title":"Screenshots","text":"<p>If applicable, add screenshots or animated GIFs to help explain your problem.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#system-information","title":"System information","text":"<p>Provide the system information which is not limited to the below:</p> <ul> <li>OS: [e.g. macOS, Windows]</li> <li>Browser (if applies) [e.g. chrome, safari]</li> <li>Version of create-shared-react-context: [e.g. 5.0.0]</li> <li>Node version:[e.g 10.15.1]</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#security-bugs","title":"Security Bugs","text":"<p>Please review our Security Policy. Please follow the instructions outlined in the policy.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#getting-in-contact","title":"Getting in contact","text":"<ul> <li>Join our Slack channel request an invite here</li> </ul>"},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#coding-conventions","title":"Coding conventions","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/CONTRIBUTING/#git-commit-guidelines","title":"Git Commit Guidelines","text":"<p>We follow conventional commits for git commit message formatting. These rules make it easier to review commit logs and improve contextual understanding of code changes. This also allows us to auto-generate the CHANGELOG from commit messages.</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/SECURITY/","title":"SECURITY","text":""},{"location":"create-shared-react-context_docs/create-shared-react-context/SECURITY/#american-express-responsible-disclosure-policy","title":"American Express Responsible Disclosure Policy","text":"<p>At American Express, we take cybersecurity seriously and value the contributions of the security community at large. The responsible disclosure of potential issues helps us ensure the security and privacy of our customers and data.  If you believe you\u2019ve found a security issue in one of our products or services please send it to us and include the following details with your report: - A description of the issue and where it is located. - A description of the steps required to reproduce the issue.</p> <p>Please note that this should not be construed as encouragement or permission to perform any of the following activities: - Hack, penetrate, or otherwise attempt to gain unauthorized access to American Express applications, systems, or data in violation of applicable law; - Download, copy, disclose or use any proprietary or confidential American Express data, including customer data; and - Adversely impact American Express or the operation of American Express applications or systems.</p> <p>American Express does not waive any rights or claims with respect to such activities.</p> <p>Please email your message and any attachments to  responsible.disclosure@aexp.com</p>"},{"location":"create-shared-react-context_docs/create-shared-react-context/SECURITY/#thank-you-for-helping-us-keep-american-express-customers-and-data-safe","title":"Thank you for helping us keep American Express customers and data safe.","text":""},{"location":"docsify_docs/docsify/","title":"Index","text":"<p>   A magical documentation site generator. </p> <p> </p> <p>Gold Sponsor via Open Collective</p> <p> </p> <p>Docsify turns one or more Markdown files into a Website, with no build process required.</p>"},{"location":"docsify_docs/docsify/#features","title":"Features","text":"<ul> <li>No statically built html files</li> <li>Simple and lightweight</li> <li>Smart full-text search plugin</li> <li>Multiple themes</li> <li>Useful plugin API</li> <li>Emoji support</li> </ul>"},{"location":"docsify_docs/docsify/#quick-start","title":"Quick Start","text":"<p>Get going fast by using a static web server or GitHub Pages with this ready-to-use Docsify Template, review the quick start tutorial or jump right into a CodeSandbox example site with the button below.</p> <p></p>"},{"location":"docsify_docs/docsify/#showcase","title":"Showcase","text":"<p>A large collection of showcase projects are included in awesome-docsify.</p>"},{"location":"docsify_docs/docsify/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Docsify CLI (Command Line Interface)</li> <li>CDN: UNPKG | jsDelivr | cdnjs</li> <li><code>develop</code> branch preview</li> <li>Awesome docsify</li> <li>Community chat</li> </ul>"},{"location":"docsify_docs/docsify/#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md.</p>"},{"location":"docsify_docs/docsify/#backers","title":"Backers","text":"<p>Thank you to all our backers! \ud83d\ude4f [Become a backer]</p> <p></p>"},{"location":"docsify_docs/docsify/#sponsors","title":"Sponsors","text":"<p>Thank you for supporting this project! \u2764\ufe0f [Become a sponsor]</p> <p></p>"},{"location":"docsify_docs/docsify/#contributors","title":"Contributors","text":"<p>This project exists thanks to all the people who contribute. [Contribute]. </p>"},{"location":"docsify_docs/docsify/#license","title":"License","text":"<p>MIT</p>"},{"location":"docsify_docs/docsify/#special-thanks","title":"Special Thanks","text":"<p>A preview of Docsify's PR and develop branch is Powered by </p>"},{"location":"docsify_docs/docsify/CHANGELOG/","title":"4.13.0 (2022-10-26)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>cornerExternalLinkTarget config. (#1814) (54cc5f9)</li> <li>correctly fix missing +1, -1 (#1722) (719dcbe)</li> <li>Coverpage when content &gt; viewport height (#1744) (301b516), closes #381</li> <li>filter null node first. (#1909) (d27af3d)</li> <li>fix docsify-ignore in search title. (#1872) (9832805)</li> <li>fix search with no content. (#1878) (9b74744)</li> <li>Ignore emoji shorthand codes in URIs (#1847) (3c9b3d9), closes #1823</li> <li>Legacy bugs (styles, site plugin error, and dev server error) (#1743) (fa6df6d)</li> <li>package.json &amp; package-lock.json to reduce vulnerabilities (#1756) (2dc5b12)</li> <li>packages/docsify-server-renderer/package.json &amp; packages/docsify-server-renderer/package-lock.json to reduce vulnerabilities (#1715) (c1227b2)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features","title":"Features","text":"<ul> <li>Emoji build (#1766) (ba5ee26)</li> <li>Native emoji w/ image-based fallbacks and improved parsing (#1746) (35002c9), closes #779</li> <li>Plugin error handling (#1742) (63b2535)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4122-2022-01-06","title":"4.12.2 (2022-01-06)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Add escapeHtml for search (#1551) (c24f7f6)</li> <li>allow also \" inside of an embed (ec16e4a)</li> <li>buble theme missing generic fallback font (#1568) (37d9f0e)</li> <li>Cannot read property 'classList' of null (#1527) (d6df2b8), closes /github.com/docsifyjs/docsify/pull/1527#issuecomment-793455105</li> <li>Cannot read property 'tagName' of null (#1655) (c3cdadc), closes #1154 /github.com/docsifyjs/docsify/blob/develop/src/core/router/history/html5.js#L25-L27 /github.com/docsifyjs/docsify/blob/develop/src/core/router/history/hash.js#L47-L49</li> <li>upgrade debug from 4.3.2 to 4.3.3 (#1692) (40e7749)</li> <li>Upgrade docsify from 4.12.0 to 4.12.1 (#1544) (d607f6d)</li> <li>upgrade dompurify from 2.2.6 to 2.2.7 (#1552) (407e4d4)</li> <li>Upgrade dompurify from 2.2.6 to 2.2.7 (#1553) (93c48f3)</li> <li>upgrade dompurify from 2.2.7 to 2.2.8 (#1577) (0dd44cc)</li> <li>upgrade dompurify from 2.2.7 to 2.3.0 (#1619) (66303fe)</li> <li>upgrade dompurify from 2.2.8 to 2.2.9 (#1600) (baf5a8a)</li> <li>upgrade dompurify from 2.2.9 to 2.3.0 (#1616) (b07fa3c)</li> <li>upgrade dompurify from 2.3.0 to 2.3.1 (#1635) (5ac8237)</li> <li>upgrade dompurify from 2.3.1 to 2.3.2 (#1647) (ff6acfa)</li> <li>upgrade node-fetch from 2.6.1 to 2.6.2 (#1641) (6ee1c14)</li> <li>upgrade node-fetch from 2.6.2 to 2.6.4 (#1649) (6f81034)</li> <li>upgrade node-fetch from 2.6.4 to 2.6.5 (#1654) (d16e657)</li> <li>upgrade node-fetch from 2.6.5 to 2.6.6 (#1668) (cefe3f8)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4121-2021-03-07","title":"4.12.1 (2021-03-07)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>isExternal check with malformed URL + tests (#1510) (ff2a66f), closes #1477 #1126 #1489</li> <li>Replace ES6 usage for IE11 compatibility (#1500) (a0f61b2)</li> <li>theme switcher in IE11 (#1502) (8cda078)</li> <li>Upgrade docsify from 4.11.6 to 4.12.0 (#1518) (47cd86c)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_1","title":"Features","text":"<ul> <li>Support search when there is no title (#1519) (bc37268)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#chore","title":"Chore","text":"<ul> <li>Fix missing carbon (#1501)</li> <li>Change Gitter to Discord throughout project (#1507)</li> <li>Add test cases on isExternal (#1515)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4120-2021-02-08","title":"4.12.0 (2021-02-08)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>add missing argument for highlighting code (#1365) (f35bf99)</li> <li>Can't search homepage content (#1391) (25bc9b7)</li> <li>Cannot read property 'startsWith' of undefined (#1358) (9351729)</li> <li>Cannot read property level of undefined (#1357) (4807e58)</li> <li>cannot search list content (#1361) (8d17dcb)</li> <li>duplicate search content when <code>/README</code> or <code>/</code> exists in the sidebar (#1403) (7c3bf98)</li> <li>package.json &amp; package-lock.json to reduce vulnerabilities (#1419) (69b6907)</li> <li>packages/docsify-server-renderer/package.json &amp; packages/docsify-server-renderer/package-lock.json to reduce vulnerabilities (#1389) (62cd35e)</li> <li>packages/docsify-server-renderer/package.json &amp; packages/docsify-server-renderer/package-lock.json to reduce vulnerabilities (#1418) (58fbca0)</li> <li>Prevent loading remote content via URL hash (#1489) (14ce7f3), closes #1477 #1126</li> <li>search on homepage test (#1398) (ee550d0)</li> <li>search titles containing ignored characters (#1395) (a2ebb21)</li> <li>sidebar active class and expand don't work as expect when use \"space\" in markdown filename (#1454) (dcf5a64), closes #1032</li> <li>sidebar horizontal scroll bar (#1362) (b480822)</li> <li>sidebar title error (#1360) (2100fc3)</li> <li>slugs are still broken when headings contain html (#1443) (76c5e68)</li> <li>the sidebar links to another site. (#1336) (c9d4f7a)</li> <li>title error when sidebar link exists with html tag (#1404) (8ccc202), closes #1408</li> <li>Unable to navigate on server without default index support (#1372) (759ffac)</li> <li>upgrade debug from 4.1.1 to 4.3.0 (#1390) (ae45b32)</li> <li>upgrade debug from 4.3.0 to 4.3.1 (#1446) (bc3350f)</li> <li>upgrade debug from 4.3.1 to 4.3.2 (#1463) (df21153)</li> <li>upgrade docsify from 4.11.4 to 4.11.6 (#1373) (c2d12ed)</li> <li>upgrade dompurify from 2.0.17 to 2.1.0 (#1397) (1863d8e)</li> <li>upgrade dompurify from 2.1.0 to 2.1.1 (#1402) (8cf9fd8)</li> <li>upgrade dompurify from 2.2.2 to 2.2.3 (#1457) (720d909)</li> <li>upgrade dompurify from 2.2.2 to 2.2.6 (#1483) (eee9507)</li> <li>upgrade dompurify from 2.2.3 to 2.2.6 (#1482) (7adad57)</li> <li>upgrade marked from 1.2.4 to 1.2.9 (#1486) (716a7fa)</li> <li>upgrade prismjs from 1.21.0 to 1.22.0 (#1415) (0806f48)</li> <li>upgrade prismjs from 1.22.0 to 1.23.0 (#1481) (5f29cde)</li> <li>Use legacy-compatible methods for IE11 (#1495) (06cbebf)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_2","title":"Features","text":"<ul> <li>search ignore diacritical marks (#1434) (8968a74)</li> <li>Add Jest + Playwright Testing (#1276)</li> <li>Add Vue components, mount options, global options, and v3 support (#1409)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4116-2020-08-22","title":"4.11.6 (2020-08-22)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>Add patch for {docsify-ignore} and {docsify-ignore-all} (ce31607)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4115-2020-08-21","title":"4.11.5 (2020-08-21)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>Russian language link error (#1270) (2a52460)</li> <li>{docsify-updated} in the sample code is parsed into time (#1321) (2048610)</li> <li>Add error handling for missing dependencies (fixes #1210) (#1232) (3673001)</li> <li>after setting the background image, the button is obscured (#1234) (34d918f)</li> <li>convert {docsify-ignore} and {docsify-ignore-all} to HTML comments (#1318) (90d283d)</li> <li>fallback page should use path not file location (#1301) (2bceabc)</li> <li>Fix search error when exist translations documents (#1300) (b869019)</li> <li>gitignore was ignoring folders in src, so VS Code search results or file fuzzy finder were not working, etc (d4c9247)</li> <li>packages/docsify-server-renderer/package.json &amp; packages/docsify-server-renderer/package-lock.json to reduce vulnerabilities (#1250) (d439bac)</li> <li>search can not search the table header (#1256) (3f03e78)</li> <li>Search plugin: matched text is replaced with search text (#1298) (78775b6)</li> <li>the uncaught typeerror when el is null (#1308) (952f4c9)</li> <li>Updated docs with instructions for installing specific version (fixes #780) (#1225) (b90c948)</li> <li>upgrade medium-zoom from 1.0.5 to 1.0.6 (3beaa66)</li> <li>upgrade tinydate from 1.2.0 to 1.3.0 (#1341) (59d090f)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_3","title":"Features","text":"<ul> <li>search: add pathNamespaces option (d179dde)</li> <li>Add title to sidebar links (#1286) (667496b)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4114-2020-06-18","title":"4.11.4 (2020-06-18)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>consistent location of search result (e9dd2de)</li> <li>cover overlapping sidebar by removing z-index (0bf03f5)</li> <li>cross-origin url cannot be redirected when  \"externalLinkTarget\" is set to \"_self\" and \"routerMode\" is set to \"history\". (#1062) (fd2cec6), closes #1046 #1046 #1046</li> <li>default html img resize if no height included (#1065) (9ff4d06)</li> <li>fixed target and rel issue (fixes #1183) (3d662a5)</li> <li>Inconsistent search and body rendering (dcb0aae)</li> <li>rendering cover width bug (717991c)</li> <li>search does not find the contents of the table (#1198) (31010e4)</li> <li>The search error after setting the ID in the title (#1159) (6e554f8)</li> <li>upgrade docsify from 4.10.2 to 4.11.2 (60b7f89)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_4","title":"Features","text":"<ul> <li>added html sanitizer for remote rendering (#1128) (714ef29)</li> <li>update src/core/index.js to export all global APIs, deprecate old globals in favor of a single global DOCSIFY, and add tests for this (7e002bf)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#reverts","title":"Reverts","text":"<ul> <li>Revert \"Updated docs site dark and light mode with switch and redesigned search bar using docsify-darklight-theme\" (#1207) (26cb940), closes #1207 #1182</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4113-2020-03-24","title":"4.11.3 (2020-03-24)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>fix: digit issue with sidebar (complete REVERT to old method) (154abf5)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4112-2020-03-09","title":"4.11.2 (2020-03-09)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>fixed rendering of color in coverpage issue (406670c)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4111-2020-03-09","title":"4.11.1 (2020-03-09)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#4110-2020-03-09","title":"4.11.0 (2020-03-09)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>emojis in titles not working correctly and update (#1016) (b3d9b96)</li> <li>searching table content (6184e50)</li> <li>stage modified files as part of pre-commit hook (#985) (5b77b0f)</li> <li>config initialization and coercion (#861)</li> <li>strip indent when embedding code fragment (#996)</li> <li>Ensure autoHeader dom result is similar to parsed H1 (#811)</li> <li>upgrade docsify from 4.9.4 to 4.10.2 (#1054) (78290b2)</li> <li>upgrade medium-zoom from 1.0.4 to 1.0.5 (39ebd73)</li> <li>upgrade prismjs from 1.17.1 to 1.19.0 (9981c43)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_5","title":"Features","text":"<ul> <li>configure pre-commit hook (#983) (eea41a1)</li> <li>Add a prepare script. (efbea24)</li> <li>added capability to add css class and id to images + links + refactoring (#820) (724ac02)</li> <li>added dark mode to docs closes #1031 (dc43d3c)</li> <li>new option <code>hideSidebar</code> (#1026) (b7547f1)</li> <li>new option <code>topMargin</code> (#1045) (8faee03)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#docs","title":"Docs","text":"<ul> <li>update docs for the <code>name</code> config option (#992)</li> <li>about cache (#854)</li> <li>removed FOSSA badge</li> <li>documented <code>__colon__</code> tip (#1025)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#chore_1","title":"Chore","text":"<ul> <li>Migrate relative links to absolute in embedded markdown (#867)</li> <li>smarter scroll behavior (#744)</li> <li>improve basic layout style (#884)</li> <li>There are currently {three=&gt;four} themes available. (#892)</li> <li>Added a redirect for images to show up in Amplify (#918)</li> <li>removed the escaping of the name of sidebar (#991)</li> <li>Eslint fixes for v4x (#989)</li> <li>added github Actions for CI (#1000)</li> <li>Add a prepare script. (#1010)</li> <li>chore(GH-action): using ubuntu 16 and removed node 8 from CI</li> <li>chore: add config (#1014)</li> <li>chore(stale): added enhancement label to exemptlabels</li> <li>chore(stale): added bug label to exemptlabels</li> <li>.markdown-section max-width 800px to 80% (#1017)</li> <li>changed the CDN from unpkg to jsDelivr #1020 (#1022)</li> <li>migrate CI to github, refactore CI and npm scripts, linting fixes (#1023)</li> <li>chore(readme): added CI badges and fixed the logo issue</li> <li>added new linter config (#1028)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4102-2019-12-16","title":"4.10.2 (2019-12-16)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#4100-2019-12-16","title":"4.10.0 (2019-12-16)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>fixed security alert for chokidar(update dep) (a62b037)</li> <li>npm audit issues (#934) (615205c)</li> <li>package security alerts (f5f1561)</li> <li>security alerts of cssnano (d7d5c8f)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#494-2019-05-05","title":"4.9.4 (2019-05-05)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#492-2019-04-21","title":"4.9.2 (2019-04-21)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>re-render gitalk when router changed (11ea1f8)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_6","title":"Features","text":"<ul> <li>allows relative path, fixed #590 (31654f1)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#491-2019-02-21","title":"4.9.1 (2019-02-21)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>github assets url (#774) (140bf10)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#490-2019-02-19","title":"4.9.0 (2019-02-19)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_13","title":"Bug Fixes","text":"<ul> <li>task list rendering (Fix #749) (#757) (69ef489)</li> <li>upgrade npm-run-all (049726e)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_7","title":"Features","text":"<ul> <li>search-plugin: add namespace option (#706) (28beff8)</li> <li>Add new theme \"dolphin\" (#735) (c3345ba)</li> <li>Provide code fragments feature (#748) (1447c8a)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#486-2018-11-12","title":"4.8.6 (2018-11-12)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_14","title":"Bug Fixes","text":"<ul> <li>IE10 compatibility (#691) (4db8cd6)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#485-2018-11-02","title":"4.8.5 (2018-11-02)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_15","title":"Bug Fixes","text":"<ul> <li>expose version info for Docsify, fixed #641 (aa719e3)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#484-2018-11-01","title":"4.8.4 (2018-11-01)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_16","title":"Bug Fixes","text":"<ul> <li>cover: Compatible with legacy styles, fixed #677 (#678) (1a945d4)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#483-2018-11-01","title":"4.8.3 (2018-11-01)","text":"<p>Fix the last release files has the old version marked...</p> <p></p>"},{"location":"docsify_docs/docsify/CHANGELOG/#482-2018-11-01","title":"4.8.2 (2018-11-01)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_17","title":"Bug Fixes","text":"<ul> <li>cover button style, fixed #670, fixed #665 (#675) (fcd1087)</li> <li>update match regex (#669) (2edf47e)</li> <li>use copy of cached value (#668) (5fcf210)</li> <li>compiler: import prism-markup-templating, fixed #672 (#676) (fdd8826)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_8","title":"Features","text":"<ul> <li>add heading config id (#671) (ab19b13)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#481-2018-10-31","title":"4.8.1 (2018-10-31)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_18","title":"Bug Fixes","text":"<ul> <li>ssr package dep, fixed #605 (2bc880d)</li> <li>compiler: extra quotes for codeblock (4f588e0)</li> <li>compiler: prevent render of html code in paragraph, fixed #663 (d35059d)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_9","title":"Features","text":"<ul> <li>upgrade PrismJS, fixed #534 (4805cb5)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#480-2018-10-31","title":"4.8.0 (2018-10-31)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_19","title":"Bug Fixes","text":"<ul> <li>Cache TOC for later usage in the case of cached file html (#649) (9e86017)</li> <li>improve external script plugin (#632) (50c2434)</li> <li>missing variable declaration (#660) (1ce37bd)</li> <li>Remove target for mailto links (#652) (18f0f03)</li> <li>Update getAllPath query selector (#653) (f6f4e32)</li> <li>Update vue.styl (#634) (bf060be)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_10","title":"Features","text":"<ul> <li>Add docsify version to $window.docsify object (#641) (94bc415), closes #521</li> <li>compiler: support embedded mermaid (#629) (42ea8af)</li> <li>Add hideOtherSidebarContent option (#661) (4a23c4a)</li> <li>Allow base64, external, and relative logo values (#642) (0a0802a), closes #577</li> <li>upgrade marked to 0.5.x, fixed #645, close #644 (#662) (a39b214)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#471-2018-08-30","title":"4.7.1 (2018-08-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#470-2018-06-29","title":"4.7.0 (2018-06-29)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_20","title":"Bug Fixes","text":"<ul> <li>alldow addition content in sidebar, fix #518, fix 539 (#543) (04b36b0)</li> <li>async install config, fixed #425 (e4e011c)</li> <li>loading embed files synchronously, fixed #525, fixed #527 (#544) (feea7f9)</li> <li>path include chinese character cause hilight bug (#556) (a5f333a)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_11","title":"Features","text":"<ul> <li>add logo option, #264 (#541) (ee72dd0)</li> <li>add unpkg field, close #531 (#558) (5c0de0a)</li> <li>support image resizing, resolve #508 (#545) (3a7ad62)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4610-2018-03-25","title":"4.6.10 (2018-03-25)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_21","title":"Bug Fixes","text":"<ul> <li>async install config, fixed #425 (e4e011c)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#469-2018-03-10","title":"4.6.9 (2018-03-10)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_22","title":"Bug Fixes","text":"<ul> <li>upgrade medium-zoom, fixed #417 (6a3d69a)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#468-2018-03-06","title":"4.6.8 (2018-03-06)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_23","title":"Bug Fixes","text":"<ul> <li>resolve path of image and embed files, fixed #412 (bfd0d18)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#467-2018-03-03","title":"4.6.7 (2018-03-03)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_24","title":"Bug Fixes","text":"<ul> <li>layout css, fixed #409 (aeb692e)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#466-2018-03-03","title":"4.6.6 (2018-03-03)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#465-2018-03-03","title":"4.6.5 (2018-03-03)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_25","title":"Bug Fixes","text":"<ul> <li>navbar: Now Navbar isn't append to DOM when loadNavbar is falsy (#407) (0933445)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_12","title":"Features","text":"<ul> <li>config: Add 404 page options. (#406) (9b3b445)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#464-2018-03-01","title":"4.6.4 (2018-03-01)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_26","title":"Bug Fixes","text":"<ul> <li>render: Disable markdown parsing when the file is an HTML (#403) (278a75e)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_13","title":"Features","text":"<ul> <li>fetch: Add fallback languages configuration. (#402) (ecc0e04)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#463-2018-02-15","title":"4.6.3 (2018-02-15)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_27","title":"Bug Fixes","text":"<ul> <li>hook: beforeEach don\\'t work, fixed #393 (6a09059)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#462-2018-02-14","title":"4.6.2 (2018-02-14)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_28","title":"Bug Fixes","text":"<ul> <li>embed: broken in IE, fixed #389, fixed #391 (45a7464)</li> <li>embed: init value (890a7bf)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#461-2018-02-12","title":"4.6.1 (2018-02-12)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_29","title":"Bug Fixes","text":"<ul> <li>embed compatible ssr (dc0c3ce)</li> <li>embed async fetch embed files, fixed #387</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#460-2018-02-11","title":"4.6.0 (2018-02-11)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_30","title":"Bug Fixes","text":"<ul> <li>search: custom clear button, fixed #271 (864aa18)</li> <li>search: escape special characters for search, fixed #369 (9755439)</li> <li>build config (342438f)</li> <li>button style for coverpage, fixed #362 (85428ef)</li> <li>dropdown scroll style, fixed #346 (c4d83f2)</li> <li>highlight homepage link, fixed #304 (f960c19)</li> <li>homepage link (e097f88)</li> <li>onlyCover (033be4f)</li> <li>ssr compatible embedd (ebc10c4)</li> <li>ssr coverpage, fixed #273 (9e824a4)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_14","title":"Features","text":"<ul> <li>click sidebar menu add collapse and expand, close #294 (5e161a1)</li> <li>compiler: support embedded file as code block, close #134 (761ccc2)</li> <li>compiler: support embedded markdown, html, video, etc files, close #383, close #333 (524f52f)</li> <li>cover: add onlyCover option, close #382 (b265fdd)</li> <li>fetch: add requestHeaders option, fixed #336 (54ab4c9)</li> <li>render: add ext option for custom file extension, close #340 (248aa72)</li> <li>render: mutilple coverpage, close #315 (f68ddf5)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#459-2018-02-07","title":"4.5.9 (2018-02-07)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_31","title":"Bug Fixes","text":"<ul> <li>upgrade marked (4157173)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#458-2018-02-07","title":"4.5.8 (2018-02-07)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_32","title":"Bug Fixes","text":"<ul> <li>cover style, fixed #381 (368754e)</li> <li>updated deps (#337) (a12d393)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#457-2017-12-29","title":"4.5.7 (2017-12-29)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_15","title":"Features","text":"<ul> <li>add navigation plugin, closed #180 (f78be4c)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#456-2017-12-14","title":"4.5.6 (2017-12-14)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_33","title":"Bug Fixes","text":"<ul> <li>style: increase the tap targets of menu button, fixed #325 (888f217)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#455-2017-11-30","title":"4.5.5 (2017-11-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_34","title":"Bug Fixes","text":"<ul> <li>disqus plugin issue (#318) (041b33e), closes #317</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#454-2017-11-29","title":"4.5.4 (2017-11-29)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_35","title":"Bug Fixes","text":"<ul> <li>compiler: task lists style, fixed #215 (e43ded4)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_16","title":"Features","text":"<ul> <li>add gitalk plugin (#306) (9208e64)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#453-2017-11-11","title":"4.5.3 (2017-11-11)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_17","title":"Features","text":"<ul> <li>add gitalk plugin (#306) (9208e64)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#452-2017-11-09","title":"4.5.2 (2017-11-09)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_18","title":"Features","text":"<ul> <li>github task lists, close #215 (#305) (d486eef)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#451-2017-11-07","title":"4.5.1 (2017-11-07)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_19","title":"Features","text":"<ul> <li>fetch files with the query params, fixed #303 (2a2ed96)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#450-2017-11-04","title":"4.5.0 (2017-11-04)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_20","title":"Features","text":"<ul> <li>add disqus plugin, closed #123 (fd7d4e0)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#441-2017-10-31","title":"4.4.1 (2017-10-31)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_36","title":"Bug Fixes","text":"<ul> <li>{docsify-ignore-all} and {docsify-ignore} bug (#299) (cc98f56)</li> <li>zoom image plugin issue, fixed #187 (#300) (fa772cf)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#440-2017-10-30","title":"4.4.0 (2017-10-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_37","title":"Bug Fixes","text":"<ul> <li>sidebar style issue on firefox, fixed #184 (#297) (36bfc9d)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_21","title":"Features","text":"<ul> <li>add helper for disabled link, fixed #295 (#296) (4ad96f3)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4315-2017-10-20","title":"4.3.15 (2017-10-20)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_38","title":"Bug Fixes","text":"<ul> <li>scroll active sidebar (a2b8eae)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4314-2017-10-20","title":"4.3.14 (2017-10-20)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_39","title":"Bug Fixes","text":"<ul> <li>codesponsor style (ab68268)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4313-2017-10-17","title":"4.3.13 (2017-10-17)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_40","title":"Bug Fixes","text":"<ul> <li>duplicate results in search fixed #257 (#284) (3476f6f)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_22","title":"Features","text":"<ul> <li>make whole search result clickable (#285) (1b91227)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4312-2017-10-15","title":"4.3.12 (2017-10-15)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_41","title":"Bug Fixes","text":"<ul> <li>incorrect active link (#281) (a3ab379)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4311-2017-10-15","title":"4.3.11 (2017-10-15)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_42","title":"Bug Fixes","text":"<ul> <li>broken links to same page heading, fix #278, fix #279 (91d6337)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4310-2017-10-12","title":"4.3.10 (2017-10-12)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_43","title":"Bug Fixes","text":"<ul> <li>link render issue after page refreshing (#276) (abd885e)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#439-2017-10-11","title":"4.3.9 (2017-10-11)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_44","title":"Bug Fixes","text":"<ul> <li>scroll issue in IE (#275) (3e94cb6)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#438-2017-10-07","title":"4.3.8 (2017-10-07)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_45","title":"Bug Fixes","text":"<ul> <li>slugify: GitHub compatible heading links, fixed #272 (9b4e666)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#437-2017-10-02","title":"4.3.7 (2017-10-02)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_46","title":"Bug Fixes","text":"<ul> <li>slugify: GitHub compatible heading links, fixed #267 (c195d2d)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#436-2017-09-21","title":"4.3.6 (2017-09-21)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_47","title":"Bug Fixes","text":"<ul> <li>style for codesponsor plugin (08afec7)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#435-2017-09-20","title":"4.3.5 (2017-09-20)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_48","title":"Bug Fixes","text":"<ul> <li>missed symbol (#254) (6c702d3)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_23","title":"Features","text":"<ul> <li>plugin: add codesponsor plugin (46ac4c3)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#434-2017-09-07","title":"4.3.4 (2017-09-07)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_49","title":"Bug Fixes","text":"<ul> <li>scroll position issue, fixed #234 (388ed3d)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#433-2017-09-06","title":"4.3.3 (2017-09-06)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_50","title":"Bug Fixes","text":"<ul> <li>buble.css: tweaks code block style, fixed #249 (9d43051)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_24","title":"Features","text":"<ul> <li>add doc for react and vue demo box plugin (#247) (f0aca19)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#432-2017-09-01","title":"4.3.2 (2017-09-01)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_51","title":"Bug Fixes","text":"<ul> <li>sidebar highlight (f82f419)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_25","title":"Features","text":"<ul> <li>add Edit on github plugin (thanks @njleonzhang) (a0e1ea8)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#431-2017-08-30","title":"4.3.1 (2017-08-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_26","title":"Features","text":"<ul> <li>markdown: supports mermaid #137 (f4800e0)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#430-2017-08-17","title":"4.3.0 (2017-08-17)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_27","title":"Features","text":"<ul> <li>markdown: supports mermaid #137 (f4800e0)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#429-2017-08-15","title":"4.2.9 (2017-08-15)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_52","title":"Bug Fixes","text":"<ul> <li>ensure document ready before init Docsify #233</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#428-2017-08-10","title":"4.2.8 (2017-08-10)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_28","title":"Features","text":"<ul> <li>compiler: support for setting target attribute for link, fixed #230 (7f270f9)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#427-2017-08-05","title":"4.2.7 (2017-08-05)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_53","title":"Bug Fixes","text":"<ul> <li>release: release shell (628e211)</li> <li>style: nowrap =&gt; pre-wrap, fixed #228 (a88252c)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#426-2017-07-27","title":"4.2.6 (2017-07-27)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_54","title":"Bug Fixes","text":"<ul> <li>css: hide the nav when the content has not yet been loaded (1fa1619)</li> <li>release: release shell (628e211)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#424-2017-07-26","title":"4.2.4 (2017-07-26)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_55","title":"Bug Fixes","text":"<ul> <li>render: Remove getRootNode to be compatible with the lower version of Chrome, fixed #225 (b8dd346)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#423-2017-07-26","title":"4.2.3 (2017-07-26)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#features_29","title":"Features","text":"<ul> <li>search: Supports the max depth of the search headline, fixed #223, resolve #129 (b7b589b)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#422-2017-07-24","title":"4.2.2 (2017-07-24)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_56","title":"Bug Fixes","text":"<ul> <li>style rerender due to setting themeColor (17ff3d1)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#421-2017-07-19","title":"4.2.1 (2017-07-19)","text":"<ul> <li>give the navbar some line-height (#216)</li> <li>Remove unnecessary moduleName option from rollup config for plugins (#209)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#420-2017-07-10","title":"4.2.0 (2017-07-10)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_57","title":"Bug Fixes","text":"<ul> <li>not found page (9af8559)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_30","title":"Features","text":"<ul> <li>alias option supports regexp, resolve #183 (c4aa22c)</li> <li>ignore to compiled link, fixed #203 (#204) (2e00f4c)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4114-2017-06-24","title":"4.1.14 (2017-06-24)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_58","title":"Bug Fixes","text":"<ul> <li>get file path, fixed jrappen/sublime-distractionless/commit/81bfadd391428823191cc03eca956a2312e04d13#commitcomment-22427070 (e8117e5), closes jrappen/sublime-distractionless/commit/81bfadd391428823191cc03eca956a2312e04d13#commitcomment-22427070</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_31","title":"Features","text":"<ul> <li>add context attribute, fixed #191 (ce0e9ac)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4113-2017-06-11","title":"4.1.13 (2017-06-11)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#4112-2017-06-03","title":"4.1.12 (2017-06-03)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_59","title":"Bug Fixes","text":"<ul> <li>render: subtitle in side bar shows undefined, fixed #182 (d087d57)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4111-2017-06-02","title":"4.1.11 (2017-06-02)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_60","title":"Bug Fixes","text":"<ul> <li>compiler: force reset toc when rendering sidebar fixed #181 (ccf4c7c)</li> <li>render: autoHeader does not work (1304d2e)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#4110-2017-06-02","title":"4.1.10 (2017-06-02)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_61","title":"Bug Fixes","text":"<ul> <li>hash: hash routing crashes when url has querystring (6d48ce1)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#419-2017-05-31","title":"4.1.9 (2017-05-31)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_62","title":"Bug Fixes","text":"<ul> <li>can't render toc on first load (d9b487e)</li> <li>lifecycle: continue to handle data (955d3d5)</li> <li>render: broken name link, fixed #167 (91b66a5)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#418-2017-05-31","title":"4.1.8 (2017-05-31)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_63","title":"Bug Fixes","text":"<ul> <li>auto replace version (22b50f0)</li> <li>update edit button demo (ec887c1)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_32","title":"Features","text":"<ul> <li>add edit button demo (a64cee1)</li> <li>add edit button demo, close #162 (036fdac)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#417-2017-05-30","title":"4.1.7 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_64","title":"Bug Fixes","text":"<ul> <li>ssr: clean files (0014895)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#416-2017-05-30","title":"4.1.6 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_65","title":"Bug Fixes","text":"<ul> <li>ssr: add debug (6b9e092)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#415-2017-05-30","title":"4.1.5 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_66","title":"Bug Fixes","text":"<ul> <li>ssr: missing package (6db8c9e)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#414-2017-05-30","title":"4.1.4 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_67","title":"Bug Fixes","text":"<ul> <li>ssr: file path (79a83bc)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#413-2017-05-30","title":"4.1.3 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_68","title":"Bug Fixes","text":"<ul> <li>update babel config (9825db4)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#412-2017-05-30","title":"4.1.2 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_69","title":"Bug Fixes","text":"<ul> <li>update babel config (80dba19)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#411-2017-05-30","title":"4.1.1 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_70","title":"Bug Fixes","text":"<ul> <li>build for ssr package (4cb20a5)</li> <li>remove history mode (0e74e6c)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#410-2017-05-30","title":"4.1.0 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#402-2017-05-30","title":"4.0.2 (2017-05-30)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_71","title":"Bug Fixes","text":"<ul> <li>basePath for history mode (fc1cd3f)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#401-2017-05-29","title":"4.0.1 (2017-05-29)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_72","title":"Bug Fixes","text":"<ul> <li>ssr: remove context (4626157)</li> <li>lint (b764b6e)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#400-2017-05-29","title":"4.0.0 (2017-05-29)","text":""},{"location":"docsify_docs/docsify/CHANGELOG/#bug-fixes_73","title":"Bug Fixes","text":"<ul> <li>render: init event in ssr (eba1c98)</li> <li>lint (1f4514d)</li> </ul>"},{"location":"docsify_docs/docsify/CHANGELOG/#features_33","title":"Features","text":"<ul> <li>finish ssr (3444884)</li> <li>init ocsify-server-renderer (6dea685)</li> <li>support history mode (f095eb8)</li> </ul>"},{"location":"docsify_docs/docsify/CONTRIBUTING/","title":"Contribute","text":""},{"location":"docsify_docs/docsify/CONTRIBUTING/#introduction","title":"Introduction","text":"<p>First, thank you for considering contributing to docsify! It's people like you that make the open source community such a great community! \ud83d\ude0a</p> <p>We welcome any type of contribution, not only code. You can help with</p> <ul> <li>QA: file bug reports, the more details you can give the better (e.g. screenshots with the console open)</li> <li>Marketing: writing blog posts, howto's, printing stickers, ...</li> <li>Community: presenting the project at meetups, organizing a dedicated meetup for the local community, ...</li> <li>Code: take a look at the open issues. Even if you can't write code, commenting on them, showing that you care about a given issue matters. It helps us triage them.</li> <li>Money: we welcome financial contributions in full transparency on our open collective.</li> </ul>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#your-first-contribution","title":"Your First Contribution","text":"<p>Working on your first Pull Request ever? You can learn how from this free series, How to Contribute to an Open Source Project on GitHub.</p>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#online-one-click-setup-for-contributing","title":"Online one-click setup for Contributing","text":"<p>You can use Gitpod (a free online VS Code-like IDE) for contributing. With a single click it'll launch a workspace and automatically:</p> <ul> <li>clone the docsify repo.</li> <li>install the dependencies.</li> <li>start <code>npm run dev</code>.</li> </ul> <pre><code>npm install &amp;&amp; npm run dev\n</code></pre> <p>So that you can start straight away.</p> <p></p> <ul> <li>Fork it!</li> <li>Create your feature branch: <code>git checkout -b my-new-feature</code></li> <li>Commit your changes: <code>git add . &amp;&amp; git commit -m 'Add some feature'</code></li> <li>Push to the branch: <code>git push origin my-new-feature</code></li> <li>Submit a pull request</li> </ul>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#submitting-code","title":"Submitting code","text":"<p>Any code change should be submitted as a pull request. The description should explain what the code does and give steps to execute it. The pull request should also contain tests.</p>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#testing","title":"Testing","text":"<p>Ensure that things work by running:</p> <pre><code>npm test\n</code></pre>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#test-snapshots","title":"Test Snapshots","text":"<p>If a snapshot fails, or to add new snapshots, run:</p> <pre><code>npx jest --updateSnapshot\n</code></pre>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#code-review-process","title":"Code review process","text":"<p>The bigger the pull request, the longer it will take to review and merge. Try to break down large pull requests in smaller chunks that are easier to review and merge. It is also always helpful to have some context for your pull request. What was the purpose? Why does it matter to you?</p>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#financial-contributions","title":"Financial contributions","text":"<p>We also welcome financial contributions in full transparency on our open collective. Anyone can file an expense. If the expense makes sense for the development of the community, it will be \"merged\" in the ledger of our open collective by the core contributors and the person who filed the expense will be reimbursed.</p>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#questions","title":"Questions","text":"<p>If you have any questions, create an issue (protip: do a quick search first to see if someone else didn't ask the same question before!). You can also reach us at hello@docsify.opencollective.com.</p>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#credits","title":"Credits","text":""},{"location":"docsify_docs/docsify/CONTRIBUTING/#contributors","title":"Contributors","text":"<p>Thank you to all the people who have already contributed to docsify! </p>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#backers","title":"Backers","text":"<p>Thank you to all our backers! [Become a backer]</p> <p></p>"},{"location":"docsify_docs/docsify/CONTRIBUTING/#sponsors","title":"Sponsors","text":"<p>Thank you to all our sponsors! (please ask your company to also support this open source project by becoming a sponsor)</p> <p> </p>"},{"location":"docsify_docs/docsify/HISTORY/","title":"HISTORY","text":""},{"location":"docsify_docs/docsify/HISTORY/#373-2017-05-22","title":"3.7.3 (2017-05-22)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>render: find =&gt; filter (eca3368)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#372-2017-05-19","title":"3.7.2 (2017-05-19)","text":""},{"location":"docsify_docs/docsify/HISTORY/#371-2017-05-19","title":"3.7.1 (2017-05-19)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>docsify-updated is undefined (b2b4742)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#370-2017-05-16","title":"3.7.0 (2017-05-16)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features","title":"Features","text":"<ul> <li>add docsify-updated, close #158 (d2be5ae)</li> <li>add externalLinkTarget, close #149 (2d73285)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#366-2017-05-06","title":"3.6.6 (2017-05-06)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_1","title":"Features","text":"<ul> <li>support query string for the search, fixed #156 (da75d70)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#365-2017-04-28","title":"3.6.5 (2017-04-28)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>util: fix crash, fixed #154 (51832d3)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#364-2017-04-28","title":"3.6.4 (2017-04-28)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>util: correctly clean up duplicate slashes, fixed #153 (76c041a)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#363-2017-04-25","title":"3.6.3 (2017-04-25)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>external-script: script attrs (2653849)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#362-2017-04-12","title":"3.6.2 (2017-04-12)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_2","title":"Features","text":"<ul> <li>event: Collapse the sidebar when click outside element in the small screen (9b7e5f5)</li> <li>external-script: detect more than one script dom, fixed #146 (94d6603)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#360-2017-04-09","title":"3.6.0 (2017-04-09)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_3","title":"Features","text":"<ul> <li>render: add mergeNavbar option, close #125, #124 (#145) (9220523)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#352-2017-04-05","title":"3.5.2 (2017-04-05)","text":""},{"location":"docsify_docs/docsify/HISTORY/#351-2017-03-25","title":"3.5.1 (2017-03-25)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>.md file extension regex (594299f)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#350-2017-03-25","title":"3.5.0 (2017-03-25)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>adjust display on small screens (bf35471)</li> <li>navbar labels for German (b022aaf)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#features_4","title":"Features","text":"<ul> <li>route: auto remove .md extension (8f11653)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#344-2017-03-17","title":"3.4.4 (2017-03-17)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>search: fix input style (2d6a51b)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#343-2017-03-16","title":"3.4.3 (2017-03-16)","text":""},{"location":"docsify_docs/docsify/HISTORY/#342-2017-03-11","title":"3.4.2 (2017-03-11)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_5","title":"Features","text":"<ul> <li>emojify: add no-emoji option (3aef37a)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#341-2017-03-10","title":"3.4.1 (2017-03-10)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>dom: Disable the dom cache when vue is present, fixed #119 (b9a7275)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#340-2017-03-09","title":"3.4.0 (2017-03-09)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_6","title":"Features","text":"<ul> <li>zoom-image: add plugin (50fa6fc)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#330-2017-03-07","title":"3.3.0 (2017-03-07)","text":""},{"location":"docsify_docs/docsify/HISTORY/#320-2017-02-28","title":"3.2.0 (2017-02-28)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>fetch: load sidebar and navbar for parent path, fixed #100 (f3fc596)</li> <li>render: Toc rendering error, fixed #106 (0d59ee9)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#features_7","title":"Features","text":"<ul> <li>search: Localization for no data tip, close #103 (d3c9fbd)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#312-2017-02-27","title":"3.1.2 (2017-02-27)","text":""},{"location":"docsify_docs/docsify/HISTORY/#311-2017-02-24","title":"3.1.1 (2017-02-24)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>render: custom cover background image (8f9bf29)</li> <li>search: don't search nameLink, fixed #102 (507d9e8)</li> <li>tpl: extra character, fixed #101 (d67d25f)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#310-2017-02-22","title":"3.1.0 (2017-02-22)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>search: incorrect anchor link, fixed #90 (b8a3d8f)</li> <li>sw: update white list (f2975a5)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#features_8","title":"Features","text":"<ul> <li>emoji: add emoji plugin (855c450)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#305-2017-02-21","title":"3.0.5 (2017-02-21)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>event: highlight sidebar when clicked, fixed #86 (2a1157a)</li> <li>gen-tree: cache toc list, fixed #88 (3394ebb)</li> <li>layout.css: loading style (42b2dba)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#features_9","title":"Features","text":"<ul> <li>pwa: add sw.js (f7111b5)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#304-2017-02-20","title":"3.0.4 (2017-02-20)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_13","title":"Bug Fixes","text":"<ul> <li>render: disable rendering sub list when loadSidebar is false (35dd2e1)</li> <li>render: execute script (780c1e5)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#303-2017-02-19","title":"3.0.3 (2017-02-19)","text":""},{"location":"docsify_docs/docsify/HISTORY/#302-2017-02-19","title":"3.0.2 (2017-02-19)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_14","title":"Bug Fixes","text":"<ul> <li>compiler: link (3b127a1)</li> <li>search: add lazy input (bf593a7)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#301-2017-02-19","title":"3.0.1 (2017-02-19)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_15","title":"Bug Fixes","text":"<ul> <li>route: empty alias (cd99b52)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#300-2017-02-19","title":"3.0.0 (2017-02-19)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_16","title":"Bug Fixes","text":"<ul> <li>compiler: link (c7e09c3)</li> <li>render: support html file (7b6a2ac)</li> <li>search: escape html (fcb66e8)</li> <li>search: fix default config (2efd859)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#features_10","title":"Features","text":"<ul> <li>front-matter: add front matter[WIP] (dbb9278)</li> <li>render: add auto header (b7768b1)</li> <li>search: Localization for search placeholder, close #80 (2351c3e)</li> <li>themes: add loading info (86594a3)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#243-2017-02-15","title":"2.4.3 (2017-02-15)","text":""},{"location":"docsify_docs/docsify/HISTORY/#242-2017-02-14","title":"2.4.2 (2017-02-14)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_17","title":"Bug Fixes","text":"<ul> <li>index: load file path error (dc536a3)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#241-2017-02-13","title":"2.4.1 (2017-02-13)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_18","title":"Bug Fixes","text":"<ul> <li>index: cover page (dd0c84b)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#240-2017-02-13","title":"2.4.0 (2017-02-13)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_11","title":"Features","text":"<ul> <li>hook: add doneEach (c6f7602)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#230-2017-02-13","title":"2.3.0 (2017-02-13)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_19","title":"Bug Fixes","text":"<ul> <li>event:  has no effect on a FF mobile browser, fixed #67 (0ff36c2)</li> <li>render: custom marked renderer (bf559b4)</li> <li>render: fix render link (a866744)</li> <li>render: image url (6f87529)</li> <li>render: render link (38ea660)</li> <li>src: fix route (324301a)</li> <li>src: get alias (784173e)</li> <li>src: get alias (ce99a04)</li> <li>themes: fix navbar style (fa54b52)</li> <li>themes: update navbar style (4864d1b)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#features_12","title":"Features","text":"<ul> <li>hook: support custom plugin (9e81a59)</li> <li>src: add alias feature (24412cd)</li> <li>src: dynamic title and fix sidebar style (6b30eb6)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#221-2017-02-11","title":"2.2.1 (2017-02-11)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_20","title":"Bug Fixes","text":"<ul> <li>event: scroll active sidebar (50f5fc2)</li> <li>search: crash when not content, fixed #68 (9d3cc89)</li> <li>search: not work in mobile (3941304)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#220-2017-02-09","title":"2.2.0 (2017-02-09)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_13","title":"Features","text":"<ul> <li>plugins: add Google Analytics plugin (#66) (ac61bb0)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#210-2017-02-09","title":"2.1.0 (2017-02-09)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_21","title":"Bug Fixes","text":"<ul> <li>render name (12e2479)</li> <li>vue.css: update sidebar style (fc140ef)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#features_14","title":"Features","text":"<ul> <li>add search, close #43 (eb5ff3e)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#203-2017-02-07","title":"2.0.3 (2017-02-07)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_22","title":"Bug Fixes","text":"<ul> <li>css var polyfill (8cd386a)</li> <li>css var polyfill (cbaee21)</li> <li>rendering emojis (8c7e4d7)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#202-2017-02-05","title":"2.0.2 (2017-02-05)","text":""},{"location":"docsify_docs/docsify/HISTORY/#bug-fixes_23","title":"Bug Fixes","text":"<ul> <li>button style in cover page (4470855)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#201-2017-02-05","title":"2.0.1 (2017-02-05)","text":""},{"location":"docsify_docs/docsify/HISTORY/#200-2017-02-05","title":"2.0.0 (2017-02-05)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_15","title":"Features","text":"<ul> <li>customize the theme color (5cc9f05)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#1105-2017-01-28","title":"1.10.5 (2017-01-28)","text":""},{"location":"docsify_docs/docsify/HISTORY/#1104-2017-01-27","title":"1.10.4 (2017-01-27)","text":""},{"location":"docsify_docs/docsify/HISTORY/#1103-2017-01-27","title":"1.10.3 (2017-01-27)","text":""},{"location":"docsify_docs/docsify/HISTORY/#1102-2017-01-25","title":"1.10.2 (2017-01-25)","text":""},{"location":"docsify_docs/docsify/HISTORY/#1101-2017-01-25","title":"1.10.1 (2017-01-25)","text":""},{"location":"docsify_docs/docsify/HISTORY/#1100-2017-01-25","title":"1.10.0 (2017-01-25)","text":""},{"location":"docsify_docs/docsify/HISTORY/#190-2017-01-24","title":"1.9.0 (2017-01-24)","text":""},{"location":"docsify_docs/docsify/HISTORY/#180-2017-01-24","title":"1.8.0 (2017-01-24)","text":""},{"location":"docsify_docs/docsify/HISTORY/#174-2017-01-13","title":"1.7.4 (2017-01-13)","text":""},{"location":"docsify_docs/docsify/HISTORY/#173-2017-01-13","title":"1.7.3 (2017-01-13)","text":""},{"location":"docsify_docs/docsify/HISTORY/#172-2017-01-12","title":"1.7.2 (2017-01-12)","text":""},{"location":"docsify_docs/docsify/HISTORY/#171-2017-01-12","title":"1.7.1 (2017-01-12)","text":""},{"location":"docsify_docs/docsify/HISTORY/#170-2017-01-12","title":"1.7.0 (2017-01-12)","text":""},{"location":"docsify_docs/docsify/HISTORY/#161-2017-01-10","title":"1.6.1 (2017-01-10)","text":""},{"location":"docsify_docs/docsify/HISTORY/#160-2017-01-10","title":"1.6.0 (2017-01-10)","text":""},{"location":"docsify_docs/docsify/HISTORY/#152-2017-01-10","title":"1.5.2 (2017-01-10)","text":""},{"location":"docsify_docs/docsify/HISTORY/#151-2017-01-09","title":"1.5.1 (2017-01-09)","text":""},{"location":"docsify_docs/docsify/HISTORY/#150-2017-01-04","title":"1.5.0 (2017-01-04)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_16","title":"Features","text":"<ul> <li>Markdown parser is configurable, #42 (8b1000a)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#143-2017-01-01","title":"1.4.3 (2017-01-01)","text":""},{"location":"docsify_docs/docsify/HISTORY/#142-2016-12-31","title":"1.4.2 (2016-12-31)","text":""},{"location":"docsify_docs/docsify/HISTORY/#141-2016-12-31","title":"1.4.1 (2016-12-31)","text":""},{"location":"docsify_docs/docsify/HISTORY/#140-2016-12-31","title":"1.4.0 (2016-12-31)","text":""},{"location":"docsify_docs/docsify/HISTORY/#135-2016-12-25","title":"1.3.5 (2016-12-25)","text":""},{"location":"docsify_docs/docsify/HISTORY/#134-2016-12-25","title":"1.3.4 (2016-12-25)","text":""},{"location":"docsify_docs/docsify/HISTORY/#133-2016-12-23","title":"1.3.3 (2016-12-23)","text":""},{"location":"docsify_docs/docsify/HISTORY/#132-2016-12-22","title":"1.3.2 (2016-12-22)","text":""},{"location":"docsify_docs/docsify/HISTORY/#131-2016-12-22","title":"1.3.1 (2016-12-22)","text":""},{"location":"docsify_docs/docsify/HISTORY/#130-2016-12-21","title":"1.3.0 (2016-12-21)","text":""},{"location":"docsify_docs/docsify/HISTORY/#120-2016-12-20","title":"1.2.0 (2016-12-20)","text":""},{"location":"docsify_docs/docsify/HISTORY/#117-2016-12-19","title":"1.1.7 (2016-12-19)","text":""},{"location":"docsify_docs/docsify/HISTORY/#116-2016-12-18","title":"1.1.6 (2016-12-18)","text":""},{"location":"docsify_docs/docsify/HISTORY/#115-2016-12-18","title":"1.1.5 (2016-12-18)","text":""},{"location":"docsify_docs/docsify/HISTORY/#114-2016-12-17","title":"1.1.4 (2016-12-17)","text":""},{"location":"docsify_docs/docsify/HISTORY/#113-2016-12-17","title":"1.1.3 (2016-12-17)","text":""},{"location":"docsify_docs/docsify/HISTORY/#112-2016-12-17","title":"1.1.2 (2016-12-17)","text":""},{"location":"docsify_docs/docsify/HISTORY/#111-2016-12-17","title":"1.1.1 (2016-12-17)","text":""},{"location":"docsify_docs/docsify/HISTORY/#110-2016-12-16","title":"1.1.0 (2016-12-16)","text":""},{"location":"docsify_docs/docsify/HISTORY/#103-2016-12-13","title":"1.0.3 (2016-12-13)","text":""},{"location":"docsify_docs/docsify/HISTORY/#102-2016-12-13","title":"1.0.2 (2016-12-13)","text":""},{"location":"docsify_docs/docsify/HISTORY/#101-2016-12-08","title":"1.0.1 (2016-12-08)","text":""},{"location":"docsify_docs/docsify/HISTORY/#100-2016-12-08","title":"1.0.0 (2016-12-08)","text":""},{"location":"docsify_docs/docsify/HISTORY/#070-2016-11-30","title":"0.7.0 (2016-11-30)","text":""},{"location":"docsify_docs/docsify/HISTORY/#061-2016-11-29","title":"0.6.1 (2016-11-29)","text":""},{"location":"docsify_docs/docsify/HISTORY/#060-2016-11-29","title":"0.6.0 (2016-11-29)","text":""},{"location":"docsify_docs/docsify/HISTORY/#050-2016-11-28","title":"0.5.0 (2016-11-28)","text":""},{"location":"docsify_docs/docsify/HISTORY/#042-2016-11-28","title":"0.4.2 (2016-11-28)","text":""},{"location":"docsify_docs/docsify/HISTORY/#041-2016-11-28","title":"0.4.1 (2016-11-28)","text":""},{"location":"docsify_docs/docsify/HISTORY/#040-2016-11-27","title":"0.4.0 (2016-11-27)","text":""},{"location":"docsify_docs/docsify/HISTORY/#features_17","title":"Features","text":"<ul> <li>custom sidebar, #4 (#5) (37e7984)</li> </ul>"},{"location":"docsify_docs/docsify/HISTORY/#031-2016-11-27","title":"0.3.1 (2016-11-27)","text":""},{"location":"docsify_docs/docsify/HISTORY/#030-2016-11-27","title":"0.3.0 (2016-11-27)","text":""},{"location":"docsify_docs/docsify/HISTORY/#021-2016-11-26","title":"0.2.1 (2016-11-26)","text":""},{"location":"docsify_docs/docsify/HISTORY/#020-2016-11-26","title":"0.2.0 (2016-11-26)","text":""},{"location":"docsify_docs/docsify/HISTORY/#010-2016-11-26","title":"0.1.0 (2016-11-26)","text":""},{"location":"docsify_docs/docsify/HISTORY/#005-2016-11-24","title":"0.0.5 (2016-11-24)","text":""},{"location":"docsify_docs/docsify/HISTORY/#004-2016-11-22","title":"0.0.4 (2016-11-22)","text":""},{"location":"docsify_docs/docsify/HISTORY/#003-2016-11-20","title":"0.0.3 (2016-11-20)","text":""},{"location":"docsify_docs/docsify/HISTORY/#002-2016-11-20","title":"0.0.2 (2016-11-20)","text":""},{"location":"docsify_docs/docsify/HISTORY/#001-2016-11-20","title":"0.0.1 (2016-11-20)","text":""},{"location":"docsify_docs/docsify/SECURITY/","title":"Security Policy","text":"<p>If you believe you have found a security vulnerability in docsify, please report it to us asap.</p>"},{"location":"docsify_docs/docsify/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>Please do not report security vulnerabilities through our public GitHub issues.</p> <p>Send email to us via :email: maintainers@docsifyjs.org.</p> <p>Please include as much of the following information as possible to help us better understand the possible issue:</p> <ul> <li>Type of issue (e.g. cross-site scripting)</li> <li>Full paths of source file(s) related to the manifestation of the issue</li> <li>The location of the affected source code (tag/branch/commit or direct URL)</li> <li>Any special configuration required to reproduce the issue</li> <li>Step-by-step instructions to reproduce the issue</li> <li>Proof-of-concept or exploit code</li> <li>Impact of the issue, including how an attacker might exploit the issue</li> </ul> <p>This information will help us triage your report more quickly.</p> <p>Thank you in advance.</p>"},{"location":"docsify_docs/docsify/docs/","title":"docsify","text":"<p>A magical documentation site generator.</p>"},{"location":"docsify_docs/docsify/docs/#what-it-is","title":"What it is","text":"<p>Docsify generates your documentation website on the fly. Unlike GitBook, it does not generate static html files. Instead, it smartly loads and parses your Markdown files and displays them as a website. To start using it, all you need to do is create an <code>index.html</code> and deploy it on GitHub Pages.</p> <p>See the Quick start guide for more details.</p>"},{"location":"docsify_docs/docsify/docs/#features","title":"Features","text":"<ul> <li>No statically built html files</li> <li>Simple and lightweight</li> <li>Smart full-text search plugin</li> <li>Multiple themes</li> <li>Useful plugin API</li> <li>Emoji support</li> </ul>"},{"location":"docsify_docs/docsify/docs/#examples","title":"Examples","text":"<p>Check out the Showcase to see docsify in use.</p>"},{"location":"docsify_docs/docsify/docs/#donate","title":"Donate","text":"<p>Please consider donating if you think docsify is helpful to you or that my work is valuable. I am happy if you can help me buy a cup of coffee. :heart:</p>"},{"location":"docsify_docs/docsify/docs/#gold-sponsors","title":"Gold sponsors","text":""},{"location":"docsify_docs/docsify/docs/#community","title":"Community","text":"<p>Users and the development team are usually in the Discord server.</p>"},{"location":"docsify_docs/docsify/docs/_coverpage/","title":"coverpage","text":""},{"location":"docsify_docs/docsify/docs/_coverpage/#docsify-4130","title":"docsify 4.13.0","text":"<p>A magical documentation site generator</p> <ul> <li>Simple and lightweight</li> <li>No statically built HTML files</li> <li>Multiple themes</li> </ul> <p>Get Started GitHub</p>"},{"location":"docsify_docs/docsify/docs/_navbar/","title":"navbar","text":"<ul> <li> <p>Translations</p> </li> <li> <p>English</p> </li> <li>\u7b80\u4f53\u4e2d\u6587</li> <li>Deutsch</li> <li>Espa\u00f1ol</li> <li>\u0420\u0443\u0441\u0441\u043a\u0438\u0439</li> </ul>"},{"location":"docsify_docs/docsify/docs/_sidebar/","title":"sidebar","text":"<ul> <li> <p>Getting started</p> </li> <li> <p>Quick start</p> </li> <li>Adding pages</li> <li>Cover page</li> <li> <p>Custom navbar</p> </li> <li> <p>Customization</p> </li> <li> <p>Configuration</p> </li> <li>Themes</li> <li>List of Plugins</li> <li>Write a Plugin</li> <li>Markdown configuration</li> <li>Language highlighting</li> <li> <p>Emoji</p> </li> <li> <p>Guide</p> </li> <li> <p>Deploy</p> </li> <li>Helpers</li> <li>Vue compatibility</li> <li>CDN</li> <li>Offline Mode (PWA)</li> <li>Embed Files</li> <li> <p>UI Kit</p> </li> <li> <p>Awesome docsify</p> </li> <li>Changelog</li> </ul>"},{"location":"docsify_docs/docsify/docs/adding-pages/","title":"Adding pages","text":"<p>If you need more pages, you can simply create more markdown files in your docsify directory. If you create a file named <code>guide.md</code>, then it is accessible via <code>/#/guide</code>.</p> <p>For example, the directory structure is as follows:</p> <pre><code>.\n\u2514\u2500\u2500 docs\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 guide.md\n    \u2514\u2500\u2500 zh-cn\n        \u251c\u2500\u2500 README.md\n        \u2514\u2500\u2500 guide.md\n</code></pre> <p>Matching routes</p> <pre><code>docs/README.md        =&gt; http://domain.com\ndocs/guide.md         =&gt; http://domain.com/#/guide\ndocs/zh-cn/README.md  =&gt; http://domain.com/#/zh-cn/\ndocs/zh-cn/guide.md   =&gt; http://domain.com/#/zh-cn/guide\n</code></pre>"},{"location":"docsify_docs/docsify/docs/adding-pages/#sidebar","title":"Sidebar","text":"<p>In order to have a sidebar, you can create your own <code>_sidebar.md</code> (see this documentation's sidebar for an example):</p> <p>First, you need to set <code>loadSidebar</code> to true. Details are available in the configuration paragraph.</p> <pre><code>&lt;!-- index.html --&gt;\n\n&lt;script&gt;\n  window.$docsify = {\n    loadSidebar: true,\n  };\n&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>Create the <code>_sidebar.md</code>:</p> <pre><code>&lt;!-- docs/_sidebar.md --&gt;\n\n- [Home](/)\n- [Page 1](page-1.md)\n</code></pre> <p>To create section headers:</p> <pre><code>&lt;!-- docs/_sidebar.md --&gt;\n\n- Section Header 1\n\n  - [Home](/)\n  - [Page 1](page-1.md)\n\n- Section Header 2\n\n  - [Page 2](page-2.md)\n  - [Page 3](page-3.md)\n</code></pre> <p>You need to create a <code>.nojekyll</code> in <code>./docs</code> to prevent GitHub Pages from ignoring files that begin with an underscore.</p> <p>!&gt; Docsify only looks for <code>_sidebar.md</code> in the current folder, and uses that, otherwise it falls back to the one configured using <code>window.$docsify.loadSidebar</code> config.</p> <p>Example file structure:</p> <pre><code>\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 _sidebar.md\n    \u251c\u2500\u2500 index.md\n    \u251c\u2500\u2500 getting-started.md\n    \u2514\u2500\u2500 running-services.md\n</code></pre>"},{"location":"docsify_docs/docsify/docs/adding-pages/#nested-sidebars","title":"Nested Sidebars","text":"<p>You may want the sidebar to update after navigation to reflect the current directory. This can be done by adding a <code>_sidebar.md</code> file to each folder.</p> <p><code>_sidebar.md</code> is loaded from each level directory. If the current directory doesn't have <code>_sidebar.md</code>, it will fall back to the parent directory. If, for example, the current path is <code>/guide/quick-start</code>, the <code>_sidebar.md</code> will be loaded from <code>/guide/_sidebar.md</code>.</p> <p>You can specify <code>alias</code> to avoid unnecessary fallback.</p> <pre><code>&lt;script&gt;\n  window.$docsify = {\n    loadSidebar: true,\n    alias: {\n      '/.*/_sidebar.md': '/_sidebar.md',\n    },\n  };\n&lt;/script&gt;\n</code></pre> <p>!&gt; You can create a <code>README.md</code> file in a subdirectory to use it as the landing page for the route.</p>"},{"location":"docsify_docs/docsify/docs/adding-pages/#set-page-titles-from-sidebar-selection","title":"Set Page Titles from Sidebar Selection","text":"<p>A page's <code>title</code> tag is generated from the selected sidebar item name. For better SEO, you can customize the title by specifying a string after the filename.</p> <pre><code>&lt;!-- docs/_sidebar.md --&gt;\n\n- [Home](/)\n- [Guide](guide.md 'The greatest guide in the world')\n</code></pre>"},{"location":"docsify_docs/docsify/docs/adding-pages/#table-of-contents","title":"Table of Contents","text":"<p>Once you've created <code>_sidebar.md</code>, the sidebar content is automatically generated based on the headers in the markdown files.</p> <p>A custom sidebar can also automatically generate a table of contents by setting a <code>subMaxLevel</code>, compare subMaxLevel configuration.</p> <pre><code>&lt;!-- index.html --&gt;\n\n&lt;script&gt;\n  window.$docsify = {\n    loadSidebar: true,\n    subMaxLevel: 2,\n  };\n&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/adding-pages/#ignoring-subheaders","title":"Ignoring Subheaders","text":"<p>When <code>subMaxLevel</code> is set, each header is automatically added to the table of contents by default. If you want to ignore a specific header, add <code>&lt;!-- {docsify-ignore} --&gt;</code> to it.</p> <pre><code># Getting Started\n\n## Header &lt;!-- {docsify-ignore} --&gt;\n\nThis header won't appear in the sidebar table of contents.\n</code></pre> <p>To ignore all headers on a specific page, you can use <code>&lt;!-- {docsify-ignore-all} --&gt;</code> on the first header of the page.</p> <pre><code># Getting Started &lt;!-- {docsify-ignore-all} --&gt;\n\n## Header\n\nThis header won't appear in the sidebar table of contents.\n</code></pre> <p>Both <code>&lt;!-- {docsify-ignore} --&gt;</code> and <code>&lt;!-- {docsify-ignore-all} --&gt;</code> will not be rendered on the page when used.</p> <p>And the <code>{docsify-ignore}</code> and <code>{docsify-ignore-all}</code> can do the samething as well.</p>"},{"location":"docsify_docs/docsify/docs/cdn/","title":"CDN","text":"<p>The docsify npm package is auto-published to CDNs with each release. The contents can be viewed on each CDN.</p> <p>Docsify recommends jsDelivr as its preferred CDN:</p> <ul> <li>https://cdn.jsdelivr.net/npm/docsify/</li> </ul> <p>Other CDNs are available and may be required in locations where jsDelivr is not available:</p> <ul> <li>https://cdnjs.com/libraries/docsify</li> <li>https://unpkg.com/browse/docsify/</li> <li>https://www.bootcdn.cn/docsify/</li> </ul>"},{"location":"docsify_docs/docsify/docs/cdn/#specifying-versions","title":"Specifying versions","text":"<p>Note the <code>@</code> version lock in the CDN URLs below. This allows specifying the latest major, minor, patch, or specific semver version number.</p> <ul> <li>MAJOR versions include breaking changes <code>1.0.0</code> \u2192 <code>2.0.0</code></li> <li>MINOR versions include non-breaking new functionality <code>1.0.0</code> \u2192 <code>1.1.0</code></li> <li>PATCH versions include non-breaking bug fixes <code>1.0.0</code> \u2192 <code>1.0.1</code></li> </ul> <p>Uncompressed resources are available by omitting the <code>.min</code> from the filename.</p>"},{"location":"docsify_docs/docsify/docs/cdn/#latest-major-version","title":"Latest \"major\" version","text":"<p>Specifying the latest major version allows your site to receive all non-breaking enhancements (\"minor\" updates) and bug fixes (\"patch\" updates) as they are released. This is good option for those who prefer a zero-maintenance way of keeping their site up to date with minimal risk as new versions are published.</p> <p>?&gt; When a new major version is released, you will need to manually update the major version number after the <code>@</code> symbol in your CDN URLs.</p> <pre><code>&lt;!-- Theme --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5/themes/vue.min.css\" /&gt;\n\n&lt;!-- Docsify --&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/cdn/#specific-version","title":"Specific version","text":"<p>Specifying an exact version prevents any future updates from affecting your site. This is good option for those who prefer to manually update their resources as new versions are published.</p> <p>?&gt; When a new version is released, you will need to manually update the version number after the <code>@</code> symbol in your CDN URLs.</p> <pre><code>&lt;!-- Theme --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5.0.0/themes/vue.min.css\" /&gt;\n\n&lt;!-- Docsify --&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5.0.0/dist/docsify.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/","title":"Configuration","text":"<p>You can configure Docsify by defining <code>window.$docsify</code> as an object:</p> <pre><code>&lt;script&gt;\n  window.$docsify = {\n    repo: 'docsifyjs/docsify',\n    maxLevel: 3,\n    coverpage: true,\n  };\n&lt;/script&gt;\n</code></pre> <p>The config can also be defined as a function, in which case the first argument is the Docsify <code>vm</code> instance. The function should return a config object. This can be useful for referencing <code>vm</code> in places like the markdown configuration:</p> <pre><code>&lt;script&gt;\n  window.$docsify = function (vm) {\n    return {\n      markdown: {\n        renderer: {\n          code(code, lang) {\n            // ... use `vm` ...\n          },\n        },\n      },\n    };\n  };\n&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#alias","title":"alias","text":"<ul> <li>Type: <code>Object</code></li> </ul> <p>Set the route alias. You can freely manage routing rules. Supports RegExp. Do note that order matters! If a route can be matched by multiple aliases, the one you declared first takes precedence.</p> <pre><code>window.$docsify = {\n  alias: {\n    '/foo/(.*)': '/bar/$1', // supports regexp\n    '/zh-cn/changelog': '/changelog',\n    '/changelog':\n      'https://raw.githubusercontent.com/docsifyjs/docsify/master/CHANGELOG',\n\n    // You may need this if you use routerMode:'history'.\n    '/.*/_sidebar.md': '/_sidebar.md', // See #301\n  },\n};\n</code></pre> <p>Note If you change <code>routerMode</code> to <code>'history'</code>, you may want to configure an alias for your <code>_sidebar.md</code> and <code>_navbar.md</code> files.</p>"},{"location":"docsify_docs/docsify/docs/configuration/#auto2top","title":"auto2top","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>false</code></li> </ul> <p>Scrolls to the top of the screen when the route is changed.</p> <pre><code>window.$docsify = {\n  auto2top: true,\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#autoheader","title":"autoHeader","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>false</code></li> </ul> <p>If <code>loadSidebar</code> and <code>autoHeader</code> are both enabled, for each link in <code>_sidebar.md</code>, prepend a header to the page before converting it to HTML. See #78.</p> <pre><code>window.$docsify = {\n  loadSidebar: true,\n  autoHeader: true,\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#basepath","title":"basePath","text":"<ul> <li>Type: <code>String</code></li> </ul> <p>Base path of the website. You can set it to another directory or another domain name.</p> <pre><code>window.$docsify = {\n  basePath: '/path/',\n\n  // Load the files from another site\n  basePath: 'https://docsify.js.org/',\n\n  // Even can load files from other repo\n  basePath:\n    'https://raw.githubusercontent.com/ryanmcdermott/clean-code-javascript/master/',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#catchpluginerrors","title":"catchPluginErrors","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>true</code></li> </ul> <p>Determines if Docsify should handle uncaught synchronous plugin errors automatically. This can prevent plugin errors from affecting docsify's ability to properly render live site content.</p>"},{"location":"docsify_docs/docsify/docs/configuration/#cornerexternallinktarget","title":"cornerExternalLinkTarget","text":"<ul> <li>Type: <code>String</code></li> <li>Default: <code>'_blank'</code></li> </ul> <p>Target to open external link at the top right corner. Default <code>'_blank'</code> (new window/tab)</p> <pre><code>window.$docsify = {\n  cornerExternalLinkTarget: '_self', // default: '_blank'\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#coverpage","title":"coverpage","text":"<ul> <li>Type: <code>Boolean|String|String[]|Object</code></li> <li>Default: <code>false</code></li> </ul> <p>Activate the cover feature. If true, it will load from <code>_coverpage.md</code>.</p> <pre><code>window.$docsify = {\n  coverpage: true,\n\n  // Custom file name\n  coverpage: 'cover.md',\n\n  // multiple covers\n  coverpage: ['/', '/zh-cn/'],\n\n  // multiple covers and custom file name\n  coverpage: {\n    '/': 'cover.md',\n    '/zh-cn/': 'cover.md',\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#el","title":"el","text":"<ul> <li>Type: <code>String</code></li> <li>Default: <code>'#app'</code></li> </ul> <p>The DOM element to be mounted on initialization. It can be a CSS selector string or an actual HTMLElement.</p> <pre><code>window.$docsify = {\n  el: '#app',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#executescript","title":"executeScript","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>null</code></li> </ul> <p>Execute the script on the page. Only parses the first script tag (demo). If Vue is detected, this is <code>true</code> by default.</p> <pre><code>window.$docsify = {\n  executeScript: true,\n};\n</code></pre> <pre><code>## This is test\n\n&lt;script&gt;\n  console.log(2333)\n&lt;/script&gt;\n</code></pre> <p>Note that if you are running an external script, e.g. an embedded jsfiddle demo, make sure to include the external-script plugin.</p>"},{"location":"docsify_docs/docsify/docs/configuration/#ext","title":"ext","text":"<ul> <li>Type: <code>String</code></li> <li>Default: <code>'.md'</code></li> </ul> <p>Request file extension.</p> <pre><code>window.$docsify = {\n  ext: '.md',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#externallinkrel","title":"externalLinkRel","text":"<ul> <li>Type: <code>String</code></li> <li>Default: <code>'noopener'</code></li> </ul> <p>Default <code>'noopener'</code> (no opener) prevents the newly opened external page (when externalLinkTarget is <code>'_blank'</code>) from having the ability to control our page. No <code>rel</code> is set when it's not <code>'_blank'</code>. See this post for more information about why you may want to use this option.</p> <pre><code>window.$docsify = {\n  externalLinkRel: '', // default: 'noopener'\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#externallinktarget","title":"externalLinkTarget","text":"<ul> <li>Type: <code>String</code></li> <li>Default: <code>'_blank'</code></li> </ul> <p>Target to open external links inside the markdown. Default <code>'_blank'</code> (new window/tab)</p> <pre><code>window.$docsify = {\n  externalLinkTarget: '_self', // default: '_blank'\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#fallbacklanguages","title":"fallbackLanguages","text":"<ul> <li>Type: <code>Array&lt;string&gt;</code></li> </ul> <p>List of languages that will fallback to the default language when a page is requested and it doesn't exist for the given locale.</p> <p>Example:</p> <ul> <li>try to fetch the page of <code>/de/overview</code>. If this page exists, it'll be displayed.</li> <li>then try to fetch the default page <code>/overview</code> (depending on the default language). If this page exists, it'll be displayed.</li> <li>then display the 404 page.</li> </ul> <pre><code>window.$docsify = {\n  fallbackLanguages: ['fr', 'de'],\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#formatupdated","title":"formatUpdated","text":"<ul> <li>Type: <code>String|Function</code></li> </ul> <p>We can display the file update date through {docsify-updated} variable. And format it by <code>formatUpdated</code>. See https://github.com/lukeed/tinydate#patterns</p> <pre><code>window.$docsify = {\n  formatUpdated: '{MM}/{DD} {HH}:{mm}',\n\n  formatUpdated(time) {\n    // ...\n\n    return time;\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#hidesidebar","title":"hideSidebar","text":"<ul> <li>Type : <code>Boolean</code></li> <li>Default: <code>true</code></li> </ul> <p>This option will completely hide your sidebar and won't render any content on the side.</p> <pre><code>window.$docsify = {\n  hideSidebar: true,\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#homepage","title":"homepage","text":"<ul> <li>Type: <code>String</code></li> <li>Default: <code>'README.md'</code></li> </ul> <p><code>README.md</code> in your docs folder will be treated as the homepage for your website, but sometimes you may need to serve another file as your homepage.</p> <pre><code>window.$docsify = {\n  // Change to /home.md\n  homepage: 'home.md',\n\n  // Or use the readme in your repo\n  homepage:\n    'https://raw.githubusercontent.com/docsifyjs/docsify/master/README.md',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#keybindings","title":"keyBindings","text":"<ul> <li>Type: <code>Boolean|Object</code></li> <li>Default: <code>Object</code></li> <li>\\ Toggle the sidebar menu</li> <li>/ Focus on search field. Also supports alt\u00a0/\u00a0ctrl\u00a0+\u00a0k.</li> </ul> <p>Binds key combination(s) to a custom callback function.</p> <p>Key <code>bindings</code> are defined as case insensitive string values separated by <code>+</code>. Modifier key values include <code>alt</code>, <code>ctrl</code>, <code>meta</code>, and <code>shift</code>. Non-modifier key values should match the keyboard event's key or code value.</p> <p>The <code>callback</code> function receive a keydown event as an argument.</p> <p>!&gt; Let site visitors know your custom key bindings are available! If a binding is associated with a DOM element, consider inserting a <code>&lt;kbd&gt;</code> element as a visual cue (e.g., alt + a) or adding title and aria-keyshortcuts attributes for hover/focus hints.</p> <pre><code>window.$docsify = {\n  keyBindings: {\n    // Custom key binding\n    myCustomBinding: {\n      bindings: ['alt+a', 'shift+a'],\n      callback(event) {\n        alert('Hello, World!');\n      },\n    },\n  },\n};\n</code></pre> <p>Key bindings can be disabled entirely or individually by setting the binding configuration to <code>false</code>.</p> <pre><code>window.$docsify = {\n  // Disable all key bindings\n  keyBindings: false,\n};\n</code></pre> <pre><code>window.$docsify = {\n  keyBindings: {\n    // Disable individual key bindings\n    focusSearch: false,\n    toggleSidebar: false,\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#loadnavbar","title":"loadNavbar","text":"<ul> <li>Type: <code>Boolean|String</code></li> <li>Default: <code>false</code></li> </ul> <p>Loads navbar from the Markdown file <code>_navbar.md</code> if true, else loads it from the path specified.</p> <pre><code>window.$docsify = {\n  // load from _navbar.md\n  loadNavbar: true,\n\n  // load from nav.md\n  loadNavbar: 'nav.md',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#loadsidebar","title":"loadSidebar","text":"<ul> <li>Type: <code>Boolean|String</code></li> <li>Default: <code>false</code></li> </ul> <p>Loads sidebar from the Markdown file <code>_sidebar.md</code> if true, else loads it from the path specified.</p> <pre><code>window.$docsify = {\n  // load from _sidebar.md\n  loadSidebar: true,\n\n  // load from summary.md\n  loadSidebar: 'summary.md',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#logo","title":"logo","text":"<ul> <li>Type: <code>String</code></li> </ul> <p>Website logo as it appears in the sidebar. You can resize it using CSS.</p> <p>!&gt; Logo will only be visible if <code>name</code> prop is also set. See name configuration.</p> <pre><code>window.$docsify = {\n  logo: '/_media/icon.svg',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#markdown","title":"markdown","text":"<ul> <li>Type: <code>Function</code></li> </ul> <p>See Markdown configuration.</p> <pre><code>window.$docsify = {\n  // object\n  markdown: {\n    smartypants: true,\n    renderer: {\n      link() {\n        // ...\n      },\n    },\n  },\n\n  // function\n  markdown(marked, renderer) {\n    // ...\n    return marked;\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#maxlevel","title":"maxLevel","text":"<ul> <li>Type: <code>Number</code></li> <li>Default: <code>6</code></li> </ul> <p>Maximum Table of content level.</p> <pre><code>window.$docsify = {\n  maxLevel: 4,\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#mergenavbar","title":"mergeNavbar","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>false</code></li> </ul> <p>Navbar will be merged with the sidebar on smaller screens.</p> <pre><code>window.$docsify = {\n  mergeNavbar: true,\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#name","title":"name","text":"<ul> <li>Type: <code>String</code></li> </ul> <p>Website name as it appears in the sidebar.</p> <pre><code>window.$docsify = {\n  name: 'docsify',\n};\n</code></pre> <p>The name field can also contain custom HTML for easier customization:</p> <pre><code>window.$docsify = {\n  name: '&lt;span&gt;docsify&lt;/span&gt;',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#namelink","title":"nameLink","text":"<ul> <li>Type: <code>String</code></li> <li>Default: <code>'window.location.pathname'</code></li> </ul> <p>The URL that the website <code>name</code> links to.</p> <pre><code>window.$docsify = {\n  nameLink: '/',\n\n  // For each route\n  nameLink: {\n    '/zh-cn/': '#/zh-cn/',\n    '/': '#/',\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#nativeemoji","title":"nativeEmoji","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>false</code></li> </ul> <p>Render emoji shorthand codes using GitHub-style emoji images or platform-native emoji characters.</p> <pre><code>window.$docsify = {\n  nativeEmoji: true,\n};\n</code></pre> <pre><code>:smile:\n:partying_face:\n:joy:\n:+1:\n:-1:\n</code></pre> <p>GitHub-style images when <code>false</code>:</p> <p>Platform-native characters when <code>true</code>:</p> \ud83d\ude04\ufe0e \ud83e\udd73\ufe0e \ud83d\ude02\ufe0e \ud83d\udc4d\ufe0e \ud83d\udc4e\ufe0e <p>To render shorthand codes as text, replace <code>:</code> characters with the <code>&amp;colon;</code> HTML entity.</p> <pre><code>&amp;colon;100&amp;colon;\n</code></pre>   :100:"},{"location":"docsify_docs/docsify/docs/configuration/#nocompilelinks","title":"noCompileLinks","text":"<ul> <li>Type: <code>Array&lt;string&gt;</code></li> </ul> <p>Sometimes we do not want docsify to handle our links. See #203. We can skip compiling of certain links by specifying an array of strings. Each string is converted into to a regular expression (<code>RegExp</code>) and the whole href of a link is matched against it.</p> <pre><code>window.$docsify = {\n  noCompileLinks: ['/foo', '/bar/.*'],\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#noemoji","title":"noEmoji","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>false</code></li> </ul> <p>Disabled emoji parsing and render all emoji shorthand as text.</p> <pre><code>window.$docsify = {\n  noEmoji: true,\n};\n</code></pre> <pre><code>:100:\n</code></pre>   :100:   <p>To disable emoji parsing of individual shorthand codes, replace <code>:</code> characters with the <code>&amp;colon;</code> HTML entity.</p> <pre><code>:100:\n\n&amp;colon;100&amp;colon;\n</code></pre>   :100:  :100:"},{"location":"docsify_docs/docsify/docs/configuration/#notfoundpage","title":"notFoundPage","text":"<ul> <li>Type: <code>Boolean</code> | <code>String</code> | <code>Object</code></li> <li>Default: <code>false</code></li> </ul> <p>Display default \"404 - Not Found\" message:</p> <pre><code>window.$docsify = {\n  notFoundPage: false,\n};\n</code></pre> <p>Load the <code>_404.md</code> file:</p> <pre><code>window.$docsify = {\n  notFoundPage: true,\n};\n</code></pre> <p>Load the customized path of the 404 page:</p> <pre><code>window.$docsify = {\n  notFoundPage: 'my404.md',\n};\n</code></pre> <p>Load the right 404 page according to the localization:</p> <pre><code>window.$docsify = {\n  notFoundPage: {\n    '/': '_404.md',\n    '/de': 'de/_404.md',\n  },\n};\n</code></pre> <p>Note: The options for fallbackLanguages don't work with the <code>notFoundPage</code> options.</p>"},{"location":"docsify_docs/docsify/docs/configuration/#onlycover","title":"onlyCover","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>false</code></li> </ul> <p>Only coverpage is loaded when visiting the home page.</p> <pre><code>window.$docsify = {\n  onlyCover: false,\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#relativepath","title":"relativePath","text":"<ul> <li>Type: <code>Boolean</code></li> <li>Default: <code>false</code></li> </ul> <p>If true, links are relative to the current context.</p> <p>For example, the directory structure is as follows:</p> <pre><code>.\n\u2514\u2500\u2500 docs\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 guide.md\n    \u2514\u2500\u2500 zh-cn\n        \u251c\u2500\u2500 README.md\n        \u251c\u2500\u2500 guide.md\n        \u2514\u2500\u2500 config\n            \u2514\u2500\u2500 example.md\n</code></pre> <p>With relative path enabled and current URL <code>http://domain.com/zh-cn/README</code>, given links will resolve to:</p> <pre><code>guide.md              =&gt; http://domain.com/zh-cn/guide\nconfig/example.md     =&gt; http://domain.com/zh-cn/config/example\n../README.md          =&gt; http://domain.com/README\n/README.md            =&gt; http://domain.com/README\n</code></pre> <pre><code>window.$docsify = {\n  // Relative path enabled\n  relativePath: true,\n\n  // Relative path disabled (default value)\n  relativePath: false,\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#repo","title":"repo","text":"<ul> <li>Type: <code>String</code></li> </ul> <p>Configure the repository url, or a string of <code>username/repo</code>, to add the GitHub Corner widget in the top right corner of the site.</p> <pre><code>window.$docsify = {\n  repo: 'docsifyjs/docsify',\n  // or\n  repo: 'https://github.com/docsifyjs/docsify/',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#requestheaders","title":"requestHeaders","text":"<ul> <li>Type: <code>Object</code></li> </ul> <p>Set the request resource headers.</p> <pre><code>window.$docsify = {\n  requestHeaders: {\n    'x-token': 'xxx',\n  },\n};\n</code></pre> <p>Such as setting the cache</p> <pre><code>window.$docsify = {\n  requestHeaders: {\n    'cache-control': 'max-age=600',\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#routermode","title":"routerMode","text":"<p>Configure the URL format that the paths of your site will use.</p> <ul> <li>Type: <code>String</code></li> <li>Default: <code>'hash'</code></li> </ul> <pre><code>window.$docsify = {\n  routerMode: 'history', // default: 'hash'\n};\n</code></pre> <p>For statically-deployed sites (f.e. on GitHub Pages) hash-based routing is simpler to set up. For websites that can re-write URLs, the history-based format is better (especially for search-engine optimization, hash-based routing is not so search-engine friendly)</p> <p>Hash-based routing means all URL paths will be prefixed with <code>/#/</code> in the address bar. This is a trick that allows the site to load <code>/index.html</code>, then it uses the path that follows the <code>#</code> to determine what markdown files to load. For example, a complete hash-based URL may look like this: <code>https://example.com/#/path/to/page</code>. The browser will actually load <code>https://example.com</code> (assuming your static server serves <code>index.html</code> by default, as most do), and then the Docsify JavaScript code will look at the <code>/#/...</code> and determine the markdown file to load and render. Additionally, when clicking on a link, the Docsify router will change the content after the hash dynamically. The value of <code>location.pathname</code> will still be <code>/</code> no matter what. The parts of a hash path are not sent to the server when visiting such a URL in a browser.</p> <p>On the other hand, history-based routing means the Docsify JavaScript will use the History API to dynamically change the URL without using a <code>#</code>. This means that all URLs will be considered \"real\" by search engines, and the full path will be sent to the server when visiting the URL in your browser. For example, a URL may look like <code>https://example.com/path/to/page</code>. The browser will try to load that full URL directly from the server, not just <code>https://example.com</code>. The upside of this is that these types of URLs are much more friendly for search engines, and can be indexed (yay!). The downside, however, is that your server, or the place where you host your site files, has to be able to handle these URLs. Various static website hosting services allow \"rewrite rules\" to be configured, such that a server can be configured to always send back <code>/index.html</code> no matter what path is visited. The value of <code>location.pathname</code> will show <code>/path/to/page</code>, because it was actually sent to the server.</p> <p>TLDR: start with <code>hash</code> routing (the default). If you feel adventurous, learn how to configure a server, then switch to <code>history</code> mode for better experience without the <code>#</code> in the URL and SEO optimization.</p> <p>Note If you use <code>routerMode: 'history'</code>, you may want to add an <code>alias</code> to make your <code>_sidebar.md</code> and <code>_navbar.md</code> files always be loaded no matter which path is being visited.</p> <pre><code>window.$docsify = {\n  routerMode: 'history',\n  alias: {\n    '/.*/_sidebar.md': '/_sidebar.md',\n    '/.*/_navbar.md': '/_navbar.md',\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#routes","title":"routes","text":"<ul> <li>Type: <code>Object</code></li> </ul> <p>Define \"virtual\" routes that can provide content dynamically. A route is a map between the expected path, to either a string or a function. If the mapped value is a string, it is treated as markdown and parsed accordingly. If it is a function, it is expected to return markdown content.</p> <p>A route function receives up to three parameters:</p> <ol> <li><code>route</code> - the path of the route that was requested (e.g. <code>/bar/baz</code>)</li> <li><code>matched</code> - the <code>RegExpMatchArray</code> that was matched by the route (e.g. for <code>/bar/(.+)</code>, you get <code>['/bar/baz', 'baz']</code>)</li> <li><code>next</code> - this is a callback that you may call when your route function is async</li> </ol> <p>Do note that order matters! Routes are matched the same order you declare them in, which means that in cases where you have overlapping routes, you might want to list the more specific ones first.</p> <pre><code>window.$docsify = {\n  routes: {\n    // Basic match w/ return string\n    '/foo': '# Custom Markdown',\n\n    // RegEx match w/ synchronous function\n    '/bar/(.*)'(route, matched) {\n      return '# Custom Markdown';\n    },\n\n    // RegEx match w/ asynchronous function\n    '/baz/(.*)'(route, matched, next) {\n      fetch('/api/users?id=12345')\n        .then(response =&gt; {\n          next('# Custom Markdown');\n        })\n        .catch(err =&gt; {\n          // Handle error...\n        });\n    },\n  },\n};\n</code></pre> <p>Other than strings, route functions can return a falsy value (<code>null</code> \\ <code>undefined</code>) to indicate that they ignore the current request:</p> <pre><code>window.$docsify = {\n  routes: {\n    // accepts everything other than dogs (synchronous)\n    '/pets/(.+)'(route, matched) {\n      if (matched[0] === 'dogs') {\n        return null;\n      } else {\n        return 'I like all pets but dogs';\n      }\n    }\n\n    // accepts everything other than cats (asynchronous)\n    '/pets/(.*)'(route, matched, next) {\n      if (matched[0] === 'cats') {\n        next();\n      } else {\n        // Async task(s)...\n        next('I like all pets but cats');\n      }\n    }\n  }\n}\n</code></pre> <p>Finally, if you have a specific path that has a real markdown file (and therefore should not be matched by your route), you can opt it out by returning an explicit <code>false</code> value:</p> <pre><code>window.$docsify = {\n  routes: {\n    // if you look up /pets/cats, docsify will skip all routes and look for \"pets/cats.md\"\n    '/pets/cats'(route, matched) {\n      return false;\n    }\n\n    // but any other pet should generate dynamic content right here\n    '/pets/(.+)'(route, matched) {\n      const pet = matched[0];\n      return `your pet is ${pet} (but not a cat)`;\n    }\n  }\n}\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#skiplink","title":"skipLink","text":"<ul> <li>Type: <code>Boolean|String|Object</code></li> <li>Default: <code>'Skip to main content'</code></li> </ul> <p>Determines if/how the site's skip navigation link will be rendered.</p> <pre><code>// Render skip link for all routes (default)\nwindow.$docsify = {\n  skipLink: 'Skip to main content',\n};\n</code></pre> <pre><code>// Render localized skip links based on route paths\nwindow.$docsify = {\n  skipLink: {\n    '/es/': 'Saltar al contenido principal',\n    '/de-de/': 'Ga naar de hoofdinhoud',\n    '/ru-ru/': '\u041f\u0435\u0440\u0435\u0439\u0442\u0438 \u043a \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c\u0443 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u044e',\n    '/zh-cn/': '\u8df3\u5230\u4e3b\u8981\u5185\u5bb9',\n  },\n};\n</code></pre> <pre><code>// Do not render skip link\nwindow.$docsify = {\n  skipLink: false,\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#submaxlevel","title":"subMaxLevel","text":"<ul> <li>Type: <code>Number</code></li> <li>Default: <code>0</code></li> </ul> <p>Add table of contents (TOC) in custom sidebar.</p> <pre><code>window.$docsify = {\n  subMaxLevel: 2,\n};\n</code></pre> <p>If you have a link to the homepage in the sidebar and want it to be shown as active when accessing the root url, make sure to update your sidebar accordingly:</p> <pre><code>- Sidebar\n  - [Home](/)\n  - [Another page](another.md)\n</code></pre> <p>For more details, see #1131.</p>"},{"location":"docsify_docs/docsify/docs/configuration/#themecolor","title":"themeColor \u26a0\ufe0f","text":"<p>!&gt; Deprecated as of v5. Use the <code>--theme-color</code> theme property to customize your theme color.</p> <ul> <li>Type: <code>String</code></li> </ul> <p>Customize the theme color.</p> <pre><code>window.$docsify = {\n  themeColor: '#3F51B5',\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#topmargin","title":"topMargin \u26a0\ufe0f","text":"<p>!&gt; Deprecated as of v5. Use the <code>--scroll-padding-top</code> theme property to specify a scroll margin when using a sticky navbar.</p> <ul> <li>Type: <code>Number|String</code></li> <li>Default: <code>0</code></li> </ul> <p>Adds scroll padding to the top of the viewport. This is useful when you have added a sticky or \"fixed\" element and would like auto scrolling to align with the bottom of your element.</p> <pre><code>window.$docsify = {\n  topMargin: 90, // 90, '90px', '2rem', etc.\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#vuecomponents","title":"vueComponents","text":"<ul> <li>Type: <code>Object</code></li> </ul> <p>Creates and registers global Vue. Components are specified using the component name as the key with an object containing Vue options as the value. Component <code>data</code> is unique for each instance and will not persist as users navigate the site.</p> <pre><code>window.$docsify = {\n  vueComponents: {\n    'button-counter': {\n      template: `\n        &lt;button @click=\"count += 1\"&gt;\n          You clicked me {{ count }} times\n        &lt;/button&gt;\n      `,\n      data() {\n        return {\n          count: 0,\n        };\n      },\n    },\n  },\n};\n</code></pre> <pre><code>&lt;button-counter&gt;&lt;/button-counter&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/configuration/#vueglobaloptions","title":"vueGlobalOptions","text":"<ul> <li>Type: <code>Object</code></li> </ul> <p>Specifies global Vue options for use with Vue content not explicitly mounted with vueMounts, vueComponents, or a markdown script. Changes to global <code>data</code> will persist and be reflected anywhere global references are used.</p> <pre><code>window.$docsify = {\n  vueGlobalOptions: {\n    data() {\n      return {\n        count: 0,\n      };\n    },\n  },\n};\n</code></pre> <pre><code>&lt;p&gt;\n  &lt;button @click=\"count -= 1\"&gt;-&lt;/button&gt;\n  {{ count }}\n  &lt;button @click=\"count += 1\"&gt;+&lt;/button&gt;\n&lt;/p&gt;\n</code></pre> <p> -     {{ count }}     + </p>"},{"location":"docsify_docs/docsify/docs/configuration/#vuemounts","title":"vueMounts","text":"<ul> <li>Type: <code>Object</code></li> </ul> <p>Specifies DOM elements to mount as Vue instances and their associated options. Mount elements are specified using a CSS selector as the key with an object containing Vue options as their value. Docsify will mount the first matching element in the main content area each time a new page is loaded. Mount element <code>data</code> is unique for each instance and will not persist as users navigate the site.</p> <pre><code>window.$docsify = {\n  vueMounts: {\n    '#counter': {\n      data() {\n        return {\n          count: 0,\n        };\n      },\n    },\n  },\n};\n</code></pre> <pre><code>&lt;div id=\"counter\"&gt;\n  &lt;button @click=\"count -= 1\"&gt;-&lt;/button&gt;\n  {{ count }}\n  &lt;button @click=\"count += 1\"&gt;+&lt;/button&gt;\n&lt;/div&gt;\n</code></pre> -   {{ count }}   +"},{"location":"docsify_docs/docsify/docs/cover/","title":"Cover","text":"<p>Activate the cover feature by setting <code>coverpage</code> to true. See coverpage configuration.</p>"},{"location":"docsify_docs/docsify/docs/cover/#basic-usage","title":"Basic usage","text":"<p>Set <code>coverpage</code> to true, and create a <code>_coverpage.md</code>:</p> <pre><code>window.$docsify = {\n  coverpage: true,\n};\n</code></pre> <pre><code>&lt;!-- _coverpage.md --&gt;\n\n![logo](_media/icon.svg)\n\n# docsify\n\n&gt; A magical documentation site generator\n\n- Simple and lightweight\n- No statically built HTML files\n- Multiple themes\n\n[GitHub](https://github.com/docsifyjs/docsify/)\n[Get Started](#docsify)\n</code></pre>"},{"location":"docsify_docs/docsify/docs/cover/#customization","title":"Customization","text":"<p>The cover page can be customized using theme properties:</p> <pre><code>:root {\n  --cover-bg         : url('path/to/image.png');\n  --cover-bg-overlay : rgba(0, 0, 0, 0.5);\n  --cover-color      : #fff;\n  --cover-title-color: var(--theme-color);\n  --cover-title-font : 600 var(--font-size-xxxl) var(--font-family);\n}\n</code></pre> <p>Alternatively, a background color or image can be specified in the cover page markdown.</p> <pre><code>&lt;!-- background color --&gt;\n\n![color](#f0f0f0)\n</code></pre> <pre><code>&lt;!-- background image --&gt;\n\n![](_media/bg.png)\n</code></pre>"},{"location":"docsify_docs/docsify/docs/cover/#coverpage-as-homepage","title":"Coverpage as homepage","text":"<p>Normally, the coverpage and the homepage appear at the same time. Of course, you can also separate the coverpage by <code>onlyCover</code> option.</p>"},{"location":"docsify_docs/docsify/docs/cover/#multiple-covers","title":"Multiple covers","text":"<p>If your docs site is in more than one language, it may be useful to set multiple covers.</p> <p>For example, your docs structure is like this</p> <pre><code>.\n\u2514\u2500\u2500 docs\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 guide.md\n    \u251c\u2500\u2500 _coverpage.md\n    \u2514\u2500\u2500 zh-cn\n        \u251c\u2500\u2500 README.md\n        \u2514\u2500\u2500 guide.md\n        \u2514\u2500\u2500 _coverpage.md\n</code></pre> <p>Now, you can set</p> <pre><code>window.$docsify = {\n  coverpage: ['/', '/zh-cn/'],\n};\n</code></pre> <p>Or a special file name</p> <pre><code>window.$docsify = {\n  coverpage: {\n    '/': 'cover.md',\n    '/zh-cn/': 'cover.md',\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/custom-navbar/","title":"Custom navbar","text":""},{"location":"docsify_docs/docsify/docs/custom-navbar/#html","title":"HTML","text":"<p>If you need custom navigation, you can create a HTML-based navigation bar.</p> <p>!&gt; Note that documentation links begin with <code>#/</code>.</p> <pre><code>&lt;!-- index.html --&gt;\n\n&lt;body&gt;\n  &lt;nav&gt;\n    &lt;a href=\"#/\"&gt;EN&lt;/a&gt;\n    &lt;a href=\"#/zh-cn/\"&gt;\u7b80\u4f53\u4e2d\u6587&lt;/a&gt;\n  &lt;/nav&gt;\n  &lt;div id=\"app\"&gt;&lt;/div&gt;\n&lt;/body&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/custom-navbar/#markdown","title":"Markdown","text":"<p>Alternatively, you can create a custom markdown-based navigation file by setting <code>loadNavbar</code> to true and creating <code>_navbar.md</code>, compare loadNavbar configuration.</p> <pre><code>&lt;!-- index.html --&gt;\n\n&lt;script&gt;\n  window.$docsify = {\n    loadNavbar: true,\n  };\n&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n</code></pre> <pre><code>&lt;!-- _navbar.md --&gt;\n\n- [En](/)\n- [chinese](/zh-cn/)\n</code></pre> <p>To create drop-down menus:</p> <pre><code>&lt;!-- _navbar.md --&gt;\n\n- Translations\n\n  - [En](/)\n  - [chinese](/zh-cn/)\n</code></pre> <p>!&gt; You need to create a <code>.nojekyll</code> in <code>./docs</code> to prevent GitHub Pages from ignoring files that begin with an underscore.</p> <p><code>_navbar.md</code> is loaded from each level directory. If the current directory doesn't have <code>_navbar.md</code>, it will fall back to the parent directory. If, for example, the current path is <code>/guide/quick-start</code>, the <code>_navbar.md</code> will be loaded from <code>/guide/_navbar.md</code>.</p>"},{"location":"docsify_docs/docsify/docs/custom-navbar/#nesting","title":"Nesting","text":"<p>You can create sub-lists by indenting items that are under a certain parent.</p> <pre><code>&lt;!-- _navbar.md --&gt;\n\n- Getting started\n\n  - [Quick start](quickstart.md)\n  - [Writing more pages](more-pages.md)\n  - [Custom navbar](custom-navbar.md)\n  - [Cover page](cover.md)\n\n- Configuration\n\n  - [Configuration](configuration.md)\n  - [Themes](themes.md)\n  - [Using plugins](plugins.md)\n  - [Markdown configuration](markdown.md)\n  - [Language highlight](language-highlight.md)\n</code></pre> <p>renders as</p> <p></p>"},{"location":"docsify_docs/docsify/docs/custom-navbar/#combining-custom-navbars-with-the-emoji-plugin","title":"Combining custom navbars with the emoji plugin","text":"<p>If you use the emoji plugin:</p> <pre><code>&lt;!-- index.html --&gt;\n\n&lt;script&gt;\n  window.$docsify = {\n    // ...\n  };\n&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/emoji.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>you could, for example, use flag emojis in your custom navbar Markdown file:</p> <pre><code>&lt;!-- _navbar.md --&gt;\n\n- [:us:, :uk:](/)\n- [:cn:](/zh-cn/)\n</code></pre>"},{"location":"docsify_docs/docsify/docs/deploy/","title":"Deploy","text":"<p>Similar to GitBook, you can deploy files to GitHub Pages, GitLab Pages or VPS.</p>"},{"location":"docsify_docs/docsify/docs/deploy/#github-pages","title":"GitHub Pages","text":"<p>There are three places to populate your docs for your GitHub repository:</p> <ul> <li><code>docs/</code> folder</li> <li>main branch</li> <li>gh-pages branch</li> </ul> <p>It is recommended that you save your files to the <code>./docs</code> subfolder of the <code>main</code> branch of your repository. Then select <code>main branch /docs folder</code> as your GitHub Pages source in your repository's settings page.</p> <p></p> <p>!&gt; You can also save files in the root directory and select <code>main branch</code>. You'll need to place a <code>.nojekyll</code> file in the deploy location (such as <code>/docs</code> or the gh-pages branch)</p>"},{"location":"docsify_docs/docsify/docs/deploy/#gitlab-pages","title":"GitLab Pages","text":"<p>If you are deploying your master branch, create a <code>.gitlab-ci.yml</code> with the following script:</p> <p>?&gt; The <code>.public</code> workaround is so <code>cp</code> doesn't also copy <code>public/</code> to itself in an infinite loop.</p> <pre><code>pages:\n  stage: deploy\n  script:\n  - mkdir .public\n  - cp -r * .public\n  - mv .public public\n  artifacts:\n    paths:\n    - public\n  only:\n  - master\n</code></pre> <p>!&gt; You can replace script with <code>- cp -r docs/. public</code>, if <code>./docs</code> is your Docsify subfolder.</p>"},{"location":"docsify_docs/docsify/docs/deploy/#firebase-hosting","title":"Firebase Hosting","text":"<p>!&gt; You'll need to install the Firebase CLI using <code>npm i -g firebase-tools</code> after signing into the Firebase Console using a Google Account.</p> <p>Using a terminal, determine and navigate to the directory for your Firebase Project. This could be <code>~/Projects/Docs</code>, etc. From there, run <code>firebase init</code> and choose <code>Hosting</code> from the menu (use space to select, arrow keys to change options and enter to confirm). Follow the setup instructions.</p> <p>Your <code>firebase.json</code> file should look similar to this (I changed the deployment directory from <code>public</code> to <code>site</code>):</p> <pre><code>{\n  \"hosting\": {\n    \"public\": \"site\",\n    \"ignore\": [\"firebase.json\", \"**/.*\", \"**/node_modules/**\"]\n  }\n}\n</code></pre> <p>Once finished, build the starting template by running <code>docsify init ./site</code> (replacing site with the deployment directory you determined when running <code>firebase init</code> - public by default). Add/edit the documentation, then run <code>firebase deploy</code> from the root project directory.</p>"},{"location":"docsify_docs/docsify/docs/deploy/#vps","title":"VPS","text":"<p>Use the following nginx config.</p> <pre><code>server {\n  listen 80;\n  server_name  your.domain.com;\n\n  location / {\n    alias /path/to/dir/of/docs/;\n    index index.html;\n  }\n}\n</code></pre>"},{"location":"docsify_docs/docsify/docs/deploy/#netlify","title":"Netlify","text":"<ol> <li>Login to your Netlify account.</li> <li>In the dashboard page, click Add New Site.</li> <li>Select Github.</li> <li>Choose the repository where you store your docs, in the Base Directory add the subfolder where the files are stored. For example, it should be <code>docs</code>.</li> <li>In the Build Command area leave it blank.</li> <li>In the Publish directory area, if you have added the <code>docs</code> in the Base Directory you will see the publish directory populated with <code>docs/</code></li> <li>Netlify is smart enough to look for the the <code>index.html</code> file inside the <code>docs/</code> folder.</li> </ol>"},{"location":"docsify_docs/docsify/docs/deploy/#html5-router","title":"HTML5 router","text":"<p>When using the HTML5 router, you need to set up redirect rules that redirect all requests to your <code>index.html</code>. It's pretty simple when you're using Netlify. Just create a file named <code>_redirects</code> in the docs directory, add this snippet to the file, and you're all set:</p> <pre><code>/*    /index.html   200\n</code></pre>"},{"location":"docsify_docs/docsify/docs/deploy/#vercel","title":"Vercel","text":"<ol> <li>Install Vercel CLI, <code>npm i -g vercel</code></li> <li>Change directory to your docsify website, for example <code>cd docs</code></li> <li>Deploy with a single command, <code>vercel</code></li> </ol>"},{"location":"docsify_docs/docsify/docs/deploy/#aws-amplify","title":"AWS Amplify","text":"<ol> <li>Set the routerMode in the Docsify project <code>index.html</code> to history mode.</li> </ol> <pre><code>&lt;script&gt;\n  window.$docsify = {\n    loadSidebar: true,\n    routerMode: 'history',\n  };\n&lt;/script&gt;\n</code></pre> <ol> <li>Login to your AWS Console.</li> <li>Go to the AWS Amplify Dashboard.</li> <li>Choose the Deploy route to setup your project.</li> <li>When prompted, keep the build settings empty if you're serving your docs within the root directory. If you're serving your docs from a different directory, customise your amplify.yml</li> </ol> <pre><code>version: 0.1\nfrontend:\n  phases:\n    build:\n      commands:\n        - echo \"Nothing to build\"\n  artifacts:\n    baseDirectory: /docs\n    files:\n      - '**/*'\n  cache:\n    paths: []\n</code></pre> <ol> <li>Add the following Redirect rules in their displayed order. Note that the second record is a PNG image where you can change it with any image format you are using.</li> </ol> Source address Target address Type /&lt;*&gt;.md /&lt;*&gt;.md 200 (Rewrite) /&lt;*&gt;.png /&lt;*&gt;.png 200 (Rewrite) /&lt;*&gt; /index.html 200 (Rewrite)"},{"location":"docsify_docs/docsify/docs/deploy/#stormkit","title":"Stormkit","text":"<ol> <li>Login to your Stormkit account.</li> <li>Using the user interface, import your docsify project from one of the three supported Git providers (GitHub, GitLab, or Bitbucket).</li> <li>Navigate to the project\u2019s production environment in Stormkit or create a new environment if needed.</li> <li>Verify the build command in your Stormkit configuration. By default, Stormkit CI will run <code>npm run build</code> but you can specify a custom build command on this page.</li> <li>Set output folder to <code>docs</code></li> <li>Click the \u201cDeploy Now\u201d button to deploy your site.</li> </ol> <p>Read more in the Stormkit Documentation.</p>"},{"location":"docsify_docs/docsify/docs/deploy/#docker","title":"Docker","text":"<ul> <li>Create docsify files</li> </ul> <p>You need prepare the initial files instead of making them inside the container.   See the Quickstart section for instructions on how to create these files manually or using docsify-cli.</p> <pre><code>index.html\nREADME.md\n</code></pre> <ul> <li>Create Dockerfile</li> </ul> <pre><code>  FROM node:latest\n  LABEL description=\"A demo Dockerfile for build Docsify.\"\n  WORKDIR /docs\n  RUN npm install -g docsify-cli@latest\n  EXPOSE 3000/tcp\n  ENTRYPOINT docsify serve .\n</code></pre> <p>The current directory structure should be this:</p> <pre><code> index.html\n README.md\n Dockerfile\n</code></pre> <ul> <li>Build docker image</li> </ul> <pre><code>docker build -f Dockerfile -t docsify/demo .\n</code></pre> <ul> <li>Run docker image</li> </ul> <pre><code>docker run -itp 3000:3000 --name=docsify -v $(pwd):/docs docsify/demo\n</code></pre>"},{"location":"docsify_docs/docsify/docs/deploy/#kinsta-static-site-hosting","title":"Kinsta Static Site Hosting","text":"<p>You can deploy Docsify as a Static Site on Kinsta.</p> <ol> <li> <p>Login or create an account to view your MyKinsta dashboard.</p> </li> <li> <p>Authorize Kinsta with your Git provider.</p> </li> <li> <p>Select Static Sites from the left sidebar and press Add sites.</p> </li> <li> <p>Select the repository and branch you want to deploy.</p> </li> <li> <p>During the build settings, Kinsta will automatically try to fill out the Build command, Node version, and Publish directory. If it won't, fill out the following:</p> </li> <li> <p>Build command: leave empty</p> </li> <li>Node version: leave on default selection or a specific version (e.g. <code>18.16.0</code>)</li> <li> <p>Publish directory: <code>docs</code></p> </li> <li> <p>Click the Create site.</p> </li> </ol>"},{"location":"docsify_docs/docsify/docs/embed-files/","title":"Embed files","text":"<p>With docsify 4.6 it is now possible to embed any type of file.</p> <p>You can embed these files as video, audio, iframes, or code blocks, and even Markdown files can even be embedded directly into the document.</p> <p>For example, here is an embedded Markdown file. You only need to do this:</p> <pre><code>[filename](_media/example.md ':include')\n</code></pre> <p>Then the content of <code>example.md</code> will be displayed directly here:</p> <p>filename</p> <p>You can check the original content for example.md.</p> <p>Normally, this will be compiled into a link, but in docsify, if you add <code>:include</code> it will be embedded. You can use single or double quotation marks around as you like.</p> <p>External links can be used too - just replace the target. If you want to use a gist URL, see Embed a gist section.</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#embedded-file-type","title":"Embedded file type","text":"<p>Currently, file extensions are automatically recognized and embedded in different ways.</p> <p>These types are supported:</p> <ul> <li>iframe <code>.html</code>, <code>.htm</code></li> <li>markdown <code>.markdown</code>, <code>.md</code></li> <li>audio <code>.mp3</code></li> <li>video <code>.mp4</code>, <code>.ogg</code></li> <li>code other file extension</li> </ul> <p>Of course, you can force the specified type. For example, a Markdown file can be embedded as a code block by setting <code>:type=code</code>.</p> <pre><code>[filename](_media/example.md ':include :type=code')\n</code></pre> <p>You will get:</p> <p>filename</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#markdown-with-yaml-front-matter","title":"Markdown with YAML Front Matter","text":"<p>Front Matter, commonly utilized in blogging systems like Jekyll, serves to define metadata for a document. The front-matter.js package facilitates the extraction of metadata (front matter) from documents.</p> <p>When using Markdown, YAML front matter will be stripped from the rendered content. The attributes cannot be used in this case.</p> <pre><code>[filename](_media/example-with-yaml.md ':include')\n</code></pre> <p>You will get just the content</p> <p>filename</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#embedded-code-fragments","title":"Embedded code fragments","text":"<p>Sometimes you don't want to embed a whole file. Maybe because you need just a few lines but you want to compile and test the file in CI.</p> <pre><code>[filename](_media/example.js ':include :type=code :fragment=demo')\n</code></pre> <p>In your code file you need to surround the fragment between <code>/// [demo]</code> lines (before and after the fragment). Alternatively you can use <code>### [demo]</code>.</p> <p>Example:</p> <p>filename</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#tag-attribute","title":"Tag attribute","text":"<p>If you embed the file as <code>iframe</code>, <code>audio</code> and <code>video</code>, then you may need to set the attributes of these tags.</p> <p>?&gt; Note, for the <code>audio</code> and <code>video</code> types, docsify adds the <code>controls</code> attribute by default. When you want add more attributes, the <code>controls</code> attribute need to be added manually if need be.</p> <pre><code>[filename](_media/example.mp4 ':include :type=video controls width=100%')\n</code></pre> <pre><code>[cinwell website](https://cinwell.com ':include :type=iframe width=100% height=400px')\n</code></pre> <p>cinwell website</p> <p>Did you see it? You only need to write directly. You can check MDN for these attributes.</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#the-code-block-highlight","title":"The code block highlight","text":"<p>Embedding any type of source code file, you can specify the highlighted language or automatically identify.</p> <pre><code>[](_media/example.html ':include :type=code text')\n</code></pre> <p>\u2b07\ufe0f</p> <p></p> <p>?&gt; How to set highlight? You can see here.</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#embed-a-gist","title":"Embed a gist","text":"<p>You can embed a gist as markdown content or as a code block - this is based on the approach at the start of Embed Files section, but uses a raw gist URL as the target.</p> <p>?&gt; No plugin or app config change is needed here to make this work. In fact, the \"Embed\" <code>script</code> tag that is copied from a gist will not load even if you make plugin or config changes to allow an external script.</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#identify-the-gists-metadata","title":"Identify the gist's metadata","text":"<p>Start by viewing a gist on <code>gist.github.com</code>. For the purposes of this guide, we use this gist:</p> <ul> <li>https://gist.github.com/anikethsaha/f88893bb563bb7229d6e575db53a8c15</li> </ul> <p>Identify the following items from the gist:</p> Field Example Description Username <code>anikethsaha</code> The gist's owner. Gist ID <code>c2bece08f27c4277001f123898d16a7c</code> Identifier for the gist. This is fixed for the gist's lifetime. Filename <code>content.md</code> Select a name of a file in the gist. This needed even on a single-file gist for embedding to work. <p>You will need those to build the raw gist URL for the target file. This has the following format:</p> <ul> <li><code>https://gist.githubusercontent.com/USERNAME/GIST_ID/raw/FILENAME</code></li> </ul> <p>Here are two examples based on the sample gist:</p> <ul> <li>https://gist.githubusercontent.com/anikethsaha/f88893bb563bb7229d6e575db53a8c15/raw/content.md</li> <li>https://gist.githubusercontent.com/anikethsaha/f88893bb563bb7229d6e575db53a8c15/raw/script.js</li> </ul> <p>?&gt; Alternatively, you can get a raw URL directly clicking the Raw button on a gist file. But, if you use that approach, just be sure to remove the revision number between <code>raw/</code> and the filename so that the URL matches the pattern above instead. Otherwise your embedded gist will not show the latest content when the gist is updated.</p> <p>Continue with one of the sections below to embed the gist on a Docsify page.</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#render-markdown-content-from-a-gist","title":"Render markdown content from a gist","text":"<p>This is a great way to embed content seamlessly in your docs, without sending someone to an external link. This approach is well-suited to reusing a gist of say installation instructions across doc sites of multiple repos. This approach works equally well with a gist owned by your account or by another user.</p> <p>Here is the format:</p> <pre><code>[LABEL](https://gist.githubusercontent.com/USERNAME/GIST_ID/raw/FILENAME ':include')\n</code></pre> <p>For example:</p> <pre><code>[gist: content.md](https://gist.githubusercontent.com/anikethsaha/f88893bb563bb7229d6e575db53a8c15/raw/content.md ':include')\n</code></pre> <p>Which renders as:</p> <p>gist: content.md</p> <p>The <code>LABEL</code> can be any text you want. It acts as a fallback message if the link is broken - so it is useful to repeat the filename here in case you need to fix a broken link. It also makes an embedded element easy to read at a glance.</p>"},{"location":"docsify_docs/docsify/docs/embed-files/#render-a-codeblock-from-a-gist","title":"Render a codeblock from a gist","text":"<p>The format is the same as the previous section, but with <code>:type=code</code> added to the alt text. As with the Embedded file type section, the syntax highlighting will be inferred from the extension (e.g. <code>.js</code> or <code>.py</code>), so you can leave the <code>type</code> set as <code>code</code>.</p> <p>Here is the format:</p> <pre><code>[LABEL](https://gist.githubusercontent.com/USERNAME/GIST_ID/raw/FILENAME ':include :type=code')\n</code></pre> <p>For example:</p> <pre><code>[gist: script.js](https://gist.githubusercontent.com/anikethsaha/f88893bb563bb7229d6e575db53a8c15/raw/script.js ':include :type=code')\n</code></pre> <p>Which renders as:</p> <p>gist: script.js</p>"},{"location":"docsify_docs/docsify/docs/emoji/","title":"Emoji","text":"<p>Below is a complete list of emoji shorthand codes. Docsify can be configured to render emoji using GitHub-style emoji images or native emoji characters using the <code>nativeEmoji</code> configuration option.</p>   :100: `:100:`  :1234: `:1234:`  :+1: `:+1:`  :-1: `:-1:`  :1st_place_medal: `:1st_place_medal:`  :2nd_place_medal: `:2nd_place_medal:`  :3rd_place_medal: `:3rd_place_medal:`  :8ball: `:8ball:`  :a: `:a:`  :ab: `:ab:`  :abacus: `:abacus:`  :abc: `:abc:`  :abcd: `:abcd:`  :accept: `:accept:`  :accessibility: `:accessibility:`  :accordion: `:accordion:`  :adhesive_bandage: `:adhesive_bandage:`  :adult: `:adult:`  :aerial_tramway: `:aerial_tramway:`  :afghanistan: `:afghanistan:`  :airplane: `:airplane:`  :aland_islands: `:aland_islands:`  :alarm_clock: `:alarm_clock:`  :albania: `:albania:`  :alembic: `:alembic:`  :algeria: `:algeria:`  :alien: `:alien:`  :ambulance: `:ambulance:`  :american_samoa: `:american_samoa:`  :amphora: `:amphora:`  :anatomical_heart: `:anatomical_heart:`  :anchor: `:anchor:`  :andorra: `:andorra:`  :angel: `:angel:`  :anger: `:anger:`  :angola: `:angola:`  :angry: `:angry:`  :anguilla: `:anguilla:`  :anguished: `:anguished:`  :ant: `:ant:`  :antarctica: `:antarctica:`  :antigua_barbuda: `:antigua_barbuda:`  :apple: `:apple:`  :aquarius: `:aquarius:`  :argentina: `:argentina:`  :aries: `:aries:`  :armenia: `:armenia:`  :arrow_backward: `:arrow_backward:`  :arrow_double_down: `:arrow_double_down:`  :arrow_double_up: `:arrow_double_up:`  :arrow_down: `:arrow_down:`  :arrow_down_small: `:arrow_down_small:`  :arrow_forward: `:arrow_forward:`  :arrow_heading_down: `:arrow_heading_down:`  :arrow_heading_up: `:arrow_heading_up:`  :arrow_left: `:arrow_left:`  :arrow_lower_left: `:arrow_lower_left:`  :arrow_lower_right: `:arrow_lower_right:`  :arrow_right: `:arrow_right:`  :arrow_right_hook: `:arrow_right_hook:`  :arrow_up: `:arrow_up:`  :arrow_up_down: `:arrow_up_down:`  :arrow_up_small: `:arrow_up_small:`  :arrow_upper_left: `:arrow_upper_left:`  :arrow_upper_right: `:arrow_upper_right:`  :arrows_clockwise: `:arrows_clockwise:`  :arrows_counterclockwise: `:arrows_counterclockwise:`  :art: `:art:`  :articulated_lorry: `:articulated_lorry:`  :artificial_satellite: `:artificial_satellite:`  :artist: `:artist:`  :aruba: `:aruba:`  :ascension_island: `:ascension_island:`  :asterisk: `:asterisk:`  :astonished: `:astonished:`  :astronaut: `:astronaut:`  :athletic_shoe: `:athletic_shoe:`  :atm: `:atm:`  :atom: `:atom:`  :atom_symbol: `:atom_symbol:`  :australia: `:australia:`  :austria: `:austria:`  :auto_rickshaw: `:auto_rickshaw:`  :avocado: `:avocado:`  :axe: `:axe:`  :azerbaijan: `:azerbaijan:`  :b: `:b:`  :baby: `:baby:`  :baby_bottle: `:baby_bottle:`  :baby_chick: `:baby_chick:`  :baby_symbol: `:baby_symbol:`  :back: `:back:`  :bacon: `:bacon:`  :badger: `:badger:`  :badminton: `:badminton:`  :bagel: `:bagel:`  :baggage_claim: `:baggage_claim:`  :baguette_bread: `:baguette_bread:`  :bahamas: `:bahamas:`  :bahrain: `:bahrain:`  :balance_scale: `:balance_scale:`  :bald_man: `:bald_man:`  :bald_woman: `:bald_woman:`  :ballet_shoes: `:ballet_shoes:`  :balloon: `:balloon:`  :ballot_box: `:ballot_box:`  :ballot_box_with_check: `:ballot_box_with_check:`  :bamboo: `:bamboo:`  :banana: `:banana:`  :bangbang: `:bangbang:`  :bangladesh: `:bangladesh:`  :banjo: `:banjo:`  :bank: `:bank:`  :bar_chart: `:bar_chart:`  :barbados: `:barbados:`  :barber: `:barber:`  :baseball: `:baseball:`  :basecamp: `:basecamp:`  :basecampy: `:basecampy:`  :basket: `:basket:`  :basketball: `:basketball:`  :basketball_man: `:basketball_man:`  :basketball_woman: `:basketball_woman:`  :bat: `:bat:`  :bath: `:bath:`  :bathtub: `:bathtub:`  :battery: `:battery:`  :beach_umbrella: `:beach_umbrella:`  :beans: `:beans:`  :bear: `:bear:`  :bearded_person: `:bearded_person:`  :beaver: `:beaver:`  :bed: `:bed:`  :bee: `:bee:`  :beer: `:beer:`  :beers: `:beers:`  :beetle: `:beetle:`  :beginner: `:beginner:`  :belarus: `:belarus:`  :belgium: `:belgium:`  :belize: `:belize:`  :bell: `:bell:`  :bell_pepper: `:bell_pepper:`  :bellhop_bell: `:bellhop_bell:`  :benin: `:benin:`  :bento: `:bento:`  :bermuda: `:bermuda:`  :beverage_box: `:beverage_box:`  :bhutan: `:bhutan:`  :bicyclist: `:bicyclist:`  :bike: `:bike:`  :biking_man: `:biking_man:`  :biking_woman: `:biking_woman:`  :bikini: `:bikini:`  :billed_cap: `:billed_cap:`  :biohazard: `:biohazard:`  :bird: `:bird:`  :birthday: `:birthday:`  :bison: `:bison:`  :biting_lip: `:biting_lip:`  :black_bird: `:black_bird:`  :black_cat: `:black_cat:`  :black_circle: `:black_circle:`  :black_flag: `:black_flag:`  :black_heart: `:black_heart:`  :black_joker: `:black_joker:`  :black_large_square: `:black_large_square:`  :black_medium_small_square: `:black_medium_small_square:`  :black_medium_square: `:black_medium_square:`  :black_nib: `:black_nib:`  :black_small_square: `:black_small_square:`  :black_square_button: `:black_square_button:`  :blond_haired_man: `:blond_haired_man:`  :blond_haired_person: `:blond_haired_person:`  :blond_haired_woman: `:blond_haired_woman:`  :blonde_woman: `:blonde_woman:`  :blossom: `:blossom:`  :blowfish: `:blowfish:`  :blue_book: `:blue_book:`  :blue_car: `:blue_car:`  :blue_heart: `:blue_heart:`  :blue_square: `:blue_square:`  :blueberries: `:blueberries:`  :blush: `:blush:`  :boar: `:boar:`  :boat: `:boat:`  :bolivia: `:bolivia:`  :bomb: `:bomb:`  :bone: `:bone:`  :book: `:book:`  :bookmark: `:bookmark:`  :bookmark_tabs: `:bookmark_tabs:`  :books: `:books:`  :boom: `:boom:`  :boomerang: `:boomerang:`  :boot: `:boot:`  :bosnia_herzegovina: `:bosnia_herzegovina:`  :botswana: `:botswana:`  :bouncing_ball_man: `:bouncing_ball_man:`  :bouncing_ball_person: `:bouncing_ball_person:`  :bouncing_ball_woman: `:bouncing_ball_woman:`  :bouquet: `:bouquet:`  :bouvet_island: `:bouvet_island:`  :bow: `:bow:`  :bow_and_arrow: `:bow_and_arrow:`  :bowing_man: `:bowing_man:`  :bowing_woman: `:bowing_woman:`  :bowl_with_spoon: `:bowl_with_spoon:`  :bowling: `:bowling:`  :bowtie: `:bowtie:`  :boxing_glove: `:boxing_glove:`  :boy: `:boy:`  :brain: `:brain:`  :brazil: `:brazil:`  :bread: `:bread:`  :breast_feeding: `:breast_feeding:`  :bricks: `:bricks:`  :bride_with_veil: `:bride_with_veil:`  :bridge_at_night: `:bridge_at_night:`  :briefcase: `:briefcase:`  :british_indian_ocean_territory: `:british_indian_ocean_territory:`  :british_virgin_islands: `:british_virgin_islands:`  :broccoli: `:broccoli:`  :broken_heart: `:broken_heart:`  :broom: `:broom:`  :brown_circle: `:brown_circle:`  :brown_heart: `:brown_heart:`  :brown_square: `:brown_square:`  :brunei: `:brunei:`  :bubble_tea: `:bubble_tea:`  :bubbles: `:bubbles:`  :bucket: `:bucket:`  :bug: `:bug:`  :building_construction: `:building_construction:`  :bulb: `:bulb:`  :bulgaria: `:bulgaria:`  :bullettrain_front: `:bullettrain_front:`  :bullettrain_side: `:bullettrain_side:`  :burkina_faso: `:burkina_faso:`  :burrito: `:burrito:`  :burundi: `:burundi:`  :bus: `:bus:`  :business_suit_levitating: `:business_suit_levitating:`  :busstop: `:busstop:`  :bust_in_silhouette: `:bust_in_silhouette:`  :busts_in_silhouette: `:busts_in_silhouette:`  :butter: `:butter:`  :butterfly: `:butterfly:`  :cactus: `:cactus:`  :cake: `:cake:`  :calendar: `:calendar:`  :call_me_hand: `:call_me_hand:`  :calling: `:calling:`  :cambodia: `:cambodia:`  :camel: `:camel:`  :camera: `:camera:`  :camera_flash: `:camera_flash:`  :cameroon: `:cameroon:`  :camping: `:camping:`  :canada: `:canada:`  :canary_islands: `:canary_islands:`  :cancer: `:cancer:`  :candle: `:candle:`  :candy: `:candy:`  :canned_food: `:canned_food:`  :canoe: `:canoe:`  :cape_verde: `:cape_verde:`  :capital_abcd: `:capital_abcd:`  :capricorn: `:capricorn:`  :car: `:car:`  :card_file_box: `:card_file_box:`  :card_index: `:card_index:`  :card_index_dividers: `:card_index_dividers:`  :caribbean_netherlands: `:caribbean_netherlands:`  :carousel_horse: `:carousel_horse:`  :carpentry_saw: `:carpentry_saw:`  :carrot: `:carrot:`  :cartwheeling: `:cartwheeling:`  :cat: `:cat:`  :cat2: `:cat2:`  :cayman_islands: `:cayman_islands:`  :cd: `:cd:`  :central_african_republic: `:central_african_republic:`  :ceuta_melilla: `:ceuta_melilla:`  :chad: `:chad:`  :chains: `:chains:`  :chair: `:chair:`  :champagne: `:champagne:`  :chart: `:chart:`  :chart_with_downwards_trend: `:chart_with_downwards_trend:`  :chart_with_upwards_trend: `:chart_with_upwards_trend:`  :checkered_flag: `:checkered_flag:`  :cheese: `:cheese:`  :cherries: `:cherries:`  :cherry_blossom: `:cherry_blossom:`  :chess_pawn: `:chess_pawn:`  :chestnut: `:chestnut:`  :chicken: `:chicken:`  :child: `:child:`  :children_crossing: `:children_crossing:`  :chile: `:chile:`  :chipmunk: `:chipmunk:`  :chocolate_bar: `:chocolate_bar:`  :chopsticks: `:chopsticks:`  :christmas_island: `:christmas_island:`  :christmas_tree: `:christmas_tree:`  :church: `:church:`  :cinema: `:cinema:`  :circus_tent: `:circus_tent:`  :city_sunrise: `:city_sunrise:`  :city_sunset: `:city_sunset:`  :cityscape: `:cityscape:`  :cl: `:cl:`  :clamp: `:clamp:`  :clap: `:clap:`  :clapper: `:clapper:`  :classical_building: `:classical_building:`  :climbing: `:climbing:`  :climbing_man: `:climbing_man:`  :climbing_woman: `:climbing_woman:`  :clinking_glasses: `:clinking_glasses:`  :clipboard: `:clipboard:`  :clipperton_island: `:clipperton_island:`  :clock1: `:clock1:`  :clock10: `:clock10:`  :clock1030: `:clock1030:`  :clock11: `:clock11:`  :clock1130: `:clock1130:`  :clock12: `:clock12:`  :clock1230: `:clock1230:`  :clock130: `:clock130:`  :clock2: `:clock2:`  :clock230: `:clock230:`  :clock3: `:clock3:`  :clock330: `:clock330:`  :clock4: `:clock4:`  :clock430: `:clock430:`  :clock5: `:clock5:`  :clock530: `:clock530:`  :clock6: `:clock6:`  :clock630: `:clock630:`  :clock7: `:clock7:`  :clock730: `:clock730:`  :clock8: `:clock8:`  :clock830: `:clock830:`  :clock9: `:clock9:`  :clock930: `:clock930:`  :closed_book: `:closed_book:`  :closed_lock_with_key: `:closed_lock_with_key:`  :closed_umbrella: `:closed_umbrella:`  :cloud: `:cloud:`  :cloud_with_lightning: `:cloud_with_lightning:`  :cloud_with_lightning_and_rain: `:cloud_with_lightning_and_rain:`  :cloud_with_rain: `:cloud_with_rain:`  :cloud_with_snow: `:cloud_with_snow:`  :clown_face: `:clown_face:`  :clubs: `:clubs:`  :cn: `:cn:`  :coat: `:coat:`  :cockroach: `:cockroach:`  :cocktail: `:cocktail:`  :coconut: `:coconut:`  :cocos_islands: `:cocos_islands:`  :coffee: `:coffee:`  :coffin: `:coffin:`  :coin: `:coin:`  :cold_face: `:cold_face:`  :cold_sweat: `:cold_sweat:`  :collision: `:collision:`  :colombia: `:colombia:`  :comet: `:comet:`  :comoros: `:comoros:`  :compass: `:compass:`  :computer: `:computer:`  :computer_mouse: `:computer_mouse:`  :confetti_ball: `:confetti_ball:`  :confounded: `:confounded:`  :confused: `:confused:`  :congo_brazzaville: `:congo_brazzaville:`  :congo_kinshasa: `:congo_kinshasa:`  :congratulations: `:congratulations:`  :construction: `:construction:`  :construction_worker: `:construction_worker:`  :construction_worker_man: `:construction_worker_man:`  :construction_worker_woman: `:construction_worker_woman:`  :control_knobs: `:control_knobs:`  :convenience_store: `:convenience_store:`  :cook: `:cook:`  :cook_islands: `:cook_islands:`  :cookie: `:cookie:`  :cool: `:cool:`  :cop: `:cop:`  :copyright: `:copyright:`  :coral: `:coral:`  :corn: `:corn:`  :costa_rica: `:costa_rica:`  :cote_divoire: `:cote_divoire:`  :couch_and_lamp: `:couch_and_lamp:`  :couple: `:couple:`  :couple_with_heart: `:couple_with_heart:`  :couple_with_heart_man_man: `:couple_with_heart_man_man:`  :couple_with_heart_woman_man: `:couple_with_heart_woman_man:`  :couple_with_heart_woman_woman: `:couple_with_heart_woman_woman:`  :couplekiss: `:couplekiss:`  :couplekiss_man_man: `:couplekiss_man_man:`  :couplekiss_man_woman: `:couplekiss_man_woman:`  :couplekiss_woman_woman: `:couplekiss_woman_woman:`  :cow: `:cow:`  :cow2: `:cow2:`  :cowboy_hat_face: `:cowboy_hat_face:`  :crab: `:crab:`  :crayon: `:crayon:`  :credit_card: `:credit_card:`  :crescent_moon: `:crescent_moon:`  :cricket: `:cricket:`  :cricket_game: `:cricket_game:`  :croatia: `:croatia:`  :crocodile: `:crocodile:`  :croissant: `:croissant:`  :crossed_fingers: `:crossed_fingers:`  :crossed_flags: `:crossed_flags:`  :crossed_swords: `:crossed_swords:`  :crown: `:crown:`  :crutch: `:crutch:`  :cry: `:cry:`  :crying_cat_face: `:crying_cat_face:`  :crystal_ball: `:crystal_ball:`  :cuba: `:cuba:`  :cucumber: `:cucumber:`  :cup_with_straw: `:cup_with_straw:`  :cupcake: `:cupcake:`  :cupid: `:cupid:`  :curacao: `:curacao:`  :curling_stone: `:curling_stone:`  :curly_haired_man: `:curly_haired_man:`  :curly_haired_woman: `:curly_haired_woman:`  :curly_loop: `:curly_loop:`  :currency_exchange: `:currency_exchange:`  :curry: `:curry:`  :cursing_face: `:cursing_face:`  :custard: `:custard:`  :customs: `:customs:`  :cut_of_meat: `:cut_of_meat:`  :cyclone: `:cyclone:`  :cyprus: `:cyprus:`  :czech_republic: `:czech_republic:`  :dagger: `:dagger:`  :dancer: `:dancer:`  :dancers: `:dancers:`  :dancing_men: `:dancing_men:`  :dancing_women: `:dancing_women:`  :dango: `:dango:`  :dark_sunglasses: `:dark_sunglasses:`  :dart: `:dart:`  :dash: `:dash:`  :date: `:date:`  :de: `:de:`  :deaf_man: `:deaf_man:`  :deaf_person: `:deaf_person:`  :deaf_woman: `:deaf_woman:`  :deciduous_tree: `:deciduous_tree:`  :deer: `:deer:`  :denmark: `:denmark:`  :department_store: `:department_store:`  :dependabot: `:dependabot:`  :derelict_house: `:derelict_house:`  :desert: `:desert:`  :desert_island: `:desert_island:`  :desktop_computer: `:desktop_computer:`  :detective: `:detective:`  :diamond_shape_with_a_dot_inside: `:diamond_shape_with_a_dot_inside:`  :diamonds: `:diamonds:`  :diego_garcia: `:diego_garcia:`  :disappointed: `:disappointed:`  :disappointed_relieved: `:disappointed_relieved:`  :disguised_face: `:disguised_face:`  :diving_mask: `:diving_mask:`  :diya_lamp: `:diya_lamp:`  :dizzy: `:dizzy:`  :dizzy_face: `:dizzy_face:`  :djibouti: `:djibouti:`  :dna: `:dna:`  :do_not_litter: `:do_not_litter:`  :dodo: `:dodo:`  :dog: `:dog:`  :dog2: `:dog2:`  :dollar: `:dollar:`  :dolls: `:dolls:`  :dolphin: `:dolphin:`  :dominica: `:dominica:`  :dominican_republic: `:dominican_republic:`  :donkey: `:donkey:`  :door: `:door:`  :dotted_line_face: `:dotted_line_face:`  :doughnut: `:doughnut:`  :dove: `:dove:`  :dragon: `:dragon:`  :dragon_face: `:dragon_face:`  :dress: `:dress:`  :dromedary_camel: `:dromedary_camel:`  :drooling_face: `:drooling_face:`  :drop_of_blood: `:drop_of_blood:`  :droplet: `:droplet:`  :drum: `:drum:`  :duck: `:duck:`  :dumpling: `:dumpling:`  :dvd: `:dvd:`  :e-mail: `:e-mail:`  :eagle: `:eagle:`  :ear: `:ear:`  :ear_of_rice: `:ear_of_rice:`  :ear_with_hearing_aid: `:ear_with_hearing_aid:`  :earth_africa: `:earth_africa:`  :earth_americas: `:earth_americas:`  :earth_asia: `:earth_asia:`  :ecuador: `:ecuador:`  :egg: `:egg:`  :eggplant: `:eggplant:`  :egypt: `:egypt:`  :eight: `:eight:`  :eight_pointed_black_star: `:eight_pointed_black_star:`  :eight_spoked_asterisk: `:eight_spoked_asterisk:`  :eject_button: `:eject_button:`  :el_salvador: `:el_salvador:`  :electric_plug: `:electric_plug:`  :electron: `:electron:`  :elephant: `:elephant:`  :elevator: `:elevator:`  :elf: `:elf:`  :elf_man: `:elf_man:`  :elf_woman: `:elf_woman:`  :email: `:email:`  :empty_nest: `:empty_nest:`  :end: `:end:`  :england: `:england:`  :envelope: `:envelope:`  :envelope_with_arrow: `:envelope_with_arrow:`  :equatorial_guinea: `:equatorial_guinea:`  :eritrea: `:eritrea:`  :es: `:es:`  :estonia: `:estonia:`  :ethiopia: `:ethiopia:`  :eu: `:eu:`  :euro: `:euro:`  :european_castle: `:european_castle:`  :european_post_office: `:european_post_office:`  :european_union: `:european_union:`  :evergreen_tree: `:evergreen_tree:`  :exclamation: `:exclamation:`  :exploding_head: `:exploding_head:`  :expressionless: `:expressionless:`  :eye: `:eye:`  :eye_speech_bubble: `:eye_speech_bubble:`  :eyeglasses: `:eyeglasses:`  :eyes: `:eyes:`  :face_exhaling: `:face_exhaling:`  :face_holding_back_tears: `:face_holding_back_tears:`  :face_in_clouds: `:face_in_clouds:`  :face_with_diagonal_mouth: `:face_with_diagonal_mouth:`  :face_with_head_bandage: `:face_with_head_bandage:`  :face_with_open_eyes_and_hand_over_mouth: `:face_with_open_eyes_and_hand_over_mouth:`  :face_with_peeking_eye: `:face_with_peeking_eye:`  :face_with_spiral_eyes: `:face_with_spiral_eyes:`  :face_with_thermometer: `:face_with_thermometer:`  :facepalm: `:facepalm:`  :facepunch: `:facepunch:`  :factory: `:factory:`  :factory_worker: `:factory_worker:`  :fairy: `:fairy:`  :fairy_man: `:fairy_man:`  :fairy_woman: `:fairy_woman:`  :falafel: `:falafel:`  :falkland_islands: `:falkland_islands:`  :fallen_leaf: `:fallen_leaf:`  :family: `:family:`  :family_man_boy: `:family_man_boy:`  :family_man_boy_boy: `:family_man_boy_boy:`  :family_man_girl: `:family_man_girl:`  :family_man_girl_boy: `:family_man_girl_boy:`  :family_man_girl_girl: `:family_man_girl_girl:`  :family_man_man_boy: `:family_man_man_boy:`  :family_man_man_boy_boy: `:family_man_man_boy_boy:`  :family_man_man_girl: `:family_man_man_girl:`  :family_man_man_girl_boy: `:family_man_man_girl_boy:`  :family_man_man_girl_girl: `:family_man_man_girl_girl:`  :family_man_woman_boy: `:family_man_woman_boy:`  :family_man_woman_boy_boy: `:family_man_woman_boy_boy:`  :family_man_woman_girl: `:family_man_woman_girl:`  :family_man_woman_girl_boy: `:family_man_woman_girl_boy:`  :family_man_woman_girl_girl: `:family_man_woman_girl_girl:`  :family_woman_boy: `:family_woman_boy:`  :family_woman_boy_boy: `:family_woman_boy_boy:`  :family_woman_girl: `:family_woman_girl:`  :family_woman_girl_boy: `:family_woman_girl_boy:`  :family_woman_girl_girl: `:family_woman_girl_girl:`  :family_woman_woman_boy: `:family_woman_woman_boy:`  :family_woman_woman_boy_boy: `:family_woman_woman_boy_boy:`  :family_woman_woman_girl: `:family_woman_woman_girl:`  :family_woman_woman_girl_boy: `:family_woman_woman_girl_boy:`  :family_woman_woman_girl_girl: `:family_woman_woman_girl_girl:`  :farmer: `:farmer:`  :faroe_islands: `:faroe_islands:`  :fast_forward: `:fast_forward:`  :fax: `:fax:`  :fearful: `:fearful:`  :feather: `:feather:`  :feelsgood: `:feelsgood:`  :feet: `:feet:`  :female_detective: `:female_detective:`  :female_sign: `:female_sign:`  :ferris_wheel: `:ferris_wheel:`  :ferry: `:ferry:`  :field_hockey: `:field_hockey:`  :fiji: `:fiji:`  :file_cabinet: `:file_cabinet:`  :file_folder: `:file_folder:`  :film_projector: `:film_projector:`  :film_strip: `:film_strip:`  :finland: `:finland:`  :finnadie: `:finnadie:`  :fire: `:fire:`  :fire_engine: `:fire_engine:`  :fire_extinguisher: `:fire_extinguisher:`  :firecracker: `:firecracker:`  :firefighter: `:firefighter:`  :fireworks: `:fireworks:`  :first_quarter_moon: `:first_quarter_moon:`  :first_quarter_moon_with_face: `:first_quarter_moon_with_face:`  :fish: `:fish:`  :fish_cake: `:fish_cake:`  :fishing_pole_and_fish: `:fishing_pole_and_fish:`  :fishsticks: `:fishsticks:`  :fist: `:fist:`  :fist_left: `:fist_left:`  :fist_oncoming: `:fist_oncoming:`  :fist_raised: `:fist_raised:`  :fist_right: `:fist_right:`  :five: `:five:`  :flags: `:flags:`  :flamingo: `:flamingo:`  :flashlight: `:flashlight:`  :flat_shoe: `:flat_shoe:`  :flatbread: `:flatbread:`  :fleur_de_lis: `:fleur_de_lis:`  :flight_arrival: `:flight_arrival:`  :flight_departure: `:flight_departure:`  :flipper: `:flipper:`  :floppy_disk: `:floppy_disk:`  :flower_playing_cards: `:flower_playing_cards:`  :flushed: `:flushed:`  :flute: `:flute:`  :fly: `:fly:`  :flying_disc: `:flying_disc:`  :flying_saucer: `:flying_saucer:`  :fog: `:fog:`  :foggy: `:foggy:`  :folding_hand_fan: `:folding_hand_fan:`  :fondue: `:fondue:`  :foot: `:foot:`  :football: `:football:`  :footprints: `:footprints:`  :fork_and_knife: `:fork_and_knife:`  :fortune_cookie: `:fortune_cookie:`  :fountain: `:fountain:`  :fountain_pen: `:fountain_pen:`  :four: `:four:`  :four_leaf_clover: `:four_leaf_clover:`  :fox_face: `:fox_face:`  :fr: `:fr:`  :framed_picture: `:framed_picture:`  :free: `:free:`  :french_guiana: `:french_guiana:`  :french_polynesia: `:french_polynesia:`  :french_southern_territories: `:french_southern_territories:`  :fried_egg: `:fried_egg:`  :fried_shrimp: `:fried_shrimp:`  :fries: `:fries:`  :frog: `:frog:`  :frowning: `:frowning:`  :frowning_face: `:frowning_face:`  :frowning_man: `:frowning_man:`  :frowning_person: `:frowning_person:`  :frowning_woman: `:frowning_woman:`  :fu: `:fu:`  :fuelpump: `:fuelpump:`  :full_moon: `:full_moon:`  :full_moon_with_face: `:full_moon_with_face:`  :funeral_urn: `:funeral_urn:`  :gabon: `:gabon:`  :gambia: `:gambia:`  :game_die: `:game_die:`  :garlic: `:garlic:`  :gb: `:gb:`  :gear: `:gear:`  :gem: `:gem:`  :gemini: `:gemini:`  :genie: `:genie:`  :genie_man: `:genie_man:`  :genie_woman: `:genie_woman:`  :georgia: `:georgia:`  :ghana: `:ghana:`  :ghost: `:ghost:`  :gibraltar: `:gibraltar:`  :gift: `:gift:`  :gift_heart: `:gift_heart:`  :ginger_root: `:ginger_root:`  :giraffe: `:giraffe:`  :girl: `:girl:`  :globe_with_meridians: `:globe_with_meridians:`  :gloves: `:gloves:`  :goal_net: `:goal_net:`  :goat: `:goat:`  :goberserk: `:goberserk:`  :godmode: `:godmode:`  :goggles: `:goggles:`  :golf: `:golf:`  :golfing: `:golfing:`  :golfing_man: `:golfing_man:`  :golfing_woman: `:golfing_woman:`  :goose: `:goose:`  :gorilla: `:gorilla:`  :grapes: `:grapes:`  :greece: `:greece:`  :green_apple: `:green_apple:`  :green_book: `:green_book:`  :green_circle: `:green_circle:`  :green_heart: `:green_heart:`  :green_salad: `:green_salad:`  :green_square: `:green_square:`  :greenland: `:greenland:`  :grenada: `:grenada:`  :grey_exclamation: `:grey_exclamation:`  :grey_heart: `:grey_heart:`  :grey_question: `:grey_question:`  :grimacing: `:grimacing:`  :grin: `:grin:`  :grinning: `:grinning:`  :guadeloupe: `:guadeloupe:`  :guam: `:guam:`  :guard: `:guard:`  :guardsman: `:guardsman:`  :guardswoman: `:guardswoman:`  :guatemala: `:guatemala:`  :guernsey: `:guernsey:`  :guide_dog: `:guide_dog:`  :guinea: `:guinea:`  :guinea_bissau: `:guinea_bissau:`  :guitar: `:guitar:`  :gun: `:gun:`  :guyana: `:guyana:`  :hair_pick: `:hair_pick:`  :haircut: `:haircut:`  :haircut_man: `:haircut_man:`  :haircut_woman: `:haircut_woman:`  :haiti: `:haiti:`  :hamburger: `:hamburger:`  :hammer: `:hammer:`  :hammer_and_pick: `:hammer_and_pick:`  :hammer_and_wrench: `:hammer_and_wrench:`  :hamsa: `:hamsa:`  :hamster: `:hamster:`  :hand: `:hand:`  :hand_over_mouth: `:hand_over_mouth:`  :hand_with_index_finger_and_thumb_crossed: `:hand_with_index_finger_and_thumb_crossed:`  :handbag: `:handbag:`  :handball_person: `:handball_person:`  :handshake: `:handshake:`  :hankey: `:hankey:`  :hash: `:hash:`  :hatched_chick: `:hatched_chick:`  :hatching_chick: `:hatching_chick:`  :headphones: `:headphones:`  :headstone: `:headstone:`  :health_worker: `:health_worker:`  :hear_no_evil: `:hear_no_evil:`  :heard_mcdonald_islands: `:heard_mcdonald_islands:`  :heart: `:heart:`  :heart_decoration: `:heart_decoration:`  :heart_eyes: `:heart_eyes:`  :heart_eyes_cat: `:heart_eyes_cat:`  :heart_hands: `:heart_hands:`  :heart_on_fire: `:heart_on_fire:`  :heartbeat: `:heartbeat:`  :heartpulse: `:heartpulse:`  :hearts: `:hearts:`  :heavy_check_mark: `:heavy_check_mark:`  :heavy_division_sign: `:heavy_division_sign:`  :heavy_dollar_sign: `:heavy_dollar_sign:`  :heavy_equals_sign: `:heavy_equals_sign:`  :heavy_exclamation_mark: `:heavy_exclamation_mark:`  :heavy_heart_exclamation: `:heavy_heart_exclamation:`  :heavy_minus_sign: `:heavy_minus_sign:`  :heavy_multiplication_x: `:heavy_multiplication_x:`  :heavy_plus_sign: `:heavy_plus_sign:`  :hedgehog: `:hedgehog:`  :helicopter: `:helicopter:`  :herb: `:herb:`  :hibiscus: `:hibiscus:`  :high_brightness: `:high_brightness:`  :high_heel: `:high_heel:`  :hiking_boot: `:hiking_boot:`  :hindu_temple: `:hindu_temple:`  :hippopotamus: `:hippopotamus:`  :hocho: `:hocho:`  :hole: `:hole:`  :honduras: `:honduras:`  :honey_pot: `:honey_pot:`  :honeybee: `:honeybee:`  :hong_kong: `:hong_kong:`  :hook: `:hook:`  :horse: `:horse:`  :horse_racing: `:horse_racing:`  :hospital: `:hospital:`  :hot_face: `:hot_face:`  :hot_pepper: `:hot_pepper:`  :hotdog: `:hotdog:`  :hotel: `:hotel:`  :hotsprings: `:hotsprings:`  :hourglass: `:hourglass:`  :hourglass_flowing_sand: `:hourglass_flowing_sand:`  :house: `:house:`  :house_with_garden: `:house_with_garden:`  :houses: `:houses:`  :hugs: `:hugs:`  :hungary: `:hungary:`  :hurtrealbad: `:hurtrealbad:`  :hushed: `:hushed:`  :hut: `:hut:`  :hyacinth: `:hyacinth:`  :ice_cream: `:ice_cream:`  :ice_cube: `:ice_cube:`  :ice_hockey: `:ice_hockey:`  :ice_skate: `:ice_skate:`  :icecream: `:icecream:`  :iceland: `:iceland:`  :id: `:id:`  :identification_card: `:identification_card:`  :ideograph_advantage: `:ideograph_advantage:`  :imp: `:imp:`  :inbox_tray: `:inbox_tray:`  :incoming_envelope: `:incoming_envelope:`  :index_pointing_at_the_viewer: `:index_pointing_at_the_viewer:`  :india: `:india:`  :indonesia: `:indonesia:`  :infinity: `:infinity:`  :information_desk_person: `:information_desk_person:`  :information_source: `:information_source:`  :innocent: `:innocent:`  :interrobang: `:interrobang:`  :iphone: `:iphone:`  :iran: `:iran:`  :iraq: `:iraq:`  :ireland: `:ireland:`  :isle_of_man: `:isle_of_man:`  :israel: `:israel:`  :it: `:it:`  :izakaya_lantern: `:izakaya_lantern:`  :jack_o_lantern: `:jack_o_lantern:`  :jamaica: `:jamaica:`  :japan: `:japan:`  :japanese_castle: `:japanese_castle:`  :japanese_goblin: `:japanese_goblin:`  :japanese_ogre: `:japanese_ogre:`  :jar: `:jar:`  :jeans: `:jeans:`  :jellyfish: `:jellyfish:`  :jersey: `:jersey:`  :jigsaw: `:jigsaw:`  :jordan: `:jordan:`  :joy: `:joy:`  :joy_cat: `:joy_cat:`  :joystick: `:joystick:`  :jp: `:jp:`  :judge: `:judge:`  :juggling_person: `:juggling_person:`  :kaaba: `:kaaba:`  :kangaroo: `:kangaroo:`  :kazakhstan: `:kazakhstan:`  :kenya: `:kenya:`  :key: `:key:`  :keyboard: `:keyboard:`  :keycap_ten: `:keycap_ten:`  :khanda: `:khanda:`  :kick_scooter: `:kick_scooter:`  :kimono: `:kimono:`  :kiribati: `:kiribati:`  :kiss: `:kiss:`  :kissing: `:kissing:`  :kissing_cat: `:kissing_cat:`  :kissing_closed_eyes: `:kissing_closed_eyes:`  :kissing_heart: `:kissing_heart:`  :kissing_smiling_eyes: `:kissing_smiling_eyes:`  :kite: `:kite:`  :kiwi_fruit: `:kiwi_fruit:`  :kneeling_man: `:kneeling_man:`  :kneeling_person: `:kneeling_person:`  :kneeling_woman: `:kneeling_woman:`  :knife: `:knife:`  :knot: `:knot:`  :koala: `:koala:`  :koko: `:koko:`  :kosovo: `:kosovo:`  :kr: `:kr:`  :kuwait: `:kuwait:`  :kyrgyzstan: `:kyrgyzstan:`  :lab_coat: `:lab_coat:`  :label: `:label:`  :lacrosse: `:lacrosse:`  :ladder: `:ladder:`  :lady_beetle: `:lady_beetle:`  :lantern: `:lantern:`  :laos: `:laos:`  :large_blue_circle: `:large_blue_circle:`  :large_blue_diamond: `:large_blue_diamond:`  :large_orange_diamond: `:large_orange_diamond:`  :last_quarter_moon: `:last_quarter_moon:`  :last_quarter_moon_with_face: `:last_quarter_moon_with_face:`  :latin_cross: `:latin_cross:`  :latvia: `:latvia:`  :laughing: `:laughing:`  :leafy_green: `:leafy_green:`  :leaves: `:leaves:`  :lebanon: `:lebanon:`  :ledger: `:ledger:`  :left_luggage: `:left_luggage:`  :left_right_arrow: `:left_right_arrow:`  :left_speech_bubble: `:left_speech_bubble:`  :leftwards_arrow_with_hook: `:leftwards_arrow_with_hook:`  :leftwards_hand: `:leftwards_hand:`  :leftwards_pushing_hand: `:leftwards_pushing_hand:`  :leg: `:leg:`  :lemon: `:lemon:`  :leo: `:leo:`  :leopard: `:leopard:`  :lesotho: `:lesotho:`  :level_slider: `:level_slider:`  :liberia: `:liberia:`  :libra: `:libra:`  :libya: `:libya:`  :liechtenstein: `:liechtenstein:`  :light_blue_heart: `:light_blue_heart:`  :light_rail: `:light_rail:`  :link: `:link:`  :lion: `:lion:`  :lips: `:lips:`  :lipstick: `:lipstick:`  :lithuania: `:lithuania:`  :lizard: `:lizard:`  :llama: `:llama:`  :lobster: `:lobster:`  :lock: `:lock:`  :lock_with_ink_pen: `:lock_with_ink_pen:`  :lollipop: `:lollipop:`  :long_drum: `:long_drum:`  :loop: `:loop:`  :lotion_bottle: `:lotion_bottle:`  :lotus: `:lotus:`  :lotus_position: `:lotus_position:`  :lotus_position_man: `:lotus_position_man:`  :lotus_position_woman: `:lotus_position_woman:`  :loud_sound: `:loud_sound:`  :loudspeaker: `:loudspeaker:`  :love_hotel: `:love_hotel:`  :love_letter: `:love_letter:`  :love_you_gesture: `:love_you_gesture:`  :low_battery: `:low_battery:`  :low_brightness: `:low_brightness:`  :luggage: `:luggage:`  :lungs: `:lungs:`  :luxembourg: `:luxembourg:`  :lying_face: `:lying_face:`  :m: `:m:`  :macau: `:macau:`  :macedonia: `:macedonia:`  :madagascar: `:madagascar:`  :mag: `:mag:`  :mag_right: `:mag_right:`  :mage: `:mage:`  :mage_man: `:mage_man:`  :mage_woman: `:mage_woman:`  :magic_wand: `:magic_wand:`  :magnet: `:magnet:`  :mahjong: `:mahjong:`  :mailbox: `:mailbox:`  :mailbox_closed: `:mailbox_closed:`  :mailbox_with_mail: `:mailbox_with_mail:`  :mailbox_with_no_mail: `:mailbox_with_no_mail:`  :malawi: `:malawi:`  :malaysia: `:malaysia:`  :maldives: `:maldives:`  :male_detective: `:male_detective:`  :male_sign: `:male_sign:`  :mali: `:mali:`  :malta: `:malta:`  :mammoth: `:mammoth:`  :man: `:man:`  :man_artist: `:man_artist:`  :man_astronaut: `:man_astronaut:`  :man_beard: `:man_beard:`  :man_cartwheeling: `:man_cartwheeling:`  :man_cook: `:man_cook:`  :man_dancing: `:man_dancing:`  :man_facepalming: `:man_facepalming:`  :man_factory_worker: `:man_factory_worker:`  :man_farmer: `:man_farmer:`  :man_feeding_baby: `:man_feeding_baby:`  :man_firefighter: `:man_firefighter:`  :man_health_worker: `:man_health_worker:`  :man_in_manual_wheelchair: `:man_in_manual_wheelchair:`  :man_in_motorized_wheelchair: `:man_in_motorized_wheelchair:`  :man_in_tuxedo: `:man_in_tuxedo:`  :man_judge: `:man_judge:`  :man_juggling: `:man_juggling:`  :man_mechanic: `:man_mechanic:`  :man_office_worker: `:man_office_worker:`  :man_pilot: `:man_pilot:`  :man_playing_handball: `:man_playing_handball:`  :man_playing_water_polo: `:man_playing_water_polo:`  :man_scientist: `:man_scientist:`  :man_shrugging: `:man_shrugging:`  :man_singer: `:man_singer:`  :man_student: `:man_student:`  :man_teacher: `:man_teacher:`  :man_technologist: `:man_technologist:`  :man_with_gua_pi_mao: `:man_with_gua_pi_mao:`  :man_with_probing_cane: `:man_with_probing_cane:`  :man_with_turban: `:man_with_turban:`  :man_with_veil: `:man_with_veil:`  :mandarin: `:mandarin:`  :mango: `:mango:`  :mans_shoe: `:mans_shoe:`  :mantelpiece_clock: `:mantelpiece_clock:`  :manual_wheelchair: `:manual_wheelchair:`  :maple_leaf: `:maple_leaf:`  :maracas: `:maracas:`  :marshall_islands: `:marshall_islands:`  :martial_arts_uniform: `:martial_arts_uniform:`  :martinique: `:martinique:`  :mask: `:mask:`  :massage: `:massage:`  :massage_man: `:massage_man:`  :massage_woman: `:massage_woman:`  :mate: `:mate:`  :mauritania: `:mauritania:`  :mauritius: `:mauritius:`  :mayotte: `:mayotte:`  :meat_on_bone: `:meat_on_bone:`  :mechanic: `:mechanic:`  :mechanical_arm: `:mechanical_arm:`  :mechanical_leg: `:mechanical_leg:`  :medal_military: `:medal_military:`  :medal_sports: `:medal_sports:`  :medical_symbol: `:medical_symbol:`  :mega: `:mega:`  :melon: `:melon:`  :melting_face: `:melting_face:`  :memo: `:memo:`  :men_wrestling: `:men_wrestling:`  :mending_heart: `:mending_heart:`  :menorah: `:menorah:`  :mens: `:mens:`  :mermaid: `:mermaid:`  :merman: `:merman:`  :merperson: `:merperson:`  :metal: `:metal:`  :metro: `:metro:`  :mexico: `:mexico:`  :microbe: `:microbe:`  :micronesia: `:micronesia:`  :microphone: `:microphone:`  :microscope: `:microscope:`  :middle_finger: `:middle_finger:`  :military_helmet: `:military_helmet:`  :milk_glass: `:milk_glass:`  :milky_way: `:milky_way:`  :minibus: `:minibus:`  :minidisc: `:minidisc:`  :mirror: `:mirror:`  :mirror_ball: `:mirror_ball:`  :mobile_phone_off: `:mobile_phone_off:`  :moldova: `:moldova:`  :monaco: `:monaco:`  :money_mouth_face: `:money_mouth_face:`  :money_with_wings: `:money_with_wings:`  :moneybag: `:moneybag:`  :mongolia: `:mongolia:`  :monkey: `:monkey:`  :monkey_face: `:monkey_face:`  :monocle_face: `:monocle_face:`  :monorail: `:monorail:`  :montenegro: `:montenegro:`  :montserrat: `:montserrat:`  :moon: `:moon:`  :moon_cake: `:moon_cake:`  :moose: `:moose:`  :morocco: `:morocco:`  :mortar_board: `:mortar_board:`  :mosque: `:mosque:`  :mosquito: `:mosquito:`  :motor_boat: `:motor_boat:`  :motor_scooter: `:motor_scooter:`  :motorcycle: `:motorcycle:`  :motorized_wheelchair: `:motorized_wheelchair:`  :motorway: `:motorway:`  :mount_fuji: `:mount_fuji:`  :mountain: `:mountain:`  :mountain_bicyclist: `:mountain_bicyclist:`  :mountain_biking_man: `:mountain_biking_man:`  :mountain_biking_woman: `:mountain_biking_woman:`  :mountain_cableway: `:mountain_cableway:`  :mountain_railway: `:mountain_railway:`  :mountain_snow: `:mountain_snow:`  :mouse: `:mouse:`  :mouse2: `:mouse2:`  :mouse_trap: `:mouse_trap:`  :movie_camera: `:movie_camera:`  :moyai: `:moyai:`  :mozambique: `:mozambique:`  :mrs_claus: `:mrs_claus:`  :muscle: `:muscle:`  :mushroom: `:mushroom:`  :musical_keyboard: `:musical_keyboard:`  :musical_note: `:musical_note:`  :musical_score: `:musical_score:`  :mute: `:mute:`  :mx_claus: `:mx_claus:`  :myanmar: `:myanmar:`  :nail_care: `:nail_care:`  :name_badge: `:name_badge:`  :namibia: `:namibia:`  :national_park: `:national_park:`  :nauru: `:nauru:`  :nauseated_face: `:nauseated_face:`  :nazar_amulet: `:nazar_amulet:`  :neckbeard: `:neckbeard:`  :necktie: `:necktie:`  :negative_squared_cross_mark: `:negative_squared_cross_mark:`  :nepal: `:nepal:`  :nerd_face: `:nerd_face:`  :nest_with_eggs: `:nest_with_eggs:`  :nesting_dolls: `:nesting_dolls:`  :netherlands: `:netherlands:`  :neutral_face: `:neutral_face:`  :new: `:new:`  :new_caledonia: `:new_caledonia:`  :new_moon: `:new_moon:`  :new_moon_with_face: `:new_moon_with_face:`  :new_zealand: `:new_zealand:`  :newspaper: `:newspaper:`  :newspaper_roll: `:newspaper_roll:`  :next_track_button: `:next_track_button:`  :ng: `:ng:`  :ng_man: `:ng_man:`  :ng_woman: `:ng_woman:`  :nicaragua: `:nicaragua:`  :niger: `:niger:`  :nigeria: `:nigeria:`  :night_with_stars: `:night_with_stars:`  :nine: `:nine:`  :ninja: `:ninja:`  :niue: `:niue:`  :no_bell: `:no_bell:`  :no_bicycles: `:no_bicycles:`  :no_entry: `:no_entry:`  :no_entry_sign: `:no_entry_sign:`  :no_good: `:no_good:`  :no_good_man: `:no_good_man:`  :no_good_woman: `:no_good_woman:`  :no_mobile_phones: `:no_mobile_phones:`  :no_mouth: `:no_mouth:`  :no_pedestrians: `:no_pedestrians:`  :no_smoking: `:no_smoking:`  :non-potable_water: `:non-potable_water:`  :norfolk_island: `:norfolk_island:`  :north_korea: `:north_korea:`  :northern_mariana_islands: `:northern_mariana_islands:`  :norway: `:norway:`  :nose: `:nose:`  :notebook: `:notebook:`  :notebook_with_decorative_cover: `:notebook_with_decorative_cover:`  :notes: `:notes:`  :nut_and_bolt: `:nut_and_bolt:`  :o: `:o:`  :o2: `:o2:`  :ocean: `:ocean:`  :octocat: `:octocat:`  :octopus: `:octopus:`  :oden: `:oden:`  :office: `:office:`  :office_worker: `:office_worker:`  :oil_drum: `:oil_drum:`  :ok: `:ok:`  :ok_hand: `:ok_hand:`  :ok_man: `:ok_man:`  :ok_person: `:ok_person:`  :ok_woman: `:ok_woman:`  :old_key: `:old_key:`  :older_adult: `:older_adult:`  :older_man: `:older_man:`  :older_woman: `:older_woman:`  :olive: `:olive:`  :om: `:om:`  :oman: `:oman:`  :on: `:on:`  :oncoming_automobile: `:oncoming_automobile:`  :oncoming_bus: `:oncoming_bus:`  :oncoming_police_car: `:oncoming_police_car:`  :oncoming_taxi: `:oncoming_taxi:`  :one: `:one:`  :one_piece_swimsuit: `:one_piece_swimsuit:`  :onion: `:onion:`  :open_book: `:open_book:`  :open_file_folder: `:open_file_folder:`  :open_hands: `:open_hands:`  :open_mouth: `:open_mouth:`  :open_umbrella: `:open_umbrella:`  :ophiuchus: `:ophiuchus:`  :orange: `:orange:`  :orange_book: `:orange_book:`  :orange_circle: `:orange_circle:`  :orange_heart: `:orange_heart:`  :orange_square: `:orange_square:`  :orangutan: `:orangutan:`  :orthodox_cross: `:orthodox_cross:`  :otter: `:otter:`  :outbox_tray: `:outbox_tray:`  :owl: `:owl:`  :ox: `:ox:`  :oyster: `:oyster:`  :package: `:package:`  :page_facing_up: `:page_facing_up:`  :page_with_curl: `:page_with_curl:`  :pager: `:pager:`  :paintbrush: `:paintbrush:`  :pakistan: `:pakistan:`  :palau: `:palau:`  :palestinian_territories: `:palestinian_territories:`  :palm_down_hand: `:palm_down_hand:`  :palm_tree: `:palm_tree:`  :palm_up_hand: `:palm_up_hand:`  :palms_up_together: `:palms_up_together:`  :panama: `:panama:`  :pancakes: `:pancakes:`  :panda_face: `:panda_face:`  :paperclip: `:paperclip:`  :paperclips: `:paperclips:`  :papua_new_guinea: `:papua_new_guinea:`  :parachute: `:parachute:`  :paraguay: `:paraguay:`  :parasol_on_ground: `:parasol_on_ground:`  :parking: `:parking:`  :parrot: `:parrot:`  :part_alternation_mark: `:part_alternation_mark:`  :partly_sunny: `:partly_sunny:`  :partying_face: `:partying_face:`  :passenger_ship: `:passenger_ship:`  :passport_control: `:passport_control:`  :pause_button: `:pause_button:`  :paw_prints: `:paw_prints:`  :pea_pod: `:pea_pod:`  :peace_symbol: `:peace_symbol:`  :peach: `:peach:`  :peacock: `:peacock:`  :peanuts: `:peanuts:`  :pear: `:pear:`  :pen: `:pen:`  :pencil: `:pencil:`  :pencil2: `:pencil2:`  :penguin: `:penguin:`  :pensive: `:pensive:`  :people_holding_hands: `:people_holding_hands:`  :people_hugging: `:people_hugging:`  :performing_arts: `:performing_arts:`  :persevere: `:persevere:`  :person_bald: `:person_bald:`  :person_curly_hair: `:person_curly_hair:`  :person_feeding_baby: `:person_feeding_baby:`  :person_fencing: `:person_fencing:`  :person_in_manual_wheelchair: `:person_in_manual_wheelchair:`  :person_in_motorized_wheelchair: `:person_in_motorized_wheelchair:`  :person_in_tuxedo: `:person_in_tuxedo:`  :person_red_hair: `:person_red_hair:`  :person_white_hair: `:person_white_hair:`  :person_with_crown: `:person_with_crown:`  :person_with_probing_cane: `:person_with_probing_cane:`  :person_with_turban: `:person_with_turban:`  :person_with_veil: `:person_with_veil:`  :peru: `:peru:`  :petri_dish: `:petri_dish:`  :philippines: `:philippines:`  :phone: `:phone:`  :pick: `:pick:`  :pickup_truck: `:pickup_truck:`  :pie: `:pie:`  :pig: `:pig:`  :pig2: `:pig2:`  :pig_nose: `:pig_nose:`  :pill: `:pill:`  :pilot: `:pilot:`  :pinata: `:pinata:`  :pinched_fingers: `:pinched_fingers:`  :pinching_hand: `:pinching_hand:`  :pineapple: `:pineapple:`  :ping_pong: `:ping_pong:`  :pink_heart: `:pink_heart:`  :pirate_flag: `:pirate_flag:`  :pisces: `:pisces:`  :pitcairn_islands: `:pitcairn_islands:`  :pizza: `:pizza:`  :placard: `:placard:`  :place_of_worship: `:place_of_worship:`  :plate_with_cutlery: `:plate_with_cutlery:`  :play_or_pause_button: `:play_or_pause_button:`  :playground_slide: `:playground_slide:`  :pleading_face: `:pleading_face:`  :plunger: `:plunger:`  :point_down: `:point_down:`  :point_left: `:point_left:`  :point_right: `:point_right:`  :point_up: `:point_up:`  :point_up_2: `:point_up_2:`  :poland: `:poland:`  :polar_bear: `:polar_bear:`  :police_car: `:police_car:`  :police_officer: `:police_officer:`  :policeman: `:policeman:`  :policewoman: `:policewoman:`  :poodle: `:poodle:`  :poop: `:poop:`  :popcorn: `:popcorn:`  :portugal: `:portugal:`  :post_office: `:post_office:`  :postal_horn: `:postal_horn:`  :postbox: `:postbox:`  :potable_water: `:potable_water:`  :potato: `:potato:`  :potted_plant: `:potted_plant:`  :pouch: `:pouch:`  :poultry_leg: `:poultry_leg:`  :pound: `:pound:`  :pouring_liquid: `:pouring_liquid:`  :pout: `:pout:`  :pouting_cat: `:pouting_cat:`  :pouting_face: `:pouting_face:`  :pouting_man: `:pouting_man:`  :pouting_woman: `:pouting_woman:`  :pray: `:pray:`  :prayer_beads: `:prayer_beads:`  :pregnant_man: `:pregnant_man:`  :pregnant_person: `:pregnant_person:`  :pregnant_woman: `:pregnant_woman:`  :pretzel: `:pretzel:`  :previous_track_button: `:previous_track_button:`  :prince: `:prince:`  :princess: `:princess:`  :printer: `:printer:`  :probing_cane: `:probing_cane:`  :puerto_rico: `:puerto_rico:`  :punch: `:punch:`  :purple_circle: `:purple_circle:`  :purple_heart: `:purple_heart:`  :purple_square: `:purple_square:`  :purse: `:purse:`  :pushpin: `:pushpin:`  :put_litter_in_its_place: `:put_litter_in_its_place:`  :qatar: `:qatar:`  :question: `:question:`  :rabbit: `:rabbit:`  :rabbit2: `:rabbit2:`  :raccoon: `:raccoon:`  :racehorse: `:racehorse:`  :racing_car: `:racing_car:`  :radio: `:radio:`  :radio_button: `:radio_button:`  :radioactive: `:radioactive:`  :rage: `:rage:`  :rage1: `:rage1:`  :rage2: `:rage2:`  :rage3: `:rage3:`  :rage4: `:rage4:`  :railway_car: `:railway_car:`  :railway_track: `:railway_track:`  :rainbow: `:rainbow:`  :rainbow_flag: `:rainbow_flag:`  :raised_back_of_hand: `:raised_back_of_hand:`  :raised_eyebrow: `:raised_eyebrow:`  :raised_hand: `:raised_hand:`  :raised_hand_with_fingers_splayed: `:raised_hand_with_fingers_splayed:`  :raised_hands: `:raised_hands:`  :raising_hand: `:raising_hand:`  :raising_hand_man: `:raising_hand_man:`  :raising_hand_woman: `:raising_hand_woman:`  :ram: `:ram:`  :ramen: `:ramen:`  :rat: `:rat:`  :razor: `:razor:`  :receipt: `:receipt:`  :record_button: `:record_button:`  :recycle: `:recycle:`  :red_car: `:red_car:`  :red_circle: `:red_circle:`  :red_envelope: `:red_envelope:`  :red_haired_man: `:red_haired_man:`  :red_haired_woman: `:red_haired_woman:`  :red_square: `:red_square:`  :registered: `:registered:`  :relaxed: `:relaxed:`  :relieved: `:relieved:`  :reminder_ribbon: `:reminder_ribbon:`  :repeat: `:repeat:`  :repeat_one: `:repeat_one:`  :rescue_worker_helmet: `:rescue_worker_helmet:`  :restroom: `:restroom:`  :reunion: `:reunion:`  :revolving_hearts: `:revolving_hearts:`  :rewind: `:rewind:`  :rhinoceros: `:rhinoceros:`  :ribbon: `:ribbon:`  :rice: `:rice:`  :rice_ball: `:rice_ball:`  :rice_cracker: `:rice_cracker:`  :rice_scene: `:rice_scene:`  :right_anger_bubble: `:right_anger_bubble:`  :rightwards_hand: `:rightwards_hand:`  :rightwards_pushing_hand: `:rightwards_pushing_hand:`  :ring: `:ring:`  :ring_buoy: `:ring_buoy:`  :ringed_planet: `:ringed_planet:`  :robot: `:robot:`  :rock: `:rock:`  :rocket: `:rocket:`  :rofl: `:rofl:`  :roll_eyes: `:roll_eyes:`  :roll_of_paper: `:roll_of_paper:`  :roller_coaster: `:roller_coaster:`  :roller_skate: `:roller_skate:`  :romania: `:romania:`  :rooster: `:rooster:`  :rose: `:rose:`  :rosette: `:rosette:`  :rotating_light: `:rotating_light:`  :round_pushpin: `:round_pushpin:`  :rowboat: `:rowboat:`  :rowing_man: `:rowing_man:`  :rowing_woman: `:rowing_woman:`  :ru: `:ru:`  :rugby_football: `:rugby_football:`  :runner: `:runner:`  :running: `:running:`  :running_man: `:running_man:`  :running_shirt_with_sash: `:running_shirt_with_sash:`  :running_woman: `:running_woman:`  :rwanda: `:rwanda:`  :sa: `:sa:`  :safety_pin: `:safety_pin:`  :safety_vest: `:safety_vest:`  :sagittarius: `:sagittarius:`  :sailboat: `:sailboat:`  :sake: `:sake:`  :salt: `:salt:`  :saluting_face: `:saluting_face:`  :samoa: `:samoa:`  :san_marino: `:san_marino:`  :sandal: `:sandal:`  :sandwich: `:sandwich:`  :santa: `:santa:`  :sao_tome_principe: `:sao_tome_principe:`  :sari: `:sari:`  :sassy_man: `:sassy_man:`  :sassy_woman: `:sassy_woman:`  :satellite: `:satellite:`  :satisfied: `:satisfied:`  :saudi_arabia: `:saudi_arabia:`  :sauna_man: `:sauna_man:`  :sauna_person: `:sauna_person:`  :sauna_woman: `:sauna_woman:`  :sauropod: `:sauropod:`  :saxophone: `:saxophone:`  :scarf: `:scarf:`  :school: `:school:`  :school_satchel: `:school_satchel:`  :scientist: `:scientist:`  :scissors: `:scissors:`  :scorpion: `:scorpion:`  :scorpius: `:scorpius:`  :scotland: `:scotland:`  :scream: `:scream:`  :scream_cat: `:scream_cat:`  :screwdriver: `:screwdriver:`  :scroll: `:scroll:`  :seal: `:seal:`  :seat: `:seat:`  :secret: `:secret:`  :see_no_evil: `:see_no_evil:`  :seedling: `:seedling:`  :selfie: `:selfie:`  :senegal: `:senegal:`  :serbia: `:serbia:`  :service_dog: `:service_dog:`  :seven: `:seven:`  :sewing_needle: `:sewing_needle:`  :seychelles: `:seychelles:`  :shaking_face: `:shaking_face:`  :shallow_pan_of_food: `:shallow_pan_of_food:`  :shamrock: `:shamrock:`  :shark: `:shark:`  :shaved_ice: `:shaved_ice:`  :sheep: `:sheep:`  :shell: `:shell:`  :shield: `:shield:`  :shinto_shrine: `:shinto_shrine:`  :ship: `:ship:`  :shipit: `:shipit:`  :shirt: `:shirt:`  :shit: `:shit:`  :shoe: `:shoe:`  :shopping: `:shopping:`  :shopping_cart: `:shopping_cart:`  :shorts: `:shorts:`  :shower: `:shower:`  :shrimp: `:shrimp:`  :shrug: `:shrug:`  :shushing_face: `:shushing_face:`  :sierra_leone: `:sierra_leone:`  :signal_strength: `:signal_strength:`  :singapore: `:singapore:`  :singer: `:singer:`  :sint_maarten: `:sint_maarten:`  :six: `:six:`  :six_pointed_star: `:six_pointed_star:`  :skateboard: `:skateboard:`  :ski: `:ski:`  :skier: `:skier:`  :skull: `:skull:`  :skull_and_crossbones: `:skull_and_crossbones:`  :skunk: `:skunk:`  :sled: `:sled:`  :sleeping: `:sleeping:`  :sleeping_bed: `:sleeping_bed:`  :sleepy: `:sleepy:`  :slightly_frowning_face: `:slightly_frowning_face:`  :slightly_smiling_face: `:slightly_smiling_face:`  :slot_machine: `:slot_machine:`  :sloth: `:sloth:`  :slovakia: `:slovakia:`  :slovenia: `:slovenia:`  :small_airplane: `:small_airplane:`  :small_blue_diamond: `:small_blue_diamond:`  :small_orange_diamond: `:small_orange_diamond:`  :small_red_triangle: `:small_red_triangle:`  :small_red_triangle_down: `:small_red_triangle_down:`  :smile: `:smile:`  :smile_cat: `:smile_cat:`  :smiley: `:smiley:`  :smiley_cat: `:smiley_cat:`  :smiling_face_with_tear: `:smiling_face_with_tear:`  :smiling_face_with_three_hearts: `:smiling_face_with_three_hearts:`  :smiling_imp: `:smiling_imp:`  :smirk: `:smirk:`  :smirk_cat: `:smirk_cat:`  :smoking: `:smoking:`  :snail: `:snail:`  :snake: `:snake:`  :sneezing_face: `:sneezing_face:`  :snowboarder: `:snowboarder:`  :snowflake: `:snowflake:`  :snowman: `:snowman:`  :snowman_with_snow: `:snowman_with_snow:`  :soap: `:soap:`  :sob: `:sob:`  :soccer: `:soccer:`  :socks: `:socks:`  :softball: `:softball:`  :solomon_islands: `:solomon_islands:`  :somalia: `:somalia:`  :soon: `:soon:`  :sos: `:sos:`  :sound: `:sound:`  :south_africa: `:south_africa:`  :south_georgia_south_sandwich_islands: `:south_georgia_south_sandwich_islands:`  :south_sudan: `:south_sudan:`  :space_invader: `:space_invader:`  :spades: `:spades:`  :spaghetti: `:spaghetti:`  :sparkle: `:sparkle:`  :sparkler: `:sparkler:`  :sparkles: `:sparkles:`  :sparkling_heart: `:sparkling_heart:`  :speak_no_evil: `:speak_no_evil:`  :speaker: `:speaker:`  :speaking_head: `:speaking_head:`  :speech_balloon: `:speech_balloon:`  :speedboat: `:speedboat:`  :spider: `:spider:`  :spider_web: `:spider_web:`  :spiral_calendar: `:spiral_calendar:`  :spiral_notepad: `:spiral_notepad:`  :sponge: `:sponge:`  :spoon: `:spoon:`  :squid: `:squid:`  :sri_lanka: `:sri_lanka:`  :st_barthelemy: `:st_barthelemy:`  :st_helena: `:st_helena:`  :st_kitts_nevis: `:st_kitts_nevis:`  :st_lucia: `:st_lucia:`  :st_martin: `:st_martin:`  :st_pierre_miquelon: `:st_pierre_miquelon:`  :st_vincent_grenadines: `:st_vincent_grenadines:`  :stadium: `:stadium:`  :standing_man: `:standing_man:`  :standing_person: `:standing_person:`  :standing_woman: `:standing_woman:`  :star: `:star:`  :star2: `:star2:`  :star_and_crescent: `:star_and_crescent:`  :star_of_david: `:star_of_david:`  :star_struck: `:star_struck:`  :stars: `:stars:`  :station: `:station:`  :statue_of_liberty: `:statue_of_liberty:`  :steam_locomotive: `:steam_locomotive:`  :stethoscope: `:stethoscope:`  :stew: `:stew:`  :stop_button: `:stop_button:`  :stop_sign: `:stop_sign:`  :stopwatch: `:stopwatch:`  :straight_ruler: `:straight_ruler:`  :strawberry: `:strawberry:`  :stuck_out_tongue: `:stuck_out_tongue:`  :stuck_out_tongue_closed_eyes: `:stuck_out_tongue_closed_eyes:`  :stuck_out_tongue_winking_eye: `:stuck_out_tongue_winking_eye:`  :student: `:student:`  :studio_microphone: `:studio_microphone:`  :stuffed_flatbread: `:stuffed_flatbread:`  :sudan: `:sudan:`  :sun_behind_large_cloud: `:sun_behind_large_cloud:`  :sun_behind_rain_cloud: `:sun_behind_rain_cloud:`  :sun_behind_small_cloud: `:sun_behind_small_cloud:`  :sun_with_face: `:sun_with_face:`  :sunflower: `:sunflower:`  :sunglasses: `:sunglasses:`  :sunny: `:sunny:`  :sunrise: `:sunrise:`  :sunrise_over_mountains: `:sunrise_over_mountains:`  :superhero: `:superhero:`  :superhero_man: `:superhero_man:`  :superhero_woman: `:superhero_woman:`  :supervillain: `:supervillain:`  :supervillain_man: `:supervillain_man:`  :supervillain_woman: `:supervillain_woman:`  :surfer: `:surfer:`  :surfing_man: `:surfing_man:`  :surfing_woman: `:surfing_woman:`  :suriname: `:suriname:`  :sushi: `:sushi:`  :suspect: `:suspect:`  :suspension_railway: `:suspension_railway:`  :svalbard_jan_mayen: `:svalbard_jan_mayen:`  :swan: `:swan:`  :swaziland: `:swaziland:`  :sweat: `:sweat:`  :sweat_drops: `:sweat_drops:`  :sweat_smile: `:sweat_smile:`  :sweden: `:sweden:`  :sweet_potato: `:sweet_potato:`  :swim_brief: `:swim_brief:`  :swimmer: `:swimmer:`  :swimming_man: `:swimming_man:`  :swimming_woman: `:swimming_woman:`  :switzerland: `:switzerland:`  :symbols: `:symbols:`  :synagogue: `:synagogue:`  :syria: `:syria:`  :syringe: `:syringe:`  :t-rex: `:t-rex:`  :taco: `:taco:`  :tada: `:tada:`  :taiwan: `:taiwan:`  :tajikistan: `:tajikistan:`  :takeout_box: `:takeout_box:`  :tamale: `:tamale:`  :tanabata_tree: `:tanabata_tree:`  :tangerine: `:tangerine:`  :tanzania: `:tanzania:`  :taurus: `:taurus:`  :taxi: `:taxi:`  :tea: `:tea:`  :teacher: `:teacher:`  :teapot: `:teapot:`  :technologist: `:technologist:`  :teddy_bear: `:teddy_bear:`  :telephone: `:telephone:`  :telephone_receiver: `:telephone_receiver:`  :telescope: `:telescope:`  :tennis: `:tennis:`  :tent: `:tent:`  :test_tube: `:test_tube:`  :thailand: `:thailand:`  :thermometer: `:thermometer:`  :thinking: `:thinking:`  :thong_sandal: `:thong_sandal:`  :thought_balloon: `:thought_balloon:`  :thread: `:thread:`  :three: `:three:`  :thumbsdown: `:thumbsdown:`  :thumbsup: `:thumbsup:`  :ticket: `:ticket:`  :tickets: `:tickets:`  :tiger: `:tiger:`  :tiger2: `:tiger2:`  :timer_clock: `:timer_clock:`  :timor_leste: `:timor_leste:`  :tipping_hand_man: `:tipping_hand_man:`  :tipping_hand_person: `:tipping_hand_person:`  :tipping_hand_woman: `:tipping_hand_woman:`  :tired_face: `:tired_face:`  :tm: `:tm:`  :togo: `:togo:`  :toilet: `:toilet:`  :tokelau: `:tokelau:`  :tokyo_tower: `:tokyo_tower:`  :tomato: `:tomato:`  :tonga: `:tonga:`  :tongue: `:tongue:`  :toolbox: `:toolbox:`  :tooth: `:tooth:`  :toothbrush: `:toothbrush:`  :top: `:top:`  :tophat: `:tophat:`  :tornado: `:tornado:`  :tr: `:tr:`  :trackball: `:trackball:`  :tractor: `:tractor:`  :traffic_light: `:traffic_light:`  :train: `:train:`  :train2: `:train2:`  :tram: `:tram:`  :transgender_flag: `:transgender_flag:`  :transgender_symbol: `:transgender_symbol:`  :triangular_flag_on_post: `:triangular_flag_on_post:`  :triangular_ruler: `:triangular_ruler:`  :trident: `:trident:`  :trinidad_tobago: `:trinidad_tobago:`  :tristan_da_cunha: `:tristan_da_cunha:`  :triumph: `:triumph:`  :troll: `:troll:`  :trolleybus: `:trolleybus:`  :trollface: `:trollface:`  :trophy: `:trophy:`  :tropical_drink: `:tropical_drink:`  :tropical_fish: `:tropical_fish:`  :truck: `:truck:`  :trumpet: `:trumpet:`  :tshirt: `:tshirt:`  :tulip: `:tulip:`  :tumbler_glass: `:tumbler_glass:`  :tunisia: `:tunisia:`  :turkey: `:turkey:`  :turkmenistan: `:turkmenistan:`  :turks_caicos_islands: `:turks_caicos_islands:`  :turtle: `:turtle:`  :tuvalu: `:tuvalu:`  :tv: `:tv:`  :twisted_rightwards_arrows: `:twisted_rightwards_arrows:`  :two: `:two:`  :two_hearts: `:two_hearts:`  :two_men_holding_hands: `:two_men_holding_hands:`  :two_women_holding_hands: `:two_women_holding_hands:`  :u5272: `:u5272:`  :u5408: `:u5408:`  :u55b6: `:u55b6:`  :u6307: `:u6307:`  :u6708: `:u6708:`  :u6709: `:u6709:`  :u6e80: `:u6e80:`  :u7121: `:u7121:`  :u7533: `:u7533:`  :u7981: `:u7981:`  :u7a7a: `:u7a7a:`  :uganda: `:uganda:`  :uk: `:uk:`  :ukraine: `:ukraine:`  :umbrella: `:umbrella:`  :unamused: `:unamused:`  :underage: `:underage:`  :unicorn: `:unicorn:`  :united_arab_emirates: `:united_arab_emirates:`  :united_nations: `:united_nations:`  :unlock: `:unlock:`  :up: `:up:`  :upside_down_face: `:upside_down_face:`  :uruguay: `:uruguay:`  :us: `:us:`  :us_outlying_islands: `:us_outlying_islands:`  :us_virgin_islands: `:us_virgin_islands:`  :uzbekistan: `:uzbekistan:`  :v: `:v:`  :vampire: `:vampire:`  :vampire_man: `:vampire_man:`  :vampire_woman: `:vampire_woman:`  :vanuatu: `:vanuatu:`  :vatican_city: `:vatican_city:`  :venezuela: `:venezuela:`  :vertical_traffic_light: `:vertical_traffic_light:`  :vhs: `:vhs:`  :vibration_mode: `:vibration_mode:`  :video_camera: `:video_camera:`  :video_game: `:video_game:`  :vietnam: `:vietnam:`  :violin: `:violin:`  :virgo: `:virgo:`  :volcano: `:volcano:`  :volleyball: `:volleyball:`  :vomiting_face: `:vomiting_face:`  :vs: `:vs:`  :vulcan_salute: `:vulcan_salute:`  :waffle: `:waffle:`  :wales: `:wales:`  :walking: `:walking:`  :walking_man: `:walking_man:`  :walking_woman: `:walking_woman:`  :wallis_futuna: `:wallis_futuna:`  :waning_crescent_moon: `:waning_crescent_moon:`  :waning_gibbous_moon: `:waning_gibbous_moon:`  :warning: `:warning:`  :wastebasket: `:wastebasket:`  :watch: `:watch:`  :water_buffalo: `:water_buffalo:`  :water_polo: `:water_polo:`  :watermelon: `:watermelon:`  :wave: `:wave:`  :wavy_dash: `:wavy_dash:`  :waxing_crescent_moon: `:waxing_crescent_moon:`  :waxing_gibbous_moon: `:waxing_gibbous_moon:`  :wc: `:wc:`  :weary: `:weary:`  :wedding: `:wedding:`  :weight_lifting: `:weight_lifting:`  :weight_lifting_man: `:weight_lifting_man:`  :weight_lifting_woman: `:weight_lifting_woman:`  :western_sahara: `:western_sahara:`  :whale: `:whale:`  :whale2: `:whale2:`  :wheel: `:wheel:`  :wheel_of_dharma: `:wheel_of_dharma:`  :wheelchair: `:wheelchair:`  :white_check_mark: `:white_check_mark:`  :white_circle: `:white_circle:`  :white_flag: `:white_flag:`  :white_flower: `:white_flower:`  :white_haired_man: `:white_haired_man:`  :white_haired_woman: `:white_haired_woman:`  :white_heart: `:white_heart:`  :white_large_square: `:white_large_square:`  :white_medium_small_square: `:white_medium_small_square:`  :white_medium_square: `:white_medium_square:`  :white_small_square: `:white_small_square:`  :white_square_button: `:white_square_button:`  :wilted_flower: `:wilted_flower:`  :wind_chime: `:wind_chime:`  :wind_face: `:wind_face:`  :window: `:window:`  :wine_glass: `:wine_glass:`  :wing: `:wing:`  :wink: `:wink:`  :wireless: `:wireless:`  :wolf: `:wolf:`  :woman: `:woman:`  :woman_artist: `:woman_artist:`  :woman_astronaut: `:woman_astronaut:`  :woman_beard: `:woman_beard:`  :woman_cartwheeling: `:woman_cartwheeling:`  :woman_cook: `:woman_cook:`  :woman_dancing: `:woman_dancing:`  :woman_facepalming: `:woman_facepalming:`  :woman_factory_worker: `:woman_factory_worker:`  :woman_farmer: `:woman_farmer:`  :woman_feeding_baby: `:woman_feeding_baby:`  :woman_firefighter: `:woman_firefighter:`  :woman_health_worker: `:woman_health_worker:`  :woman_in_manual_wheelchair: `:woman_in_manual_wheelchair:`  :woman_in_motorized_wheelchair: `:woman_in_motorized_wheelchair:`  :woman_in_tuxedo: `:woman_in_tuxedo:`  :woman_judge: `:woman_judge:`  :woman_juggling: `:woman_juggling:`  :woman_mechanic: `:woman_mechanic:`  :woman_office_worker: `:woman_office_worker:`  :woman_pilot: `:woman_pilot:`  :woman_playing_handball: `:woman_playing_handball:`  :woman_playing_water_polo: `:woman_playing_water_polo:`  :woman_scientist: `:woman_scientist:`  :woman_shrugging: `:woman_shrugging:`  :woman_singer: `:woman_singer:`  :woman_student: `:woman_student:`  :woman_teacher: `:woman_teacher:`  :woman_technologist: `:woman_technologist:`  :woman_with_headscarf: `:woman_with_headscarf:`  :woman_with_probing_cane: `:woman_with_probing_cane:`  :woman_with_turban: `:woman_with_turban:`  :woman_with_veil: `:woman_with_veil:`  :womans_clothes: `:womans_clothes:`  :womans_hat: `:womans_hat:`  :women_wrestling: `:women_wrestling:`  :womens: `:womens:`  :wood: `:wood:`  :woozy_face: `:woozy_face:`  :world_map: `:world_map:`  :worm: `:worm:`  :worried: `:worried:`  :wrench: `:wrench:`  :wrestling: `:wrestling:`  :writing_hand: `:writing_hand:`  :x: `:x:`  :x_ray: `:x_ray:`  :yarn: `:yarn:`  :yawning_face: `:yawning_face:`  :yellow_circle: `:yellow_circle:`  :yellow_heart: `:yellow_heart:`  :yellow_square: `:yellow_square:`  :yemen: `:yemen:`  :yen: `:yen:`  :yin_yang: `:yin_yang:`  :yo_yo: `:yo_yo:`  :yum: `:yum:`  :zambia: `:zambia:`  :zany_face: `:zany_face:`  :zap: `:zap:`  :zebra: `:zebra:`  :zero: `:zero:`  :zimbabwe: `:zimbabwe:`  :zipper_mouth_face: `:zipper_mouth_face:`  :zombie: `:zombie:`  :zombie_man: `:zombie_man:`  :zombie_woman: `:zombie_woman:`  :zzz: `:zzz:`"},{"location":"docsify_docs/docsify/docs/helpers/","title":"Doc helper","text":"<p>docsify extends Markdown syntax to make your documents more readable.</p> <p>Note: For the special code syntax cases, it's better to put them within code backticks to avoid any conflict from configurations or emojis.</p>"},{"location":"docsify_docs/docsify/docs/helpers/#callouts","title":"Callouts","text":""},{"location":"docsify_docs/docsify/docs/helpers/#important-content","title":"Important content","text":"<p>Important content like:</p> <pre><code>!&gt; **Time** is money, my friend!\n</code></pre> <p>is rendered as:</p> <p>!&gt; Time is money, my friend!</p>"},{"location":"docsify_docs/docsify/docs/helpers/#tips","title":"Tips","text":"<p>General tips like:</p> <pre><code>?&gt; _TODO_ unit test\n</code></pre> <p>are rendered as:</p> <p>?&gt; TODO unit test</p>"},{"location":"docsify_docs/docsify/docs/helpers/#link-attributes","title":"Link attributes","text":""},{"location":"docsify_docs/docsify/docs/helpers/#disabled","title":"disabled","text":"<pre><code>[link](/demo ':disabled')\n</code></pre>"},{"location":"docsify_docs/docsify/docs/helpers/#href","title":"href","text":"<p>Sometimes we will use some other relative path for the link, and we have to tell docsify that we don't need to compile this link. For example:</p> <pre><code>[link](/demo/)\n</code></pre> <p>It will be compiled to <code>&lt;a href=\"/#/demo/\"&gt;link&lt;/a&gt;</code> and will load <code>/demo/README.md</code>. Maybe you want to jump to <code>/demo/index.html</code>.</p> <p>Now you can do that</p> <pre><code>[link](/demo/ ':ignore')\n</code></pre> <p>You will get <code>&lt;a href=\"/demo/\"&gt;link&lt;/a&gt;</code>html. Do not worry, you can still set the title for the link.</p> <pre><code>[link](/demo/ ':ignore title')\n\n&lt;a href=\"/demo/\" title=\"title\"&gt;link&lt;/a&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/helpers/#target","title":"target","text":"<pre><code>[link](/demo ':target=_blank')\n[link](/demo2 ':target=_self')\n</code></pre>"},{"location":"docsify_docs/docsify/docs/helpers/#task-lists","title":"Task lists","text":"<pre><code>- [ ] foo\n- bar\n- [x] baz\n- [] bam &lt;~ not working\n  - [ ] bim\n  - [ ] lim\n</code></pre> <ul> <li>[ ] foo</li> <li>bar</li> <li>[x] baz</li> <li>[] bam &lt;~ not working</li> <li>[ ] bim</li> <li>[ ] lim</li> </ul>"},{"location":"docsify_docs/docsify/docs/helpers/#images","title":"Images","text":""},{"location":"docsify_docs/docsify/docs/helpers/#class-names","title":"Class names","text":"<pre><code>![logo](https://docsify.js.org/_media/icon.svg ':class=someCssClass')\n</code></pre>"},{"location":"docsify_docs/docsify/docs/helpers/#ids","title":"IDs","text":"<pre><code>![logo](https://docsify.js.org/_media/icon.svg ':id=someCssId')\n</code></pre>"},{"location":"docsify_docs/docsify/docs/helpers/#sizes","title":"Sizes","text":"<pre><code>![logo](https://docsify.js.org/_media/icon.svg ':size=WIDTHxHEIGHT')\n![logo](https://docsify.js.org/_media/icon.svg ':size=50x100')\n![logo](https://docsify.js.org/_media/icon.svg ':size=100')\n\n&lt;!-- Support percentage --&gt;\n\n![logo](https://docsify.js.org/_media/icon.svg ':size=10%')\n</code></pre>"},{"location":"docsify_docs/docsify/docs/helpers/#heading-ids","title":"Heading IDs","text":"<pre><code>### Hello, world! :id=hello-world\n</code></pre>"},{"location":"docsify_docs/docsify/docs/helpers/#markdown-html","title":"Markdown + HTML","text":"<p>You need to insert a space between the html and markdown content. This is useful for rendering markdown content in the details element.</p> <pre><code>&lt;details&gt;\n&lt;summary&gt;Self-assessment (Click to expand)&lt;/summary&gt;\n\n- Abc\n- Abc\n\n&lt;/details&gt;\n</code></pre> Self-assessment (Click to expand)  - Abc - Abc   <p>Markdown content can also be wrapped in html tags.</p> <pre><code>&lt;div style='color: red'&gt;\n\n- listitem\n- listitem\n- listitem\n\n&lt;/div&gt;\n</code></pre>   - listitem - listitem - listitem"},{"location":"docsify_docs/docsify/docs/language-highlight/","title":"Language highlighting","text":""},{"location":"docsify_docs/docsify/docs/language-highlight/#prism","title":"Prism","text":"<p>Docsify uses Prism for syntax highlighting within code blocks. Prism supports the following languages by default (additional language support also available):</p> <ul> <li>Markup: HTML, XML, SVG, MathML, SSML, Atom, RSS</li> <li>CSS</li> <li>C-like</li> <li>JavaScript</li> </ul> <p>To enable syntax highlighting, create a markdown codeblock using backticks (<code>```</code>) with a language specified on the first line (e.g., <code>html</code>, <code>css</code>, <code>js</code>):</p> <pre><code>```html\n&lt;p&gt;This is a paragraph&lt;/p&gt;\n&lt;a href=\"//docsify.js.org/\"&gt;Docsify&lt;/a&gt;\n```\n</code></pre> <pre><code>```css\np {\n  color: red;\n}\n```\n</code></pre> <pre><code>```js\nfunction add(a, b) {\n  return a + b;\n}\n```\n</code></pre> <p>The above markdown will be rendered as:</p> <pre><code>&lt;p&gt;This is a paragraph&lt;/p&gt;\n&lt;a href=\"//docsify.js.org/\"&gt;Docsify&lt;/a&gt;\n</code></pre> <pre><code>p {\n  color: red;\n}\n</code></pre> <pre><code>function add(a, b) {\n  return a + b;\n}\n</code></pre>"},{"location":"docsify_docs/docsify/docs/language-highlight/#language-support","title":"Language support","text":"<p>Support for additional languages is available by loading the Prism grammar files:</p> <p>!&gt; Prism grammar files must be loaded after Docsify.</p> <pre><code>&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-bash.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-docker.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-git.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-java.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-jsx.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-markdown.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-php.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-rust.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-sql.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-swift.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-typescript.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/prismjs@1/components/prism-yaml.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/language-highlight/#theme-support","title":"Theme support","text":"<p>Docsify's official themes are compatible with Prism syntax highlighting themes.</p> <p>!&gt; Prism themes must be loaded after Docsify themes.</p> <pre><code>&lt;!-- Light and dark mode --&gt;\n&lt;link\n  rel=\"stylesheet\"\n  href=\"//cdn.jsdelivr.net/npm/prism-themes@1/themes/prism-one-light.min.css\"\n/&gt;\n</code></pre> <p>Themes can be applied in light and/or dark mode</p> <pre><code>&lt;!-- Dark mode only --&gt;\n&lt;link\n  rel=\"stylesheet\"\n  media=\"(prefers-color-scheme: dark)\"\n  href=\"//cdn.jsdelivr.net/npm/prism-themes@1/themes/prism-one-dark.min.css\"\n/&gt;\n\n&lt;!-- Light mode only --&gt;\n&lt;link\n  rel=\"stylesheet\"\n  media=\"(prefers-color-scheme: light)\"\n  href=\"//cdn.jsdelivr.net/npm/prism-themes@1/themes/prism-one-light.min.css\"\n/&gt;\n</code></pre> <p>The following Docsify theme properties will override Prism theme styles by default:</p> <pre><code>--border-radius\n--font-family-mono\n--font-size-mono\n</code></pre> <p>To use the values specified in the Prism theme, set the desired theme property to <code>unset</code>:</p> <pre><code>&lt;style&gt;\n  :root {\n    --border-radius   : unset;\n    --font-family-mono: unset;\n    --font-size-mono  : unset;\n  }\n&lt;/style&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/language-highlight/#dynamic-content","title":"Dynamic content","text":"<p>Dynamically generated Code blocks can be highlighted using Prism's <code>highlightElement()</code> method:</p> <pre><code>const code = document.createElement('code');\ncode.innerHTML = \"console.log('Hello World!')\";\ncode.setAttribute('class', 'language-javascript');\nPrism.highlightElement(code);\n</code></pre>"},{"location":"docsify_docs/docsify/docs/markdown/","title":"Markdown configuration","text":"<p>docsify uses marked v13+ as its Markdown parser. You can customize how it renders your Markdown content to HTML by customizing <code>renderer</code>:</p> <pre><code>window.$docsify = {\n  markdown: {\n    smartypants: true,\n    renderer: {\n      link() {\n        // ...\n      },\n    },\n  },\n};\n</code></pre> <p>?&gt; Configuration Options Reference: marked documentation</p> <p>You can completely customize the parsing rules.</p> <pre><code>window.$docsify = {\n  markdown(marked, renderer) {\n    // ...\n\n    return marked;\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/markdown/#supports-mermaid","title":"Supports mermaid","text":"<p>!&gt; Currently, docsify doesn't support the async mermaid render (the latest mermaid version supported is <code>v9.3.0</code>).</p> <pre><code>//  &lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.css\"&gt;\n//  &lt;script src=\"//cdn.jsdelivr.net/npm/mermaid@9.3.0/dist/mermaid.min.js\"&gt;&lt;/script&gt;\n\nlet num = 0;\nmermaid.initialize({ startOnLoad: false });\n\nwindow.$docsify = {\n  markdown: {\n    renderer: {\n      code({ text, lang }) {\n        if (lang === 'mermaid') {\n          return /* html */ `\n            &lt;div class=\"mermaid\"&gt;${mermaid.render(\n              'mermaid-svg-' + num++,\n              text,\n            )}&lt;/div&gt;\n          `;\n        }\n        return this.origin.code.apply(this, arguments);\n      },\n    },\n  },\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/plugins/","title":"List of Plugins","text":""},{"location":"docsify_docs/docsify/docs/plugins/#full-text-search","title":"Full text search","text":"<p>By default, the hyperlink on the current page is recognized and the content is saved in <code>localStorage</code>. You can also specify the path to the files.</p> <pre><code>&lt;script&gt;\n  window.$docsify = {\n    search: 'auto', // default\n\n    search: [\n      '/',            // =&gt; /README.md\n      '/guide',       // =&gt; /guide.md\n      '/get-started', // =&gt; /get-started.md\n      '/zh-cn/',      // =&gt; /zh-cn/README.md\n    ],\n\n    // Complete configuration parameters\n    search: {\n      // Location in sidebar (default: prepended as first child)\n      // Optionally specify insertAfter or insertBefore (not both)\n      insertAfter: '.app-name', // CSS selector in .sidebar scope\n      insertBefore: '.sidebar-nav', // CSS selector in .sidebar scope\n\n      maxAge: 86400000, // Expiration time, the default one day\n      paths: [], // or 'auto'\n      placeholder: 'Type to search',\n\n      // Localization\n      placeholder: {\n        '/zh-cn/': '\u641c\u7d22',\n        '/': 'Type to search',\n      },\n\n      noData: 'No Results!',\n\n      // Localization\n      noData: {\n        '/zh-cn/': '\u627e\u4e0d\u5230\u7ed3\u679c',\n        '/': 'No Results',\n      },\n\n      // Headline depth, 1 - 6\n      depth: 2,\n\n      hideOtherSidebarContent: true, // Deprecated as of v5\n\n      // To avoid search index collision\n      // between multiple websites under the same domain\n      namespace: 'website-1',\n\n      // Use different indexes for path prefixes (namespaces).\n      // NOTE: Only works in 'auto' mode.\n      //\n      // When initialiazing an index, we look for the first path from the sidebar.\n      // If it matches the prefix from the list, we switch to the corresponding index.\n      pathNamespaces: ['/zh-cn', '/ru-ru', '/ru-ru/v1'],\n\n      // You can provide a regexp to match prefixes. In this case,\n      // the matching substring will be used to identify the index\n      pathNamespaces: /^(\\/(zh-cn|ru-ru))?(\\/(v1|v2))?/,\n    },\n  };\n&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/search.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>This plugin ignores diacritical marks when performing a full text search (e.g., \"cafe\" will also match \"caf\u00e9\").</p>"},{"location":"docsify_docs/docsify/docs/plugins/#google-analytics","title":"Google Analytics","text":"<p>Google's Universal Analytics service will no longer process new data in standard properties beginning July 1, 2023. Prepare now by setting up and switching over to a Google Analytics 4 property and docsify's gtag.js plugin.</p> <p>Install the plugin and configure the track id.</p> <pre><code>&lt;script&gt;\n  window.$docsify = {\n    ga: 'UA-XXXXX-Y',\n  };\n&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/ga.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>Configure by <code>data-ga</code>.</p> <pre><code>&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\" data-ga=\"UA-XXXXX-Y\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/ga.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/plugins/#google-analytics-4-ga4","title":"Google Analytics 4 (GA4)","text":"<p>Install the plugin and configure the track id.</p> <pre><code>&lt;script&gt;\n  // Single ID\n  window.$docsify = {\n    gtag: 'UA-XXXXX-Y',\n  };\n\n  // Multiple IDs\n  window.$docsify = {\n    gtag: [\n      'G-XXXXXXXX', // Google Analytics 4 (GA4)\n      'UA-XXXXXXXX', // Google Universal Analytics (GA3)\n      'AW-XXXXXXXX', // Google Ads\n      'DC-XXXXXXXX', // Floodlight\n    ],\n  };\n&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/gtag.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/plugins/#emoji","title":"Emoji","text":"<p>Renders a larger collection of emoji shorthand codes. Without this plugin, Docsify is able to render only a limited number of emoji shorthand codes.</p> <p>!&gt; Deprecated as of v4.13. Docsify no longer requires this plugin for full emoji support.</p> <pre><code>&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/emoji.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/plugins/#external-script","title":"External Script","text":"<p>If the script on the page is an external one (imports a js file via <code>src</code> attribute), you'll need this plugin to make it work.</p> <pre><code>&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/external-script.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/plugins/#zoom-image","title":"Zoom image","text":"<p>Medium's image zoom. Based on medium-zoom.</p> <pre><code>&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/zoom-image.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>Exclude the special image</p> <pre><code>![](image.png ':no-zoom')\n</code></pre>"},{"location":"docsify_docs/docsify/docs/plugins/#edit-on-github","title":"Edit on github","text":"<p>Add <code>Edit on github</code> button on every pages. Provided by @njleonzhang, see this document</p>"},{"location":"docsify_docs/docsify/docs/plugins/#demo-code-with-instant-preview-and-jsfiddle-integration","title":"Demo code with instant preview and jsfiddle integration","text":"<p>With this plugin, sample code can be rendered on the page instantly, so that the readers can see the preview immediately. When readers expand the demo box, the source code and description are shown there. if they click the button <code>Try in Jsfiddle</code>, <code>jsfiddle.net</code> will be open with the code of this sample, which allow readers to revise the code and try on their own.</p> <p>Vue and React are both supported.</p>"},{"location":"docsify_docs/docsify/docs/plugins/#copy-to-clipboard","title":"Copy to Clipboard","text":"<p>Add a simple <code>Click to copy</code> button to all preformatted code blocks to effortlessly allow users to copy example code from your docs. Provided by @jperasmus</p> <pre><code>&lt;script src=\"//cdn.jsdelivr.net/npm/docsify-copy-code/dist/docsify-copy-code.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See here for more details.</p>"},{"location":"docsify_docs/docsify/docs/plugins/#disqus","title":"Disqus","text":"<p>Disqus comments. https://disqus.com/</p> <pre><code>&lt;script&gt;\n  window.$docsify = {\n    disqus: 'shortname',\n  };\n&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/disqus.min.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/plugins/#gitalk","title":"Gitalk","text":"<p>Gitalk is a modern comment component based on Github Issue and Preact.</p> <pre><code>&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/gitalk/dist/gitalk.css\" /&gt;\n\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/gitalk.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n  const gitalk = new Gitalk({\n    clientID: 'Github Application Client ID',\n    clientSecret: 'Github Application Client Secret',\n    repo: 'Github repo',\n    owner: 'Github repo owner',\n    admin: [\n      'Github repo collaborators, only these guys can initialize github issues',\n    ],\n    // facebook-like distraction free mode\n    distractionFreeMode: false,\n  });\n&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/plugins/#pagination","title":"Pagination","text":"<p>Pagination for docsify. By @imyelo</p> <pre><code>&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/docsify.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify-pagination/dist/docsify-pagination.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>Click here to get more information.</p>"},{"location":"docsify_docs/docsify/docs/plugins/#tabs","title":"Tabs","text":"<p>A docsify.js plugin for displaying tabbed content from markdown.</p> <ul> <li>Documentation &amp; Demos</li> </ul> <p>Provided by @jhildenbiddle.</p>"},{"location":"docsify_docs/docsify/docs/plugins/#more-plugins","title":"More plugins","text":"<p>See awesome-docsify</p>"},{"location":"docsify_docs/docsify/docs/pwa/","title":"Offline Mode","text":"<p>Progressive Web Apps (PWA) are experiences that combine the best of the web with the best of apps. We can enhance our website with service workers to work offline or on low-quality networks.</p> <p>It is also very easy to use.</p>"},{"location":"docsify_docs/docsify/docs/pwa/#create-serviceworker","title":"Create serviceWorker","text":"<p>Create a <code>sw.js</code> file in your project's root directory and copy the following code:</p> <p>sw.js</p> <pre><code>/* ===========================================================\n * docsify sw.js\n * ===========================================================\n * Copyright 2016 @huxpro\n * Licensed under Apache 2.0\n * Register service worker.\n * ========================================================== */\n\nconst RUNTIME = 'docsify';\nconst HOSTNAME_WHITELIST = [\n  self.location.hostname,\n  'fonts.gstatic.com',\n  'fonts.googleapis.com',\n  'cdn.jsdelivr.net',\n];\n\n// The Util Function to hack URLs of intercepted requests\nconst getFixedUrl = req =&gt; {\n  const now = Date.now();\n  const url = new URL(req.url);\n\n  // 1. fixed http URL\n  // Just keep syncing with location.protocol\n  // fetch(httpURL) belongs to active mixed content.\n  // And fetch(httpRequest) is not supported yet.\n  url.protocol = self.location.protocol;\n\n  // 2. add query for caching-busting.\n  // Github Pages served with Cache-Control: max-age=600\n  // max-age on mutable content is error-prone, with SW life of bugs can even extend.\n  // Until cache mode of Fetch API landed, we have to workaround cache-busting with query string.\n  // Cache-Control-Bug: https://bugs.chromium.org/p/chromium/issues/detail?id=453190\n  if (url.hostname === self.location.hostname) {\n    url.search += (url.search ? '&amp;' : '?') + 'cache-bust=' + now;\n  }\n  return url.href;\n};\n\n/**\n *  @Lifecycle Activate\n *  New one activated when old isnt being used.\n *\n *  waitUntil(): activating ====&gt; activated\n */\nself.addEventListener('activate', event =&gt; {\n  event.waitUntil(self.clients.claim());\n});\n\n/**\n *  @Functional Fetch\n *  All network requests are being intercepted here.\n *\n *  void respondWith(Promise&lt;Response&gt; r)\n */\nself.addEventListener('fetch', event =&gt; {\n  // Skip some of cross-origin requests, like those for Google Analytics.\n  if (HOSTNAME_WHITELIST.indexOf(new URL(event.request.url).hostname) &gt; -1) {\n    // Stale-while-revalidate\n    // similar to HTTP's stale-while-revalidate: https://www.mnot.net/blog/2007/12/12/stale\n    // Upgrade from Jake's to Surma's: https://gist.github.com/surma/eb441223daaedf880801ad80006389f1\n    const cached = caches.match(event.request);\n    const fixedUrl = getFixedUrl(event.request);\n    const fetched = fetch(fixedUrl, { cache: 'no-store' });\n    const fetchedCopy = fetched.then(resp =&gt; resp.clone());\n\n    // Call respondWith() with whatever we get first.\n    // If the fetch fails (e.g disconnected), wait for the cache.\n    // If there\u2019s nothing in cache, wait for the fetch.\n    // If neither yields a response, return offline pages.\n    event.respondWith(\n      Promise.race([fetched.catch(_ =&gt; cached), cached])\n        .then(resp =&gt; resp || fetched)\n        .catch(_ =&gt; {\n          /* eat any errors */\n        }),\n    );\n\n    // Update the cache with the version we fetched (only for ok status)\n    event.waitUntil(\n      Promise.all([fetchedCopy, caches.open(RUNTIME)])\n        .then(\n          ([response, cache]) =&gt;\n            response.ok &amp;&amp; cache.put(event.request, response),\n        )\n        .catch(_ =&gt; {\n          /* eat any errors */\n        }),\n    );\n  }\n});\n</code></pre>"},{"location":"docsify_docs/docsify/docs/pwa/#register","title":"Register","text":"<p>Now, register it in your <code>index.html</code>. It only works on some modern browsers, so we need to check:</p> <p>index.html</p> <pre><code>&lt;script&gt;\n  if (typeof navigator.serviceWorker !== 'undefined') {\n    navigator.serviceWorker.register('sw.js');\n  }\n&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/pwa/#enjoy-it","title":"Enjoy it","text":"<p>Release your website and start experiencing the magical offline feature. :ghost: You can turn off Wi-Fi and refresh the current site to experience it.</p>"},{"location":"docsify_docs/docsify/docs/quickstart/","title":"Quick start","text":"<p>It is recommended to install <code>docsify-cli</code> globally, which helps initializing and previewing the website locally.</p> <pre><code>npm i docsify-cli -g\n</code></pre>"},{"location":"docsify_docs/docsify/docs/quickstart/#initialize","title":"Initialize","text":"<p>If you want to write the documentation in the <code>./docs</code> subdirectory, you can use the <code>init</code> command.</p> <pre><code>docsify init ./docs\n</code></pre>"},{"location":"docsify_docs/docsify/docs/quickstart/#writing-content","title":"Writing content","text":"<p>After the <code>init</code> is complete, you can see the file list in the <code>./docs</code> subdirectory.</p> <ul> <li><code>index.html</code> as the entry file</li> <li><code>README.md</code> as the home page</li> <li><code>.nojekyll</code> prevents GitHub Pages from ignoring files that begin with an underscore</li> </ul> <p>You can easily update the documentation in <code>./docs/README.md</code>, of course you can add more pages.</p>"},{"location":"docsify_docs/docsify/docs/quickstart/#preview-your-site","title":"Preview your site","text":"<p>Run the local server with <code>docsify serve</code>. You can preview your site in your browser on <code>http://localhost:3000</code>.</p> <pre><code>docsify serve docs\n</code></pre> <p>?&gt; For more use cases of <code>docsify-cli</code>, head over to the docsify-cli documentation.</p>"},{"location":"docsify_docs/docsify/docs/quickstart/#manual-initialization","title":"Manual initialization","text":"<p>Download or create an <code>index.html</code> template using the following markup:</p> Download Template <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, viewport-fit=cover\"&gt;\n\n    &lt;!-- Core Theme --&gt;\n    &lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5/themes/core.min.css\"&gt;\n  &lt;/head&gt;\n  &lt;body class=\"loading\"&gt;\n    &lt;div id=\"app\"&gt;&lt;/div&gt;\n\n    &lt;!-- Configuration --&gt;\n    &lt;script&gt;\n      window.$docsify = {\n        //...\n      };\n    &lt;/script&gt;\n\n    &lt;!-- Docsify.js --&gt;\n    &lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5\"&gt;&lt;/script&gt;\n\n    &lt;!-- Plugins (optional) --&gt;\n    &lt;!-- &lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5/dist/plugins/search.min.js\"&gt;&lt;/script&gt; --&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/quickstart/#specifying-docsify-versions","title":"Specifying docsify versions","text":"<p>?&gt; Note that in both of the examples below, docsify URLs will need to be manually updated when a new major version of docsify is released (e.g. <code>v5.x.x</code> =&gt; <code>v6.x.x</code>). Check the docsify website periodically to see if a new major version has been released.</p> <p>Specifying a major version in the URL (<code>@5</code>) will allow your site to receive non-breaking enhancements (i.e. \"minor\" updates) and bug fixes (i.e. \"patch\" updates) automatically. This is the recommended way to load docsify resources.</p> <pre><code>&lt;!-- Core Theme --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5/themes/core.min.css\"&gt;\n\n&lt;!-- Docsify --&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5\"&gt;&lt;/script&gt;\n</code></pre> <p>If you prefer to lock docsify to a specific version, specify the full version after the <code>@</code> symbol in the URL. This is the safest way to ensure your site will look and behave the same way regardless of any changes made to future versions of docsify.</p> <pre><code>&lt;!-- Core Theme --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5.0.0/themes/core.min.css\"&gt;\n\n&lt;!-- Docsify --&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/docsify@5.0.0\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/quickstart/#manually-preview-your-site","title":"Manually preview your site","text":"<p>If you have Python installed on your system, you can easily use it to run a static server to preview your site.</p> <pre><code># Python 2\ncd docs &amp;&amp; python -m SimpleHTTPServer 3000\n</code></pre> <pre><code># Python 3\ncd docs &amp;&amp; python -m http.server 3000\n</code></pre>"},{"location":"docsify_docs/docsify/docs/quickstart/#loading-dialog","title":"Loading dialog","text":"<p>If you want, you can show a loading dialog before docsify starts to render your documentation:</p> <pre><code>&lt;!-- index.html --&gt;\n\n&lt;div id=\"app\"&gt;Please wait...&lt;/div&gt;\n</code></pre> <p>You should set the <code>data-app</code> attribute if you changed <code>el</code>:</p> <pre><code>&lt;!-- index.html --&gt;\n\n&lt;div data-app id=\"main\"&gt;Please wait...&lt;/div&gt;\n\n&lt;script&gt;\n  window.$docsify = {\n    el: '#main',\n  };\n&lt;/script&gt;\n</code></pre> <p>Compare el configuration.</p>"},{"location":"docsify_docs/docsify/docs/themes/","title":"Themes","text":""},{"location":"docsify_docs/docsify/docs/themes/#core-theme","title":"Core theme","text":"<p>The Docsify \"core\" theme contains all of the styles and theme properties needed to render a Docsify site. This theme is designed to serve as a minimalist theme on its own, in combination with theme add-ons, modified using core classes, and as a starting point for customization.</p> <pre><code>&lt;!-- Core Theme --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5/dist/themes/core.min.css\" /&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#theme-add-ons","title":"Theme add-ons","text":"<p>Theme add-ons are used in combination with the core theme. Add-ons contain CSS rules that modify theme properties values and/or add custom style declarations. They can often (but not always) be used with other add-ons.</p> <p>!&gt; Theme add-ons must be loaded after the core theme.</p> <pre><code>&lt;!-- Core Theme --&gt;\n&lt;link rel=\"stylesheet\" href=\"...\" /&gt;\n\n&lt;!-- Theme (add-on) --&gt;\n&lt;link rel=\"stylesheet\" href=\"...\" /&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#core-dark-add-on","title":"Core Dark (Add-on)","text":"<p>Dark mode styles for the core theme. Styles can applied only when an operating system's dark mode is active by specifying a <code>media</code> attribute.</p> <p>   &lt;input     class=\"toggle\"     type=\"checkbox\"     value=\"core-dark\"     data-group=\"addon\"     data-sheet <p>Preview Core Dark     &lt;input     class=\"toggle\"     type=\"checkbox\"     value=\"core-dark-auto\"     data-group=\"addon\"     data-sheet <p>Preview Core Dark (Dark Mode Only) </p> <pre><code>&lt;!-- Core Dark (add-on) --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5/dist/themes/addons/core-dark.min.css\" /&gt;\n</code></pre> <pre><code>&lt;!-- Core Dark - Dark Mode Only (add-on) --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5/dist/themes/addons/core-dark.min.css\" media=\"(prefers-color-scheme: dark)\" /&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#vue-theme-add-on","title":"Vue theme (Add-on)","text":"<p>The popular Docsify v4 theme.</p> <p>   &lt;input    class=\"toggle\"    type=\"checkbox\"    value=\"vue\"    data-group=\"addon\"    data-sheet <p>Preview Vue </p> <pre><code>&lt;!-- Vue Theme (add-on) --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5/dist/themes/addons/vue.min.css\" /&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#classes","title":"Classes","text":"<p>The core theme provides several CSS classes for customizing your Docsify site. These classes should be applied to the <code>&lt;body&gt;</code> element within your <code>index.html</code> page.</p> <pre><code>&lt;body class=\"...\"&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#loading","title":"Loading","text":"<p>Display a loading animation while waiting for Docsify to initialize.</p> <pre><code>&lt;body class=\"loading\"&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#sidebar-chevrons","title":"Sidebar chevrons","text":"<p>Display expand/collapse icons on page links in the sidebar.</p> <p>  Preview <code>sidebar-chevron-right</code>  Preview <code>sidebar-chevron-left</code> </p> <pre><code>&lt;body class=\"sidebar-chevron-right\"&gt;\n</code></pre> <pre><code>&lt;body class=\"sidebar-chevron-left\"&gt;\n</code></pre> <p>To prevent chevrons from displaying for specific page links, add a <code>no-chevron</code> class as follows:</p> <pre><code>[My Page](page.md ':class=no-chevron')\n</code></pre> <p>Theme properties</p> <pre><code>:root {\n  --sidebar-chevron-collapsed-color: var(--color-mono-3);\n  --sidebar-chevron-expanded-color : var(--theme-color);\n}\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#sidebar-groups","title":"Sidebar groups","text":"<p>Add visual distinction between groups of links in the sidebar.</p> <p>  Preview <code>sidebar-group-box</code>  Preview <code>sidebar-group-underline</code> </p> <pre><code>&lt;body class=\"sidebar-group-box\"&gt;\n</code></pre> <pre><code>&lt;body class=\"sidebar-group-underline\"&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#sidebar-link-clamp","title":"Sidebar link clamp","text":"<p>Limit multi-line sidebar links to a single line followed by an ellipses.</p> <p>    Preview <code>sidebar-link-clamp</code> </p> <pre><code>&lt;body class=\"sidebar-link-clamp\"&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#sidebar-toggle","title":"Sidebar toggle","text":"<p>Display a \"hamburger\" icon (three lines) in the sidebar toggle button instead of the default \"kebab\" icon.</p> <p>    Preview <code>sidebar-toggle-chevron</code>    Preview <code>sidebar-toggle-hamburger</code> </p> <pre><code>&lt;body class=\"sidebar-toggle-chevron\"&gt;\n</code></pre> <pre><code>&lt;body class=\"sidebar-toggle-hamburger\"&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#customization","title":"Customization","text":"<p>Docsify provides theme properties for simplified customization of frequently modified styles.</p> <ol> <li>Add a <code>&lt;style&gt;</code> tag after the theme stylesheet in your <code>index.html</code>.</li> </ol> <pre><code>&lt;!-- Theme --&gt;\n&lt;link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/docsify@5/dist/themes/core.min.css\" /&gt;\n\n&lt;!-- Custom theme styles --&gt;\n&lt;style&gt;\n  :root {\n    /* ... */\n  }\n&lt;/style&gt;\n</code></pre> <p>Theme properties can also be set on a per-page basis in markdown.</p> <pre><code># My Heading\n\nHello, World!\n\n&lt;style&gt;\n  :root {\n    /* ... */\n  }\n&lt;/style&gt;\n</code></pre> <ol> <li>Set custom theme properties within a <code>:root</code> declaration.</li> </ol> <pre><code>:root {\n  --theme-color: red;\n  --font-size  : 15px;\n  --line-height: 1.5;\n}\n</code></pre> <p>Custom theme properties can be conditionally applied in light and/or dark mode.</p> <pre><code>/* Light and dark mode */\n:root {\n  --theme-color: pink;\n}\n\n/* Light mode only */\n@media (prefers-color-scheme: light) {\n  :root {\n    --color-bg  : #eee;\n    --color-text: #444;\n  }\n}\n\n/* Dark mode only */\n@media screen and (prefers-color-scheme: dark) {\n  :root {\n    --color-bg  : #222;\n    --color-text: #ddd;\n  }\n}\n</code></pre> <ol> <li>Custom fonts can be used by adding web font resources and modifying <code>--font-family</code> properties as needed:</li> </ol> <pre><code>/* Fonts: Noto Sans, Noto Emoji, Noto Mono */\n@import url('https://fonts.googleapis.com/css2?family=Noto+Color+Emoji&amp;family=Noto+Sans+Mono:wght@100..900&amp;family=Noto+Sans:ital,wght@0,100..900;1,100..900&amp;display=swap');\n\n:root {\n  --font-family      : 'Noto Sans', sans-serif;\n  --font-family-emoji: 'Noto Color Emoji', sans-serif;\n  --font-family-mono : 'Noto Sans Mono', monospace;\n}\n</code></pre> <p>?&gt; Theme authors: Consider providing instructions for loading your recommended web fonts manually instead of including them in your theme using <code>@import</code>. This allows users who prefer a different font to avoid loading your recommended web font(s) unnecessarily.</p> <ol> <li>Advanced styling may require custom CSS declarations. This is expected, however custom CSS declarations may break when new versions of Docsify are released. When possible, leverage theme properties instead of custom declarations or lock your CDN URLs to a specific version to avoid potential issues when using custom CSS declarations.</li> </ol> <pre><code>.sidebar li.active &gt; a {\n  border-right: 3px solid var(--theme-color);\n}\n</code></pre>"},{"location":"docsify_docs/docsify/docs/themes/#theme-properties","title":"Theme properties","text":"<p>The following properties are available in all official Docsify themes. Default values for the \"Core\" theme are shown.</p> <p>?&gt; Theme and plugin authors: We encourage you to leverage these custom theme properties and to offer similar customization options in your projects.</p>"},{"location":"docsify_docs/docsify/docs/themes/#common","title":"Common","text":"<p>Below are the most commonly modified theme properties. Advanced theme properties are also available for use but typically do not need to be modified.</p> <p>TBD</p>"},{"location":"docsify_docs/docsify/docs/themes/#advanced","title":"Advanced","text":"<p>Advanced theme properties are also available for use but typically do not need to be modified. Values derived from common theme properties but can be set explicitly if preferred.</p> <p>TBD</p>"},{"location":"docsify_docs/docsify/docs/themes/#community","title":"Community","text":"<p>See Awesome Docsify for additional community themes.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/","title":"Ui kit","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#ui-kit","title":"UI Kit","text":"View the markdown source for this page   [ui-kit.md](ui-kit.md ':include :type=code')"},{"location":"docsify_docs/docsify/docs/ui-kit/#blockquotes","title":"Blockquotes","text":"<p>Cras aliquet nulla quis metus tincidunt, sed placerat enim cursus. Etiam turpis nisl, posuere eu condimentum ut, interdum a risus. Sed non luctus mi. Quisque malesuada risus sit amet tortor aliquet, a posuere ex iaculis. Vivamus ultrices enim dui, eleifend porttitor elit aliquet sed.</p> <p>- Quote Source</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#nested","title":"Nested","text":"<p>Level 1</p> <p>Level 2</p> <p>Level 3</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#buttons","title":"Buttons","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#default","title":"Default","text":"<p>Button</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#basic","title":"Basic","text":"<p>Button Link Button </p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#primary","title":"Primary","text":"<p>Button Link Button </p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#secondary","title":"Secondary","text":"<p>Button Link Button </p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#callouts","title":"Callouts","text":"<p>!&gt; Important callout with <code>inline code</code> and additional placeholder text used to force the content to wrap and span multiple lines.</p> <p>?&gt; Tip callout with <code>inline code</code> and additional placeholder text used to force the content to wrap and span multiple lines.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#code","title":"Code","text":"<p>This is <code>inline code</code></p> <pre><code>const add = (num1, num2) =&gt; num1 + num2;\nconst total = add(1, 2);\n\nconsole.log(total); // 3\n</code></pre> <pre><code>&lt;body&gt;\n  &lt;p&gt;Hello&lt;/p&gt;\n&lt;/body&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/ui-kit/#colors","title":"Colors","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#theme","title":"Theme","text":"1 2 3 4 Theme Color 5 6 7 8"},{"location":"docsify_docs/docsify/docs/ui-kit/#monochromatic","title":"Monochromatic","text":"Min 1 2 3 4 5 6 7 8 9 Max"},{"location":"docsify_docs/docsify/docs/ui-kit/#details","title":"Details","text":"Details (click to open)  Suscipit nemo aut ex suscipit voluptatem laboriosam odio velit. Ipsum eveniet labore sequi non optio vel. Ut culpa ad accusantium est aut harum ipsam voluptatum. Velit eum incidunt non sint. Et molestiae veniam natus autem vel assumenda ut numquam esse. Non nisi id qui vero corrupti quos et.   Details (open by default)  Suscipit nemo aut ex suscipit voluptatem laboriosam odio velit. Ipsum eveniet labore sequi non optio vel. Ut culpa ad accusantium est aut harum ipsam voluptatum. Velit eum incidunt non sint. Et molestiae veniam natus autem vel assumenda ut numquam esse. Non nisi id qui vero corrupti quos et."},{"location":"docsify_docs/docsify/docs/ui-kit/#form-elements","title":"Form Elements","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#fieldset","title":"Fieldset","text":"Legend <p>          Label </p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#input","title":"Input","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#checkbox","title":"Checkbox","text":"HTML  CSS  JavaScript"},{"location":"docsify_docs/docsify/docs/ui-kit/#datalist","title":"Datalist","text":"Label Earth Jupiter Mars Mercury Neptune Saturn Uranus Venus"},{"location":"docsify_docs/docsify/docs/ui-kit/#radio","title":"Radio","text":"HTML  CSS  JavaScript"},{"location":"docsify_docs/docsify/docs/ui-kit/#text","title":"Text","text":"First name"},{"location":"docsify_docs/docsify/docs/ui-kit/#toggles","title":"Toggles","text":"Checkbox (multi-select)   HTML  CSS  JavaScript  Radio (single-select)   HTML  CSS  JavaScript"},{"location":"docsify_docs/docsify/docs/ui-kit/#select","title":"Select","text":"Label Select a planet... Earth Jupiter Mars Mercury Neptune Saturn Uranus Venus"},{"location":"docsify_docs/docsify/docs/ui-kit/#textarea","title":"Textarea","text":"Ipsam totam tempora. Dolorum voluptas error tempore asperiores vitae error laboriosam autem possimus."},{"location":"docsify_docs/docsify/docs/ui-kit/#headings","title":"Headings","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#heading-1-docsify-ignore","title":"Heading 1 {docsify-ignore}","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse luctus nulla eu ex varius, a varius elit tincidunt. Aenean arcu magna, gravida id purus a, interdum convallis turpis. Aenean id ipsum eu tortor sollicitudin scelerisque in quis elit.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#heading-2-docsify-ignore","title":"Heading 2 {docsify-ignore}","text":"<p>Vestibulum lobortis laoreet nunc vel vulputate. In et augue non lectus pellentesque molestie et ac justo. Sed sed turpis ut diam gravida sagittis nec at neque. Vivamus id tellus est. Nam ac dignissim mi. Vestibulum nec sem convallis, condimentum augue at, commodo diam.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#heading-3-docsify-ignore","title":"Heading 3 {docsify-ignore}","text":"<p>Suspendisse sit amet tincidunt nibh, ac interdum velit. Ut orci diam, dignissim at enim sit amet, placerat rutrum magna. Mauris consectetur nibh eget sem feugiat, sit amet congue quam laoreet. Curabitur sed massa metus.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#heading-4-docsify-ignore","title":"Heading 4 {docsify-ignore}","text":"<p>Donec odio orci, facilisis ac vehicula in, vestibulum ut urna. Ut bibendum ullamcorper risus, ac euismod leo maximus sed. In pulvinar sagittis rutrum. Morbi quis cursus diam. Cras ac laoreet nulla, rhoncus sodales dui.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#heading-5-docsify-ignore","title":"Heading 5 {docsify-ignore}","text":"<p>Commodo sit veniam nulla cillum labore ullamco aliquip quis. Consequat nulla fugiat consequat ex duis proident. Adipisicing excepteur tempor exercitation ad. Consectetur voluptate Lorem sint elit exercitation ullamco dolor.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#heading-6-docsify-ignore","title":"Heading 6 {docsify-ignore}","text":"<p>Ipsum ea amet dolore mollit incididunt fugiat nulla laboris est sint voluptate. Ex culpa id amet ipsum amet pariatur ipsum officia sit laborum irure ullamco deserunt. Consequat qui tempor occaecat nostrud proident.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#horizontal-rule","title":"Horizontal Rule","text":"<p>Text before rule.</p> <p>Text after rule.</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#iframe","title":"IFrame","text":"<p>Example</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#images","title":"Images","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#inline-style","title":"Inline-style","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#reference-style","title":"Reference-style","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#light-dark-theme","title":"Light / Dark Theme","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#keyboard","title":"Keyboard","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#default_1","title":"Default","text":"<p>\u2303\u2325\u232b</p> <p>CtrlAltDel</p> <p>\u2303 Control\u2325 Alt\u232b Delete</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#alternate","title":"Alternate","text":"<p>\u2303\u2325\u232b</p> <p>CtrlAltDel</p> <p>\u2303 Control\u2325 Alt\u232b Delete</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#entities","title":"Entities","text":"\u2191 \u2191 Arrow Up \u2193 \u2193 Arrow Down \u2190 \u2190 Arrow Left \u2192 \u2192 Arrow Right \u21ea \u21ea Caps Lock \u2318 \u2318 Command \u2303 \u2303 Control \u232b \u232b Delete \u2326 \u2326 Delete (Forward) \u2198 \u2198 End \u2324 \u2324 Enter \u238b \u238b Escape \u2196 \u2196 Home \u21de \u21de Page Up \u21df \u21df Page Down \u2325 \u2325 Option, Alt \u21b5 \u21b5 Return \u21e7 \u21e7 Shift \u2423 \u2423 Space \u21e5 \u21e5 Tab \u21e4 \u21e4 Tab + Shift"},{"location":"docsify_docs/docsify/docs/ui-kit/#links","title":"Links","text":"<p>Inline link</p> <p>Inline link with title</p> <p>Reference link by name</p> <p>Reference link by number</p> <p>Reference link by self</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#lists","title":"Lists","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#ordered-list","title":"Ordered List","text":"<ol> <li>Ordered</li> <li>Ordered</li> <li>Nested</li> <li>Nested (Wrapping): Similique tempora et. Voluptatem consequuntur ut. Rerum minus et sed beatae. Consequatur ut nemo laboriosam quo architecto quia qui. Corrupti aut omnis velit.</li> <li>Ordered (Wrapping): Error minima modi rem sequi facere voluptatem. Est nihil veritatis doloribus et corporis ipsam. Pariatur eos ipsam qui odit labore est voluptatem enim. Veritatis est qui ut pariatur inventore.</li> </ol>"},{"location":"docsify_docs/docsify/docs/ui-kit/#unordered-list","title":"Unordered List","text":"<ul> <li>Unordered</li> <li>Unordered</li> <li>Nested</li> <li>Nested (Wrapping): Quia consectetur sint vel ut excepturi ipsa voluptatum suscipit hic. Ipsa error qui molestiae harum laboriosam. Rerum non amet illo voluptatem odio pariatur. Ut minus enim.</li> <li>Unordered (Wrapping): Fugiat qui tempore ratione amet repellendus repudiandae non. Rerum nisi officia enim. Itaque est alias voluptatibus id molestiae accusantium. Cupiditate sequi qui omnis sed facere aliquid quia ut.</li> </ul>"},{"location":"docsify_docs/docsify/docs/ui-kit/#task-list","title":"Task List","text":"<ul> <li>[x] Task</li> <li>[ ] Task</li> <li>[ ] Subtask</li> <li>[ ] Subtask</li> <li>[x] Subtask</li> <li>[ ] Task (Wrapping): Earum consequuntur itaque numquam sunt error omnis ipsum repudiandae. Est assumenda neque eum quia quisquam laborum beatae autem ad. Fuga fugiat perspiciatis harum quia dignissimos molestiae. Officia quo eveniet tempore modi voluptates consequatur. Eum odio adipisci labore.</li> <li>[x] Subtask (Wrapping): Vel possimus eaque laborum. Voluptates qui debitis quaerat atque molestiae quia explicabo doloremque. Reprehenderit perspiciatis a aut impedit temporibus aut quasi quia. Incidunt sed recusandae vitae asperiores sit in.</li> </ul>"},{"location":"docsify_docs/docsify/docs/ui-kit/#output","title":"Output","text":"<p>Et cum fugiat nesciunt voluptates. A atque quos doloribus dolorem quo. Et dignissimos omnis nam. Recusandae voluptatem nam. Tenetur veniam et qui consequatur. Aut sequi atque fuga itaque iusto eum nihil quod iure.</p> <ol> <li>Item</li> <li>Item</li> <li>Item</li> </ol>"},{"location":"docsify_docs/docsify/docs/ui-kit/#tables","title":"Tables","text":""},{"location":"docsify_docs/docsify/docs/ui-kit/#alignment","title":"Alignment","text":"Left Align Center Align Right Align Non\u2011Breaking\u00a0Header A1 A2 A3 A4 B1 B2 B3 B4 C1 C2 C3 C4"},{"location":"docsify_docs/docsify/docs/ui-kit/#headerless","title":"Headerless","text":"A1 A2 A3 A4 B1 B2 B3 B4 C1 C2 C3 C4"},{"location":"docsify_docs/docsify/docs/ui-kit/#scrolling","title":"Scrolling","text":"Header Dicta\u00a0in\u00a0nobis\u00a0dolor\u00a0adipisci\u00a0qui.\u00a0Accusantium\u00a0voluptates\u00a0est\u00a0dolor\u00a0laboriosam\u00a0qui\u00a0voluptatibus.\u00a0Veritatis\u00a0eos\u00a0aspernatur\u00a0iusto\u00a0et\u00a0dicta\u00a0quas.\u00a0Fugit\u00a0voluptatem\u00a0dolorum\u00a0qui\u00a0quisquam.\u00a0nihil Aut\u00a0praesentium\u00a0officia\u00a0aut\u00a0delectus.\u00a0Quas\u00a0atque\u00a0reprehenderit\u00a0saepe.\u00a0Et\u00a0voluptatibus\u00a0qui\u00a0dolores\u00a0rem\u00a0facere\u00a0in\u00a0dignissimos\u00a0id\u00a0aut.\u00a0Debitis\u00a0excepturi\u00a0delectus\u00a0et\u00a0quos\u00a0numquam\u00a0magnam. Sed\u00a0eum\u00a0atque\u00a0at\u00a0laborum\u00a0aut\u00a0et\u00a0repellendus\u00a0ullam\u00a0dolor.\u00a0Cupiditate\u00a0saepe\u00a0voluptatibus\u00a0odit\u00a0est\u00a0pariatur\u00a0qui.\u00a0Hic\u00a0sunt\u00a0nihil\u00a0optio\u00a0enim\u00a0eum\u00a0laudantium.\u00a0Repellendus\u00a0voluptate."},{"location":"docsify_docs/docsify/docs/ui-kit/#text-elements","title":"Text Elements","text":"<p>Marked text</p> <pre>Preformatted text</pre> <p>Sample Output</p> <p>Small Text</p> <p>This is <sub>subscript</sub></p> <p>This is <sup>superscript</sup></p> <p>Underlined Text</p>"},{"location":"docsify_docs/docsify/docs/ui-kit/#text-styles","title":"Text Styles","text":"<p>Body text</p> <p>Bold text</p> <p>Italic text</p> <p>Bold and italic text</p> <p>~~Strikethrough~~</p>"},{"location":"docsify_docs/docsify/docs/vue/","title":"Vue compatibility","text":"<p>Docsify allows Vue.js content to be added directly to your markdown pages. This can greatly simplify working with data and adding reactivity to your site.</p> <p>Vue template syntax can be used to add dynamic content to your pages. Vue content becomes more interesting when data, computed properties, methods, and lifecycle hooks are used. These options can be specified as global options or within DOM mounts and components.</p>"},{"location":"docsify_docs/docsify/docs/vue/#setup","title":"Setup","text":"<p>To get started, add Vue.js to your <code>index.html</code> file. Choose the production version for your live site or the development version for helpful console warnings and Vue.js devtools support.</p> <pre><code>&lt;!-- Production --&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/vue@3/dist/vue.global.prod.js\"&gt;&lt;/script&gt;\n\n&lt;!-- Development --&gt;\n&lt;script src=\"//cdn.jsdelivr.net/npm/vue@3/dist/vue.global.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/vue/#template-syntax","title":"Template syntax","text":"<p>Vue template syntax offers several useful features like support for JavaScript expressions and Vue directives for loops and conditional rendering.</p> <pre><code>&lt;!-- Hide in docsify, show elsewhere (e.g. GitHub) --&gt;\n&lt;p v-if=\"false\"&gt;Text for GitHub&lt;/p&gt;\n\n&lt;!-- Sequenced content (i.e. loop)--&gt;\n&lt;ul&gt;\n  &lt;li v-for=\"i in 3\"&gt;Item {{ i }}&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;!-- JavaScript expressions --&gt;\n&lt;p&gt;2 + 2 = {{ 2 + 2 }}&lt;/p&gt;\n</code></pre> <p>Text for GitHub</p> <ul> <li>Item {{ i }}</li> </ul> <p>2 + 2 = {{ 2 + 2 }}</p> <p>View output on GitHub</p>"},{"location":"docsify_docs/docsify/docs/vue/#code-blocks","title":"Code Blocks","text":"<p>Docsify ignores Vue template syntax within code blocks by default:</p> <pre><code>```\n{{ message}}\n```\n</code></pre> <p>To process Vue template syntax within a code block, wrap the code block in an element with a <code>v-template</code> attribute:</p> <pre><code>&lt;div v-template&gt;\n\n```\n{{ message}}\n```\n\n&lt;/div&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/vue/#data","title":"Data","text":"<pre><code>{\n  data() {\n    return {\n      message: 'Hello, World!'\n    };\n  }\n}\n</code></pre> <pre><code>&lt;!-- Show message in docsify, show \"{{ message }}\" elsewhere (e.g. GitHub)  --&gt;\n{{ message }}\n\n&lt;!-- Show message in docsify, hide elsewhere (e.g. GitHub)  --&gt;\n&lt;p v-text=\"message\"&gt;&lt;/p&gt;\n</code></pre> <p>{{ message }}</p> <p></p> <p>View output on GitHub</p>"},{"location":"docsify_docs/docsify/docs/vue/#computed-properties","title":"Computed properties","text":"<pre><code>{\n  computed: {\n    timeOfDay() {\n      const date = new Date();\n      const hours = date.getHours();\n\n      if (hours &lt; 12) {\n        return 'morning';\n      }\n      else if (hours &lt; 18) {\n        return 'afternoon';\n      }\n      else {\n        return 'evening'\n      }\n    }\n  },\n}\n</code></pre> <pre><code>Good {{ timeOfDay }}!\n</code></pre>   Good {{ timeOfDay }}!"},{"location":"docsify_docs/docsify/docs/vue/#methods","title":"Methods","text":"<pre><code>{\n  data() {\n    return {\n      message: 'Hello, World!'\n    };\n  },\n  methods: {\n    hello() {\n      alert(this.message);\n    }\n  },\n}\n</code></pre> <pre><code>&lt;button @click=\"hello\"&gt;Say Hello&lt;/button&gt;\n</code></pre> <p>Say Hello</p>"},{"location":"docsify_docs/docsify/docs/vue/#lifecycle-hooks","title":"Lifecycle Hooks","text":"<pre><code>{\n  data() {\n    return {\n      images: null,\n    };\n  },\n  created() {\n    fetch('https://api.domain.com/')\n      .then(response =&gt; response.json())\n      .then(data =&gt; (this.images = data))\n      .catch(err =&gt; console.log(err));\n  }\n}\n\n// API response:\n// [\n//   { title: 'Image 1', url: 'https://domain.com/1.jpg' },\n//   { title: 'Image 2', url: 'https://domain.com/2.jpg' },\n//   { title: 'Image 3', url: 'https://domain.com/3.jpg' },\n// ];\n</code></pre> <pre><code>&lt;div style=\"display: flex;\"&gt;\n  &lt;figure style=\"flex: 1;\"&gt;\n    &lt;img v-for=\"image in images\" :src=\"image.url\" :title=\"image.title\"&gt;\n    &lt;figcaption&gt;{{ image.title }}&lt;/figcaption&gt;\n  &lt;/figure&gt;\n&lt;/div&gt;\n</code></pre> {{ image.title }}"},{"location":"docsify_docs/docsify/docs/vue/#global-options","title":"Global options","text":"<p>Use <code>vueGlobalOptions</code> to specify global Vue options for use with Vue content not explicitly mounted with vueMounts, vueComponents, or a markdown script. Changes to global <code>data</code> will persist and be reflected anywhere global references are used.</p> <pre><code>window.$docsify = {\n  vueGlobalOptions: {\n    data() {\n      return {\n        count: 0,\n      };\n    },\n  },\n};\n</code></pre> <pre><code>&lt;p&gt;\n  &lt;button @click=\"count += 1\"&gt;+&lt;/button&gt;\n  {{ count }}\n  &lt;button @click=\"count -= 1\"&gt;-&lt;/button&gt;\n&lt;/p&gt;\n</code></pre> <p> +     {{ count }}     - </p> <p>Notice the behavior when multiple global counters are rendered:</p> <p> +     {{ count }}     - </p> <p>Changes made to one counter affect the both counters. This is because both instances reference the same global <code>count</code> value. Now, navigate to a new page and return to this section to see how changes made to global data persist between page loads.</p>"},{"location":"docsify_docs/docsify/docs/vue/#mounts","title":"Mounts","text":"<p>Use <code>vueMounts</code> to specify DOM elements to mount as Vue instances and their associated options. Mount elements are specified using a CSS selector as the key with an object containing Vue options as their value. Docsify will mount the first matching element in the main content area each time a new page is loaded. Mount element <code>data</code> is unique for each instance and will not persist as users navigate the site.</p> <pre><code>window.$docsify = {\n  vueMounts: {\n    '#counter': {\n      data() {\n        return {\n          count: 0,\n        };\n      },\n    },\n  },\n};\n</code></pre> <pre><code>&lt;div id=\"counter\"&gt;\n  &lt;button @click=\"count += 1\"&gt;+&lt;/button&gt;\n  {{ count }}\n  &lt;button @click=\"count -= 1\"&gt;-&lt;/button&gt;\n&lt;/div&gt;\n</code></pre> +   {{ count }}   -"},{"location":"docsify_docs/docsify/docs/vue/#components","title":"Components","text":"<p>Use <code>vueComponents</code> to create and register global Vue components. Components are specified using the component name as the key with an object containing Vue options as the value. Component <code>data</code> is unique for each instance and will not persist as users navigate the site.</p> <pre><code>window.$docsify = {\n  vueComponents: {\n    'button-counter': {\n      template: `\n        &lt;button @click=\"count += 1\"&gt;\n          You clicked me {{ count }} times\n        &lt;/button&gt;\n      `,\n      data() {\n        return {\n          count: 0,\n        };\n      },\n    },\n  },\n};\n</code></pre> <pre><code>&lt;button-counter&gt;&lt;/button-counter&gt;\n&lt;button-counter&gt;&lt;/button-counter&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/vue/#markdown-script","title":"Markdown script","text":"<p>Vue content can mounted using a <code>&lt;script&gt;</code> tag in your markdown pages.</p> <p>!&gt; Only the first <code>&lt;script&gt;</code> tag in a markdown file is executed. If you wish to mount multiple Vue instances using a script tag, all instances must be mounted within the first script tag in your markdown.</p> <pre><code>&lt;script&gt;\n  Vue.createApp({\n    // Options...\n  }).mount('#example');\n&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/vue/#technical-notes","title":"Technical Notes","text":"<ul> <li>Docsify processes Vue content in the following order on each page load:</li> <li>Execute markdown <code>&lt;script&gt;</code></li> <li>Register global <code>vueComponents</code></li> <li>Mount <code>vueMounts</code></li> <li>Auto-mount unmounted <code>vueComponents</code></li> <li>Auto-mount unmounted Vue template syntax using <code>vueGlobalOptions</code></li> <li>When auto-mounting Vue content, docsify will mount each top-level element in your markdown that contains template syntax or a component. For example, in the following HTML the top-level <code>&lt;p&gt;</code>, <code>&lt;my-component /&gt;</code>, and <code>&lt;div&gt;</code> elements will be mounted.   <pre><code>&lt;p&gt;{{ foo }}&lt;/p&gt;\n&lt;my-component /&gt;\n&lt;div&gt;\n  &lt;span&gt;{{ bar }}&lt;/span&gt;\n  &lt;some-other-component /&gt;\n&lt;/div&gt;\n</code></pre></li> <li>Docsify will not mount an existing Vue instance or an element that contains an existing Vue instance.</li> <li>Docsify will automatically destroy/unmount all Vue instances it creates before each page load.</li> </ul>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/","title":"Write a plugin","text":"<p>A docsify plugin is a function with the ability to execute custom JavaScript code at various stages of Docsify's lifecycle.</p>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#setup","title":"Setup","text":"<p>Docsify plugins can be added directly to the <code>plugins</code> array:</p> <pre><code>window.$docsify = {\n  plugins: [\n    function myPlugin1(hook, vm) {\n      // ...\n    },\n    function myPlugin2(hook, vm) {\n      // ...\n    },\n  ],\n};\n</code></pre> <p>Alternatively, a plugin can be stored in a separate file and \"installed\" using a standard <code>&lt;script&gt;</code> tag:</p> <pre><code>// docsify-plugin-myplugin.js\n\n{\n  function myPlugin(hook, vm) {\n    // ...\n  }\n\n  // Add plugin to docsify's plugin array\n  window.$docsify = window.$docsify || {};\n  $docsify.plugins = [...($docsify.plugins || []), myPlugin];\n}\n</code></pre> <pre><code>&lt;script src=\"docsify-plugin-myplugin.js\"&gt;&lt;/script&gt;\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#template","title":"Template","text":"<p>Below is a plugin template with placeholders for all available lifecycle hooks.</p> <ol> <li>Copy the template</li> <li>Modify the <code>myPlugin</code> name as appropriate</li> <li>Add your plugin logic</li> <li>Remove unused lifecycle hooks</li> <li>Save the file as <code>docsify-plugin-[name].js</code></li> <li>Load your plugin using a standard <code>&lt;script&gt;</code> tag</li> </ol> <pre><code>{\n  function myPlugin(hook, vm) {\n    // Invoked one time when docsify script is initialized\n    hook.init(() =&gt; {\n      // ...\n    });\n\n    // Invoked one time when the docsify instance has mounted on the DOM\n    hook.mounted(() =&gt; {\n      // ...\n    });\n\n    // Invoked on each page load before new markdown is transformed to HTML.\n    // Supports asynchronous tasks (see beforeEach documentation for details).\n    hook.beforeEach(markdown =&gt; {\n      // ...\n      return markdown;\n    });\n\n    // Invoked on each page load after new markdown has been transformed to HTML.\n    // Supports asynchronous tasks (see afterEach documentation for details).\n    hook.afterEach(html =&gt; {\n      // ...\n      return html;\n    });\n\n    // Invoked on each page load after new HTML has been appended to the DOM\n    hook.doneEach(() =&gt; {\n      // ...\n    });\n\n    // Invoked one time after rendering the initial page\n    hook.ready(() =&gt; {\n      // ...\n    });\n  }\n\n  // Add plugin to docsify's plugin array\n  window.$docsify = window.$docsify || {};\n  $docsify.plugins = [myPlugin, ...($docsify.plugins || [])];\n}\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#lifecycle-hooks","title":"Lifecycle Hooks","text":"<p>Lifecycle hooks are provided via the <code>hook</code> argument passed to the plugin function.</p>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#init","title":"init()","text":"<p>Invoked one time when docsify script is initialized.</p> <pre><code>hook.init(() =&gt; {\n  // ...\n});\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#mounted","title":"mounted()","text":"<p>Invoked one time when the docsify instance has mounted on the DOM.</p> <pre><code>hook.mounted(() =&gt; {\n  // ...\n});\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#beforeeach","title":"beforeEach()","text":"<p>Invoked on each page load before new markdown is transformed to HTML.</p> <pre><code>hook.beforeEach(markdown =&gt; {\n  // ...\n  return markdown;\n});\n</code></pre> <p>For asynchronous tasks, the hook function accepts a <code>next</code> callback as a second argument. Call this function with the final <code>markdown</code> value when ready. To prevent errors from affecting docsify and other plugins, wrap async code in a <code>try/catch/finally</code> block.</p> <pre><code>hook.beforeEach((markdown, next) =&gt; {\n  try {\n    // Async task(s)...\n  } catch (err) {\n    // ...\n  } finally {\n    next(markdown);\n  }\n});\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#aftereach","title":"afterEach()","text":"<p>Invoked on each page load after new markdown has been transformed to HTML.</p> <pre><code>hook.afterEach(html =&gt; {\n  // ...\n  return html;\n});\n</code></pre> <p>For asynchronous tasks, the hook function accepts a <code>next</code> callback as a second argument. Call this function with the final <code>html</code> value when ready. To prevent errors from affecting docsify and other plugins, wrap async code in a <code>try/catch/finally</code> block.</p> <pre><code>hook.afterEach((html, next) =&gt; {\n  try {\n    // Async task(s)...\n  } catch (err) {\n    // ...\n  } finally {\n    next(html);\n  }\n});\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#doneeach","title":"doneEach()","text":"<p>Invoked on each page load after new HTML has been appended to the DOM.</p> <pre><code>hook.doneEach(() =&gt; {\n  // ...\n});\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#ready","title":"ready()","text":"<p>Invoked one time after rendering the initial page.</p> <pre><code>hook.ready(() =&gt; {\n  // ...\n});\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#tips","title":"Tips","text":"<ul> <li>Access Docsify methods and properties using <code>window.Docsify</code></li> <li>Access the current Docsify instance using the <code>vm</code> argument</li> <li>Developers who prefer using a debugger can set the <code>catchPluginErrors</code> configuration option to <code>false</code> to allow their debugger to pause JavaScript execution on error</li> <li>Be sure to test your plugin on all supported platforms and with related configuration options (if applicable) before publishing</li> </ul>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#examples","title":"Examples","text":""},{"location":"docsify_docs/docsify/docs/write-a-plugin/#page-footer","title":"Page Footer","text":"<pre><code>window.$docsify = {\n  plugins: [\n    function pageFooter(hook, vm) {\n      const footer = /* html */ `\n        &lt;hr/&gt;\n        &lt;footer&gt;\n          &lt;span&gt;&lt;a href=\"https://github.com/QingWei-Li\"&gt;cinwell&lt;/a&gt; &amp;copy;2017.&lt;/span&gt;\n          &lt;span&gt;Proudly published with &lt;a href=\"https://github.com/docsifyjs/docsify\" target=\"_blank\"&gt;docsify&lt;/a&gt;.&lt;/span&gt;\n        &lt;/footer&gt;\n      `;\n\n      hook.afterEach(html =&gt; {\n        return html + footer;\n      });\n    },\n  ],\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/write-a-plugin/#edit-button-github","title":"Edit Button (GitHub)","text":"<pre><code>window.$docsify = {\n  plugins: [\n    function editButton(hook, vm) {\n      // The date template pattern\n      $docsify.formatUpdated = '{YYYY}/{MM}/{DD} {HH}:{mm}';\n\n      hook.beforeEach(html =&gt; {\n        const url =\n          'https://github.com/docsifyjs/docsify/blob/master/docs/' +\n          vm.route.file;\n        const editHtml = '[\ud83d\udcdd EDIT DOCUMENT](' + url + ')\\n';\n\n        return (\n          editHtml +\n          html +\n          '\\n----\\n' +\n          'Last modified {docsify-updated}' +\n          editHtml\n        );\n      });\n    },\n  ],\n};\n</code></pre>"},{"location":"docsify_docs/docsify/docs/_media/example-with-yaml/","title":"Example with yaml","text":"<p>This is from the <code>example-with-yaml.md</code></p>"},{"location":"docsify_docs/docsify/docs/_media/example/","title":"Example","text":"<p>This is from the <code>example.md</code></p>"},{"location":"docsify_docs/docsify/test/","title":"Docsify Testing","text":""},{"location":"docsify_docs/docsify/test/#environment","title":"Environment","text":"<ul> <li>Jest: A test framework used for assertions, mocks, spies, etc.</li> <li>Playwright: A test automation tool for launching browsers and manipulating the DOM.</li> </ul>"},{"location":"docsify_docs/docsify/test/#test-files","title":"Test files","text":"<ul> <li>Unit tests located in <code>/test/unit/</code> and use Jest.</li> <li>Integration tests are located in <code>/test/integration/</code> and use Jest.</li> <li>E2E tests are located in <code>/test/e2e/</code> and use Jest + Playwright.</li> </ul>"},{"location":"docsify_docs/docsify/test/#cli-commands","title":"CLI commands","text":"<pre><code># Run all tests\nnpm t\n\n# Run test types\nnpm run test:e2e\nnpm run test:integration\nnpm run test:unit\n</code></pre>"},{"location":"docsify_docs/docsify/test/#unit-integration-jest","title":"Unit / Integration (Jest)","text":"<pre><code># Run test file(s)\nnpm run test:unit -- -i ./path/to/file.test.js\nnpm run test:unit -- -i ./path/to/*.test.js\n\n# Run test name(s)\nnpm run test:unit -- -t \"my test\"\n\n# Run test name(s) in file\nnpm run test:unit -- -i ./path/to/file.test.js -t \"my test\"\n\n# ------------------------------------------------------------------------------\n\n# Update snapshots\nnpm run test:unit -- -u\n\n# Update snapshots for test file(s)\nnpm run test:unit -- -u -i ./path/to/file.test.js\nnpm run test:unit -- -u -i ./path/to/*.test.js\n\n# Update snapshots for test name(s)\nnpm run test:unit -- -u -t \"my test\"\n\n# Update snapshots for test name(s) in file\nnpm run test:unit -- -u -i ./path/to/file.test.js -t \"my test\"\n</code></pre>"},{"location":"docsify_docs/docsify/test/#e2e-playwright","title":"E2E (Playwright)","text":"<pre><code># Run test file(s)\nnpm run test:e2e -- ./path/to/file.test.js\nnpm run test:e2e -- ./path/to/*.test.js\n\n# Run test name(s)\nnpm run test:e2e -- -g \"my test\"\n\n# Run test name(s) in file\nnpm run test:e2e -- ./path/to/file.test.js -g \"my test\"\n\n# ------------------------------------------------------------------------------\n\n# Update snapshots\nnpm run test:e2e -- -u\n\n# Update snapshots for test file(s)\nnpm run test:e2e -- -u ./path/to/file.test.js\nnpm run test:e2e -- -u ./path/to/*.test.js\n\n# Update snapshots for test name(s)\nnpm run test:e2e -- -u -g \"my test\"\n\n# Update snapshots for test name(s) in file\nnpm run test:e2e -- -u ./path/to/file.test.js -g \"my test\"\n</code></pre>"},{"location":"opencv_docs/opencv/","title":"Index","text":""},{"location":"opencv_docs/opencv/#opencv-open-source-computer-vision-library","title":"OpenCV: Open Source Computer Vision Library","text":""},{"location":"opencv_docs/opencv/#resources","title":"Resources","text":"<ul> <li>Homepage: https://opencv.org</li> <li>Courses: https://opencv.org/courses</li> <li>Docs: https://docs.opencv.org/4.x/</li> <li>Q&amp;A forum: https://forum.opencv.org</li> <li>previous forum (read only): http://answers.opencv.org</li> <li>Issue tracking: https://github.com/opencv/opencv/issues</li> <li>Additional OpenCV functionality: https://github.com/opencv/opencv_contrib</li> <li>Donate to OpenCV: https://opencv.org/support/</li> </ul>"},{"location":"opencv_docs/opencv/#contributing","title":"Contributing","text":"<p>Please read the contribution guidelines before starting work on a pull request.</p>"},{"location":"opencv_docs/opencv/#summary-of-the-guidelines","title":"Summary of the guidelines:","text":"<ul> <li>One pull request per issue;</li> <li>Choose the right base branch;</li> <li>Include tests and documentation;</li> <li>Clean up \"oops\" commits before submitting;</li> <li>Follow the coding style guide.</li> </ul>"},{"location":"opencv_docs/opencv/#additional-resources","title":"Additional Resources","text":"<ul> <li>Submit your OpenCV-based project for inclusion in Community Friday on opencv.org</li> <li>Subscribe to the OpenCV YouTube Channel featuring OpenCV Live, an hour-long streaming show</li> <li>Follow OpenCV on LinkedIn for daily posts showing the state-of-the-art in computer vision &amp; AI</li> <li>Apply to be an OpenCV Volunteer to help organize events and online campaigns as well as amplify them</li> <li>Follow OpenCV on Mastodon in the Fediverse</li> <li>Follow OpenCV on Twitter</li> <li>OpenCV.ai: Computer Vision and AI development services from the OpenCV team.</li> </ul>"},{"location":"opencv_docs/opencv/CONTRIBUTING/","title":"CONTRIBUTING","text":""},{"location":"opencv_docs/opencv/CONTRIBUTING/#contributing-guidelines","title":"Contributing guidelines","text":"<p>All guidelines for contributing to the OpenCV repository can be found at <code>How to contribute guideline</code>.</p>"},{"location":"opencv_docs/opencv/SECURITY/","title":"Security Policy","text":""},{"location":"opencv_docs/opencv/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>If you have information about a security issue or vulnerability in OpenCV, please send an e-mail to security@opencv.org.</p> <p>Please provide as much information as possible:</p> <ul> <li> <p>A detailed description of the vulnerability we can use to reproduce your findings.</p> </li> <li> <p>Who can exploit this vulnerability and what would they gain. An attack scenario.</p> </li> <li> <p>Information about known exploits if any.</p> </li> </ul> <p>A member of the Security Team will review your e-mail and contact you to collaborate on resolving the issue.</p>"},{"location":"opencv_docs/opencv/SECURITY/#pgp-key","title":"PGP Key","text":"<p>If a security vulnerability report has extremely sensitive information you may encrypt it using our PGP public key (updated 2023-APR-19):</p> <pre><code>-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\ntVpvxypkp4FAHRP7cIS8ODU=\n=g4fo\n-----END PGP PUBLIC KEY BLOCK-----\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/carotene/","title":"Index","text":"<p>This is Carotene, a low-level library containing optimized CPU routines that are useful for computer vision algorithms.</p>"},{"location":"opencv_docs/opencv/3rdparty/cpufeatures/","title":"Index","text":"<p>The Android NDK provides a small library named cpufeatures that your app can use at runtime to detect the target device's CPU family and the optional features it supports. It is designed to work as-is on all official Android platform versions.</p> <p>https://developer.android.com/ndk/guides/cpu-features.html</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/","title":"Background","text":"<p>libjpeg-turbo is a JPEG image codec that uses SIMD instructions to accelerate baseline JPEG compression and decompression on x86, x86-64, Arm, PowerPC, and MIPS systems, as well as progressive JPEG compression on x86, x86-64, and Arm systems.  On such systems, libjpeg-turbo is generally 2-6x as fast as libjpeg, all else being equal.  On other types of systems, libjpeg-turbo can still outperform libjpeg by a significant amount, by virtue of its highly-optimized Huffman coding routines.  In many cases, the performance of libjpeg-turbo rivals that of proprietary high-speed JPEG codecs.</p> <p>libjpeg-turbo implements both the traditional libjpeg API as well as the less powerful but more straightforward TurboJPEG API.  libjpeg-turbo also features colorspace extensions that allow it to compress from/decompress to 32-bit and big-endian pixel buffers (RGBX, XBGR, etc.), as well as a full-featured Java interface.</p> <p>libjpeg-turbo was originally based on libjpeg/SIMD, an MMX-accelerated derivative of libjpeg v6b developed by Miyasaka Masaru.  The TigerVNC and VirtualGL projects made numerous enhancements to the codec in 2009, and in early 2010, libjpeg-turbo spun off into an independent project, with the goal of making high-speed JPEG compression/decompression technology available to a broader range of users and developers.  libjpeg-turbo is an ISO/IEC and ITU-T reference implementation of the JPEG standard.</p> <p>More information about libjpeg-turbo can be found at https://libjpeg-turbo.org.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#funding","title":"Funding","text":"<p>libjpeg-turbo is an independent open source project, but we rely on patronage and funded development in order to maintain that independence.  The easiest way to ensure that libjpeg-turbo remains community-focused and free of any one organization's agenda is to sponsor our project through GitHub. All sponsorship money goes directly toward funding the labor necessary to maintain libjpeg-turbo, support the user community, and implement bug fixes and strategically important features.</p> <p></p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#license","title":"License","text":"<p>libjpeg-turbo is covered by three compatible BSD-style open source licenses. Refer to LICENSE.md for a roll-up of license terms.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#building-libjpeg-turbo","title":"Building libjpeg-turbo","text":"<p>Refer to BUILDING.md for complete instructions.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#using-libjpeg-turbo","title":"Using libjpeg-turbo","text":"<p>libjpeg-turbo includes two APIs that can be used to compress and decompress JPEG images:</p> <ul> <li> <p>TurboJPEG API   This API provides an easy-to-use interface for compressing and decompressing   JPEG images in memory.  It also provides some functionality that would not be   straightforward to achieve using the underlying libjpeg API, such as   generating planar YUV images and performing multiple simultaneous lossless   transforms on an image.  The Java interface for libjpeg-turbo is written on   top of the TurboJPEG API.  The TurboJPEG API is recommended for first-time   users of libjpeg-turbo.  Refer to tjexample.c and   TJExample.java for examples of its usage and to   http://libjpeg-turbo.org/Documentation/Documentation for API documentation.</p> </li> <li> <p>libjpeg API   This is the de facto industry-standard API for compressing and decompressing   JPEG images.  It is more difficult to use than the TurboJPEG API but also   more powerful.  The libjpeg API implementation in libjpeg-turbo is both   API/ABI-compatible and mathematically compatible with libjpeg v6b.  It can   also optionally be configured to be API/ABI-compatible with libjpeg v7 and v8   (see below.)  Refer to cjpeg.c and djpeg.c for examples   of its usage and to libjpeg.txt for API documentation.</p> </li> </ul> <p>There is no significant performance advantage to either API when both are used to perform similar operations.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#colorspace-extensions","title":"Colorspace Extensions","text":"<p>libjpeg-turbo includes extensions that allow JPEG images to be compressed directly from (and decompressed directly to) buffers that use BGR, BGRX, RGBX, XBGR, and XRGB pixel ordering.  This is implemented with ten new colorspace constants:</p> <pre><code>JCS_EXT_RGB   /* red/green/blue */\nJCS_EXT_RGBX  /* red/green/blue/x */\nJCS_EXT_BGR   /* blue/green/red */\nJCS_EXT_BGRX  /* blue/green/red/x */\nJCS_EXT_XBGR  /* x/blue/green/red */\nJCS_EXT_XRGB  /* x/red/green/blue */\nJCS_EXT_RGBA  /* red/green/blue/alpha */\nJCS_EXT_BGRA  /* blue/green/red/alpha */\nJCS_EXT_ABGR  /* alpha/blue/green/red */\nJCS_EXT_ARGB  /* alpha/red/green/blue */\n</code></pre> <p>Setting <code>cinfo.in_color_space</code> (compression) or <code>cinfo.out_color_space</code> (decompression) to one of these values will cause libjpeg-turbo to read the red, green, and blue values from (or write them to) the appropriate position in the pixel when compressing from/decompressing to an RGB buffer.</p> <p>Your application can check for the existence of these extensions at compile time with:</p> <pre><code>#ifdef JCS_EXTENSIONS\n</code></pre> <p>At run time, attempting to use these extensions with a libjpeg implementation that does not support them will result in a \"Bogus input colorspace\" error. Applications can trap this error in order to test whether run-time support is available for the colorspace extensions.</p> <p>When using the RGBX, BGRX, XBGR, and XRGB colorspaces during decompression, the X byte is undefined, and in order to ensure the best performance, libjpeg-turbo can set that byte to whatever value it wishes.  If an application expects the X byte to be used as an alpha channel, then it should specify <code>JCS_EXT_RGBA</code>, <code>JCS_EXT_BGRA</code>, <code>JCS_EXT_ABGR</code>, or <code>JCS_EXT_ARGB</code>.  When these colorspace constants are used, the X byte is guaranteed to be 0xFF, which is interpreted as opaque.</p> <p>Your application can check for the existence of the alpha channel colorspace extensions at compile time with:</p> <pre><code>#ifdef JCS_ALPHA_EXTENSIONS\n</code></pre> <p>jcstest.c, located in the libjpeg-turbo source tree, demonstrates how to check for the existence of the colorspace extensions at compile time and run time.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#libjpeg-v7-and-v8-apiabi-emulation","title":"libjpeg v7 and v8 API/ABI Emulation","text":"<p>With libjpeg v7 and v8, new features were added that necessitated extending the compression and decompression structures.  Unfortunately, due to the exposed nature of those structures, extending them also necessitated breaking backward ABI compatibility with previous libjpeg releases.  Thus, programs that were built to use libjpeg v7 or v8 did not work with libjpeg-turbo, since it is based on the libjpeg v6b code base.  Although libjpeg v7 and v8 are not as widely used as v6b, enough programs (including a few Linux distros) made the switch that there was a demand to emulate the libjpeg v7 and v8 ABIs in libjpeg-turbo.  It should be noted, however, that this feature was added primarily so that applications that had already been compiled to use libjpeg v7+ could take advantage of accelerated baseline JPEG encoding/decoding without recompiling.  libjpeg-turbo does not claim to support all of the libjpeg v7+ features, nor to produce identical output to libjpeg v7+ in all cases (see below.)</p> <p>By passing an argument of <code>-DWITH_JPEG7=1</code> or <code>-DWITH_JPEG8=1</code> to <code>cmake</code>, you can build a version of libjpeg-turbo that emulates the libjpeg v7 or v8 ABI, so that programs that are built against libjpeg v7 or v8 can be run with libjpeg-turbo.  The following section describes which libjpeg v7+ features are supported and which aren't.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#support-for-libjpeg-v7-and-v8-features","title":"Support for libjpeg v7 and v8 Features","text":""},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#fully-supported","title":"Fully supported","text":"<ul> <li> <p>libjpeg API: IDCT scaling extensions in decompressor   libjpeg-turbo supports IDCT scaling with scaling factors of 1/8, 1/4, 3/8,   1/2, 5/8, 3/4, 7/8, 9/8, 5/4, 11/8, 3/2, 13/8, 7/4, 15/8, and 2/1 (only 1/4   and 1/2 are SIMD-accelerated.)</p> </li> <li> <p>libjpeg API: Arithmetic coding</p> </li> <li> <p>libjpeg API: In-memory source and destination managers   See notes below.</p> </li> <li> <p>cjpeg: Separate quality settings for luminance and chrominance   Note that the libpjeg v7+ API was extended to accommodate this feature only   for convenience purposes.  It has always been possible to implement this   feature with libjpeg v6b (see rdswitch.c for an example.)</p> </li> <li> <p>cjpeg: 32-bit BMP support</p> </li> <li> <p>cjpeg: <code>-rgb</code> option</p> </li> <li> <p>jpegtran: Lossless cropping</p> </li> <li> <p>jpegtran: <code>-perfect</code> option</p> </li> <li> <p>jpegtran: Forcing width/height when performing lossless crop</p> </li> <li> <p>rdjpgcom: <code>-raw</code> option</p> </li> <li> <p>rdjpgcom: Locale awareness</p> </li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#not-supported","title":"Not supported","text":"<p>NOTE:  As of this writing, extensive research has been conducted into the usefulness of DCT scaling as a means of data reduction and SmartScale as a means of quality improvement.  Readers are invited to peruse the research at http://www.libjpeg-turbo.org/About/SmartScale and draw their own conclusions, but it is the general belief of our project that these features have not demonstrated sufficient usefulness to justify inclusion in libjpeg-turbo.</p> <ul> <li> <p>libjpeg API: DCT scaling in compressor <code>cinfo.scale_num</code> and <code>cinfo.scale_denom</code> are silently ignored.   There is no technical reason why DCT scaling could not be supported when   emulating the libjpeg v7+ API/ABI, but without the SmartScale extension (see   below), only scaling factors of 1/2, 8/15, 4/7, 8/13, 2/3, 8/11, 4/5, and   8/9 would be available, which is of limited usefulness.</p> </li> <li> <p>libjpeg API: SmartScale <code>cinfo.block_size</code> is silently ignored.   SmartScale is an extension to the JPEG format that allows for DCT block   sizes other than 8x8.  Providing support for this new format would be   feasible (particularly without full acceleration.)  However, until/unless   the format becomes either an official industry standard or, at minimum, an   accepted solution in the community, we are hesitant to implement it, as   there is no sense of whether or how it might change in the future.  It is   our belief that SmartScale has not demonstrated sufficient usefulness as a   lossless format nor as a means of quality enhancement, and thus our primary   interest in providing this feature would be as a means of supporting   additional DCT scaling factors.</p> </li> <li> <p>libjpeg API: Fancy downsampling in compressor <code>cinfo.do_fancy_downsampling</code> is silently ignored.   This requires the DCT scaling feature, which is not supported.</p> </li> <li> <p>jpegtran: Scaling   This requires both the DCT scaling and SmartScale features, which are not   supported.</p> </li> <li> <p>Lossless RGB JPEG files   This requires the SmartScale feature, which is not supported.</p> </li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#what-about-libjpeg-v9","title":"What About libjpeg v9?","text":"<p>libjpeg v9 introduced yet another field to the JPEG compression structure (<code>color_transform</code>), thus making the ABI backward incompatible with that of libjpeg v8.  This new field was introduced solely for the purpose of supporting lossless SmartScale encoding.  Furthermore, there was actually no reason to extend the API in this manner, as the color transform could have just as easily been activated by way of a new JPEG colorspace constant, thus preserving backward ABI compatibility.</p> <p>Our research (see link above) has shown that lossless SmartScale does not generally accomplish anything that can't already be accomplished better with existing, standard lossless formats.  Therefore, at this time it is our belief that there is not sufficient technical justification for software projects to upgrade from libjpeg v8 to libjpeg v9, and thus there is not sufficient technical justification for us to emulate the libjpeg v9 ABI.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#in-memory-sourcedestination-managers","title":"In-Memory Source/Destination Managers","text":"<p>By default, libjpeg-turbo 1.3 and later includes the <code>jpeg_mem_src()</code> and <code>jpeg_mem_dest()</code> functions, even when not emulating the libjpeg v8 API/ABI. Previously, it was necessary to build libjpeg-turbo from source with libjpeg v8 API/ABI emulation in order to use the in-memory source/destination managers, but several projects requested that those functions be included when emulating the libjpeg v6b API/ABI as well.  This allows the use of those functions by programs that need them, without breaking ABI compatibility for programs that don't, and it allows those functions to be provided in the \"official\" libjpeg-turbo binaries.</p> <p>Note that, on most Un*x systems, the dynamic linker will not look for a function in a library until that function is actually used.  Thus, if a program is built against libjpeg-turbo 1.3+ and uses <code>jpeg_mem_src()</code> or <code>jpeg_mem_dest()</code>, that program will not fail if run against an older version of libjpeg-turbo or against libjpeg v7- until the program actually tries to call <code>jpeg_mem_src()</code> or <code>jpeg_mem_dest()</code>.  Such is not the case on Windows. If a program is built against the libjpeg-turbo 1.3+ DLL and uses <code>jpeg_mem_src()</code> or <code>jpeg_mem_dest()</code>, then it must use the libjpeg-turbo 1.3+ DLL at run time.</p> <p>Both cjpeg and djpeg have been extended to allow testing the in-memory source/destination manager functions.  See their respective man pages for more details.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#mathematical-compatibility","title":"Mathematical Compatibility","text":"<p>For the most part, libjpeg-turbo should produce identical output to libjpeg v6b.  There are two exceptions:</p> <ol> <li> <p>When decompressing a JPEG image that uses 4:4:0 chrominance subsampling, the outputs of libjpeg v6b and libjpeg-turbo can differ because libjpeg-turbo implements a \"fancy\" (smooth) 4:4:0 upsampling algorithm and libjpeg did not.</p> </li> <li> <p>When using the floating point DCT/IDCT, the outputs of libjpeg v6b and libjpeg-turbo can differ for the following reasons:</p> <ul> <li> <p>The SSE/SSE2 floating point DCT implementation in libjpeg-turbo is ever   so slightly more accurate than the implementation in libjpeg v6b, but not   by any amount perceptible to human vision (generally in the range of 0.01   to 0.08 dB gain in PNSR.)</p> </li> <li> <p>When not using the SIMD extensions, libjpeg-turbo uses the more accurate   (and slightly faster) floating point IDCT algorithm introduced in libjpeg   v8a as opposed to the algorithm used in libjpeg v6b.  It should be noted,   however, that this algorithm basically brings the accuracy of the   floating point IDCT in line with the accuracy of the accurate integer   IDCT.  The floating point DCT/IDCT algorithms are mainly a legacy   feature, and they do not produce significantly more accuracy than the   accurate integer algorithms.  (To put numbers on this, the typical   difference in PNSR between the two algorithms is less than 0.10 dB,   whereas changing the quality level by 1 in the upper range of the quality   scale is typically more like a 1.0 dB difference.)</p> </li> <li> <p>If the floating point algorithms in libjpeg-turbo are not implemented   using SIMD instructions on a particular platform, then the accuracy of   the floating point DCT/IDCT can depend on the compiler settings.</p> </li> </ul> </li> </ol> <p>While libjpeg-turbo does emulate the libjpeg v8 API/ABI, under the hood it is still using the same algorithms as libjpeg v6b, so there are several specific cases in which libjpeg-turbo cannot be expected to produce the same output as libjpeg v8:</p> <ul> <li> <p>When decompressing using scaling factors of 1/2 and 1/4, because libjpeg v8   implements those scaling algorithms differently than libjpeg v6b does, and   libjpeg-turbo's SIMD extensions are based on the libjpeg v6b behavior.</p> </li> <li> <p>When using chrominance subsampling, because libjpeg v8 implements this   with its DCT/IDCT scaling algorithms rather than with a separate   downsampling/upsampling algorithm.  In our testing, the subsampled/upsampled   output of libjpeg v8 is less accurate than that of libjpeg v6b for this   reason.</p> </li> <li> <p>When decompressing using a scaling factor &gt; 1 and merged (AKA \"non-fancy\" or   \"non-smooth\") chrominance upsampling, because libjpeg v8 does not support   merged upsampling with scaling factors &gt; 1.</p> </li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#performance-pitfalls","title":"Performance Pitfalls","text":""},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#restart-markers","title":"Restart Markers","text":"<p>The optimized Huffman decoder in libjpeg-turbo does not handle restart markers in a way that makes the rest of the libjpeg infrastructure happy, so it is necessary to use the slow Huffman decoder when decompressing a JPEG image that has restart markers.  This can cause the decompression performance to drop by as much as 20%, but the performance will still be much greater than that of libjpeg.  Many consumer packages, such as Photoshop, use restart markers when generating JPEG images, so images generated by those programs will experience this issue.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#fast-integer-forward-dct-at-high-quality-levels","title":"Fast Integer Forward DCT at High Quality Levels","text":"<p>The algorithm used by the SIMD-accelerated quantization function cannot produce correct results whenever the fast integer forward DCT is used along with a JPEG quality of 98-100.  Thus, libjpeg-turbo must use the non-SIMD quantization function in those cases.  This causes performance to drop by as much as 40%. It is therefore strongly advised that you use the accurate integer forward DCT whenever encoding images with a JPEG quality of 98 or higher.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/#memory-debugger-pitfalls","title":"Memory Debugger Pitfalls","text":"<p>Valgrind and Memory Sanitizer (MSan) can generate false positives (specifically, incorrect reports of uninitialized memory accesses) when used with libjpeg-turbo's SIMD extensions.  It is generally recommended that the SIMD extensions be disabled, either by passing an argument of <code>-DWITH_SIMD=0</code> to <code>cmake</code> when configuring the build or by setting the environment variable <code>JSIMD_FORCENONE</code> to <code>1</code> at run time, when testing libjpeg-turbo with Valgrind, MSan, or other memory debuggers.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/LICENSE/","title":"libjpeg-turbo Licenses","text":"<p>libjpeg-turbo is covered by two compatible BSD-style open source licenses:</p> <ul> <li>The IJG (Independent JPEG Group) License, which is listed in   README.ijg</li> </ul> <p>This license applies to the libjpeg API library and associated programs,   including any code inherited from libjpeg and any modifications to that   code.  Note that the libjpeg-turbo SIMD source code bears the   zlib License, but in the context of   the overall libjpeg API library, the terms of the zlib License are subsumed   by the terms of the IJG License.</p> <ul> <li>The Modified (3-clause) BSD License, which is listed below</li> </ul> <p>This license applies to the TurboJPEG API library and associated programs, as   well as the build system.  Note that the TurboJPEG API library wraps the   libjpeg API library, so in the context of the overall TurboJPEG API library,   both the terms of the IJG License and the terms of the Modified (3-clause)   BSD License apply.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/LICENSE/#complying-with-the-libjpeg-turbo-licenses","title":"Complying with the libjpeg-turbo Licenses","text":"<p>This section provides a roll-up of the libjpeg-turbo licensing terms, to the best of our understanding.  This is not a license in and of itself.  It is intended solely for clarification.</p> <ol> <li> <p>If you are distributing a modified version of the libjpeg-turbo source,     then:</p> <ol> <li> <p>You cannot alter or remove any existing copyright or license notices     from the source.</p> <p>Origin - Clause 1 of the IJG License - Clause 1 of the Modified BSD License - Clauses 1 and 3 of the zlib License</p> </li> <li> <p>You must add your own copyright notice to the header of each source     file you modified, so others can tell that you modified that file.  (If     there is not an existing copyright header in that file, then you can     simply add a notice stating that you modified the file.)</p> <p>Origin - Clause 1 of the IJG License - Clause 2 of the zlib License</p> </li> <li> <p>You must include the IJG README file, and you must not alter any of the     copyright or license text in that file.</p> <p>Origin - Clause 1 of the IJG License</p> </li> </ol> </li> <li> <p>If you are distributing only libjpeg-turbo binaries without the source, or     if you are distributing an application that statically links with     libjpeg-turbo, then:</p> <ol> <li> <p>Your product documentation must include a message stating:</p> <p>This software is based in part on the work of the Independent JPEG Group.</p> <p>Origin - Clause 2 of the IJG license</p> </li> <li> <p>If your binary distribution includes or uses the TurboJPEG API, then     your product documentation must include the text of the Modified BSD     License (see below.)</p> <p>Origin - Clause 2 of the Modified BSD License</p> </li> </ol> </li> <li> <p>You cannot use the name of the IJG or The libjpeg-turbo Project or the     contributors thereof in advertising, publicity, etc.</p> <p>Origin - IJG License - Clause 3 of the Modified BSD License</p> </li> <li> <p>The IJG and The libjpeg-turbo Project do not warrant libjpeg-turbo to be     free of defects, nor do we accept any liability for undesirable     consequences resulting from your use of the software.</p> <p>Origin - IJG License - Modified BSD License - zlib License</p> </li> </ol>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/LICENSE/#the-modified-3-clause-bsd-license","title":"The Modified (3-clause) BSD License","text":"<p>Copyright (C)2009-2023 D. R. Commander.  All Rights Reserved. Copyright (C)2015 Viktor Szathm\u00e1ry.  All Rights Reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ul> <li>Redistributions of source code must retain the above copyright notice,   this list of conditions and the following disclaimer.</li> <li>Redistributions in binary form must reproduce the above copyright notice,   this list of conditions and the following disclaimer in the documentation   and/or other materials provided with the distribution.</li> <li>Neither the name of the libjpeg-turbo Project nor the names of its   contributors may be used to endorse or promote products derived from this   software without specific prior written permission.</li> </ul> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\", AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"opencv_docs/opencv/3rdparty/libjpeg-turbo/LICENSE/#why-two-licenses","title":"Why Two Licenses?","text":"<p>The zlib License could have been used instead of the Modified (3-clause) BSD License, and since the IJG License effectively subsumes the distribution conditions of the zlib License, this would have effectively placed libjpeg-turbo binary distributions under the IJG License.  However, the IJG License specifically refers to the Independent JPEG Group and does not extend attribution and endorsement protections to other entities.  Thus, it was desirable to choose a license that granted us the same protections for new code that were granted to the IJG for code derived from their software.</p>"},{"location":"opencv_docs/opencv/3rdparty/libtiff/","title":"Index","text":""},{"location":"opencv_docs/opencv/3rdparty/libtiff/#tiff-software-distribution","title":"TIFF Software Distribution","text":"<p>This file is just a placeholder; the entire documentation is now located as reStructuredText in the doc directory. To view the documentation as HTML, visit https://libtiff.gitlab.io/libtiff/ or http://www.simplesystems.org/libtiff/ or within the release package in the doc/html-prebuilt directory. The manual pages are located at doc/man-prebuilt.</p> <p>The release package can be downloaded at</p> <p>http://download.osgeo.org/libtiff/</p> <p>If you can't hack either of these options then basically what you want to do is:</p> <pre><code>% ./configure\n% make\n% su\n# make install\n</code></pre> <p>More information, email contacts, and mailing list information can be  found online at http://www.simplesystems.org/libtiff/</p>"},{"location":"opencv_docs/opencv/3rdparty/libtiff/#source-code-repository","title":"Source code repository","text":"<p>GitLab</p>"},{"location":"opencv_docs/opencv/3rdparty/libtiff/#bug-database","title":"Bug database","text":"<p>GitLab issues</p> <p>Previously, the project used Bugzilla. This is no longer in use, and all remaining issues have been migrated to GitLab.</p>"},{"location":"opencv_docs/opencv/3rdparty/libtiff/#use-and-copyright","title":"Use and Copyright","text":"<p>Silicon Graphics has seen fit to allow us to give this work away.  It is free.  There is no support or guarantee of any sort as to its operations, correctness, or whatever.  If you do anything useful with all or parts of it you need to honor the copyright notices.   I would also be interested in knowing about it and, hopefully, be acknowledged.</p> <p>The legal way of saying that is:</p> <p>Copyright (c) 1988-1997 Sam Leffler Copyright (c) 1991-1997 Silicon Graphics, Inc.</p> <p>Permission to use, copy, modify, distribute, and sell this software and  its documentation for any purpose is hereby granted without fee, provided that (i) the above copyright notices and this permission notice appear in all copies of the software and related documentation, and (ii) the names of Sam Leffler and Silicon Graphics may not be used in any advertising or publicity relating to the software without the specific, prior written permission of Sam Leffler and Silicon Graphics.</p> <p>THE SOFTWARE IS PROVIDED \"AS-IS\" AND WITHOUT WARRANTY OF ANY KIND,  EXPRESS, IMPLIED OR OTHERWISE, INCLUDING WITHOUT LIMITATION, ANY  WARRANTY OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.  </p> <p>IN NO EVENT SHALL SAM LEFFLER OR SILICON GRAPHICS BE LIABLE FOR ANY SPECIAL, INCIDENTAL, INDIRECT OR CONSEQUENTIAL DAMAGES OF ANY KIND, OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER OR NOT ADVISED OF THE POSSIBILITY OF DAMAGE, AND ON ANY THEORY OF  LIABILITY, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE  OF THIS SOFTWARE.</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/","title":"OPENJPEG Library and Applications","text":""},{"location":"opencv_docs/opencv/3rdparty/openjpeg/#what-is-openjpeg","title":"What is OpenJPEG ?","text":"<p>OpenJPEG is an open-source JPEG 2000 codec written in C language. It has been developed in order to promote the use of JPEG 2000, a still-image compression standard from the Joint Photographic Experts Group (JPEG).  Since April 2015, it is officially recognized by ISO/IEC and ITU-T as a JPEG 2000 Reference Software.</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/#who-can-use-the-code","title":"Who can use the code ?","text":"<p>Anyone. As the OpenJPEG code is released under the BSD 2-clause \"Simplified\" License, anyone can use or modify the code, even for commercial applications. The only restriction is to retain the copyright in the sources or in the binaries documentation. Of course, if you modified the code in a way that might be of interest for other users, you are encouraged to share it (through a github pull request or by filling an issue) but this is not a requirement.</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/#how-to-install-and-use-openjpeg","title":"How to install and use OpenJPEG ?","text":"<p>API Documentation needs a major refactoring. Meanwhile, you can check installation instructions and codec documentation.</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/#current-status","title":"Current Status","text":""},{"location":"opencv_docs/opencv/3rdparty/openjpeg/#who-are-the-developers","title":"Who are the developers ?","text":"<p>The library is developed and maintained by the Image and Signal Processing Group (ISPGroup), in the Universit\u00e9 catholique de Louvain (UCL, with the support of the CNES, the CS company and the intoPIX company. The JPWL module has been developed by the Digital Signal Processing Lab (DSPLab) of the University of Perugia, Italy (UNIPG).</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/#details-on-folders-hierarchy","title":"Details on folders hierarchy","text":"<ul> <li>src</li> <li>lib<ul> <li>openjp2: contains the sources of the openjp2 library (Part 1 &amp; 2)</li> <li>openjpip: complete client-server architecture for remote browsing of jpeg 2000 images.</li> </ul> </li> <li>bin: contains all applications that use the openjpeg library<ul> <li>common: common files to all applications</li> <li>jp2: a basic codec</li> <li>jpip: OpenJPIP applications (server and dec server)</li> <li>java: a Java client viewer for JPIP</li> <li>wx</li> <li>OPJViewer: gui for displaying j2k files (based on wxWidget)</li> </ul> </li> <li>wrapping</li> <li>java: java jni to use openjpeg in a java program</li> <li>thirdparty: thirdparty libraries used by some applications. These libraries will be built only if there are not found on the system. Note that libopenjpeg itself does not have any dependency.</li> <li>doc: doxygen documentation setup file and man pages</li> <li>tests: configuration files and utilities for the openjpeg test suite. All test images are located in openjpeg-data repository.</li> <li>cmake: cmake related files</li> <li>scripts: scripts for developers</li> </ul> <p>See LICENSE for license and copyright information.</p> <p>See INSTALL for installation procedures.</p> <p>See NEWS for user visible changes in successive releases.</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/#deprecated-components","title":"Deprecated components","text":"<p>The openjpwl, openjp3d and openmj2 components have been removed after the 2.4.0 release. Their building and working state is unknown. People interested in them should start from the 2.4.0 tag.</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/#apiabi","title":"API/ABI","text":"<p>An API/ABI timeline is automatically updated here.</p> <p>OpenJPEG strives to provide a stable API/ABI for your applications. As such it only exposes a limited subset of its functions.  It uses a mechanism of exporting/hiding functions. If you are unsure which functions you can use in your applications, you should compile OpenJPEG using something similar to gcc: <code>-fvisibility=hidden</code> compilation flag. See also: http://gcc.gnu.org/wiki/Visibility</p> <p>On windows, MSVC directly supports export/hiding function and as such the only API available is the one supported by OpenJPEG.</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/AUTHORS/","title":"Authors of OpenJPEG","text":"<p>See also THANKS</p> <p>David Janssens designed and implemented the first version of OpenJPEG.</p> <p>Kaori Hagihara designed and implemented the first version of OpenJPIP.</p> <p>Jerome Fimes implemented the alpha version of OpenJPEG 2.0.</p> <p>Giuseppe Baruffa added the JPWL functionalities.</p> <p>Micka\u00ebl Savinaud implemented the final OpenJPEG 2.0 version based on a big merge between 1.5 version and alpha version of 2.0.</p> <p>Mathieu Malaterre participated to the OpenJPEG 2.0 version and improved the libraries and utilities. </p> <p>Yannick Verschueren,  Herve Drolon,  Francois-Olivier Devaux,  Antonin Descampe     improved the libraries and utilities.</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/","title":"Changelog","text":""},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#v250-2022-05-13","title":"v2.5.0 (2022-05-13)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>tools/travis-ci/install.sh: git clone with https:// to fix 'The unaut\u2026 #1419 (rouault)</li> <li>Java Support 1.8 now... #1418 (jiapei100)</li> <li>Separate fuzz targets to increase coverage #1416 (Navidem)</li> <li>CMakeLists.txt: do not set INSTALL_NAME_DIR for MacOS builds for CMake &gt;= 3.0 (fixes #1404) #1410 (rouault)</li> <li>Avoid integer overflows in DWT. Fixes https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=44544 #1408 (rouault)</li> <li>Updated \"added support for partial bitstream decoding\" #1407 (Neopallium)</li> <li>opj_encoder_set_extra_options(): add a GUARD_BITS=value option #1403 (rouault)</li> <li>More overflow related bug fixes #1402 (Eharve14)</li> <li>opj_j2k_setup_encoder(): validate number of tiles to avoid illegal values and potential overflow (fixes #1399) #1401 (rouault)</li> <li>Missed conversion from unsigned int to OPJ_INT32  #1398 (Eharve14)</li> <li>Added check for integer overflow in get_num_images  #1397 (Eharve14)</li> <li>Added overflow check for CVE-2021-29338  #1396 (Eharve14)</li> <li>Fix integer overflow in num_images #1395 (baparham)</li> <li>Remove duplicate assignments in function tiftoimage #1392 (stweil)</li> <li>Fix some typos (found by codespell) #1390 (stweil)</li> <li>CI: Add CIFuzz action #1386 (DavidKorczynski)</li> <li>Feature/decimation #1384 (msheby)</li> <li>API: deprecate 'bpp' member in favor of 'prec' #1383 (rouault)</li> <li>Added support for high throughput (HTJ2K) decoding. #1381 (rouault)</li> <li>verify-indentation.sh: fix for pull request from bar/master to foo/master #1380 (rouault)</li> <li>Change integer for version libtiff 4.3.0 #1377 (Jamaika1)</li> <li>Port continuous integration to github actions #1376 (rouault)</li> <li>Avoid integer overflows in DWT.  #1375 (rouault)</li> <li>LGTM warning: Comparison result is always the same #1373 (DimitriPapadopoulos)</li> <li>A couple typos found by codespell #1371 (DimitriPapadopoulos)</li> <li>cmake: add install interface include directory #1370 (madebr)</li> <li>fix issues 1368: exist a issues of freeing uninitialized pointer in src/bin/jp2/opj_decompress.c\uff0cthat will cause a segfault #1369 (xiaoxiaoafeifei)</li> <li>opj_j2k_is_imf_compliant: Fix out of bounds access #1366 (sebras)</li> <li>opj_j2k_is_imf_compliant: Fix argument formatting for warnings. #1365 (sebras)</li> <li>CMakeLists.txt/appveyor.yml: update version number to 2.5.0\u2026 #1361 (rouault)</li> <li>.travis.yml: try to fix gcc 4.8 config by updating to xenial #1360 (rouault)</li> <li>Add support for enabling generation of TLM markers in encoder #1359 (rouault)</li> <li>Fix various compiler warnings #1358 (rouault)</li> <li>fix #1345: don't remove big endian test for other platforms #1354 (msabwat)</li> <li>Remove obsolete components JPWL, JP3D and MJ2 #1350 (rouault)</li> <li>tools/travis-ci/install.sh: fix links to Kakadu and jpylyzer binaries #1348 (rouault)</li> <li>emscripten: disable big endian test #1345 (msabwat)</li> <li>Fix cmake file with DESTDIR #1321 (ffontaine)</li> <li>CMakeLists.txt: Don't require a C++ compiler #1317 (ffontaine)</li> <li>Import files tiff and yuv(raw) #1316 (Jamaika1)</li> <li>Fix year in NEWS #1312 (stweil)</li> <li>Fix lcms2 static linking using pgk config #867 (pseiderer)</li> <li>fix static build only against tiff and its indirect dependencies #866 (tSed)</li> </ul> <p>Closed issues:</p> <ul> <li>integer constant is too large for 'long' type #61</li> <li>Openjpeg3D lossy compression not working #501</li> <li>mj2: Array overflow #485</li> <li>OPJ fails to decode image that KDU manages correctly #419</li> <li>yuvtoimage() bug in v1 and v2 for 16-Bit: please apply ASAP #384</li> <li>JP3D: Fix CVE-2013-4289 CVE-2013-4290 #298</li> <li>MJ2 libraries are installed in lib #204</li> <li>MJ2: realloc is misused and may leak memory #168</li> <li>MJ2 wrapper not functional #143</li> <li>JPWL is broken in trunk #137</li> <li>MJ2 files not using OPENJPEG API correctly #53</li> <li>Maximum bit depth supported by the OpenJPEG implementation of JP3D #9</li> <li>does openjpeg support either visually lossless or numerically lossless jpeg2000 compression? #1406</li> <li>extract jpeg2000 tile without decompression #1405</li> <li>openjpeg doesn't install a relocatable shared lib on macOS #1404</li> <li>pull request - the cinema industry awaits! #1400</li> <li>Integer overflows in j2K #1399</li> <li>why lossly compression performance worse than jpeg when compress png\uff1f #1393</li> <li>cect #1389</li> <li>the docs don't describe bpp and prec in opj_image_comp very well #1379</li> <li>converting .png to .jp2 by opj_compress is different from the original image #1378</li> <li>Comparison result is always the same #1372</li> <li>Exist a issues of freeing uninitialized pointer in src/bin/jp2/opj_decompress.c\uff0cthat will cause a segfault #1368</li> <li>[TEST NOT RUNNING]: bigendian test #1355</li> <li>opj_decompress 2.4.0 built with library 2.3.0.  #1352</li> <li>New library htjpeg2000 #1351</li> <li>Integer Overflow in num_images #1338</li> <li>All IMF Profile Selections Result in PART1 #1337</li> <li>grayscale image #1334</li> <li>error C2169: 'lrintf': intrinsic function, cannot be defined #1333</li> <li>Generate lower-case extension #1332</li> <li>color of reconstructed png file much darker #1330</li> <li>CVE-2019-6988, CVE-2018-20846 and CVE-2018-16376 #1328</li> <li>opj 2.4.0: opj_free missing in opj3d #1327</li> <li>Not able to compress volumetric data #1326</li> <li>HTML documents are not installed in specified place #1322</li> <li>Can't find openjpeg.h when cross-compile. #1320</li> <li>OpenJPEG is available with EasyConfig #1319</li> <li>Building Test Programs #1318</li> <li>Builds are not reproducible #1275</li> <li>strange behaviour of opj_jp3d_compress/decompress utility #1274</li> <li>Potential heap-based buffer overflow in function t2_encode_packet in src/lib/openmj2/t2.c and src/lib/openjp3d/t2.c #1272</li> <li>Function tgatoimage  in src/bin/jpwl/convert.c  need to check that the file is big enough to avoid excessive memory allocations #1271</li> <li>memory &amp; cpu are exhausted when converting jp2 file into png #1250</li> <li>Cannot compress PGX into JP3D despite following the directions? #1134</li> <li>sscanf buffer overflow in opj_jp3d_compress.c #1130</li> <li>integer underflow may lead to writing garbage #1089</li> <li>sscanf buffer overflow #1087</li> <li>strcpy overflows #1086</li> <li>sprintf buffer overflows #1084</li> <li>strcpy buffer overflow #1083</li> <li>integer overflow in malloc() #1082</li> <li>out of bounds writes #1078</li> <li>out of bounds writes #1077</li> <li>divide by zero, perhaps multiplication overflow #1076</li> <li>missing format string parameter #1075</li> <li>leaks from cppcheck in lib folder #1038</li> <li>How to initialize DEBUG_PROFILE in color.c? #958</li> <li>JP3D sample files #865</li> <li>BIG_ENDIAN bug in jpwl.c #839</li> <li>OpenJPEG fails to decode partial j2c where kdu succeeds #715</li> <li>building mj2 binaries fails #652</li> <li>openmj2\\mj2.c: Out of bounds #646</li> <li>bin\\mj2\\opj_mj2_decompress.c 101 wrong check / leak #608</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#v240-2020-12-28","title":"v2.4.0 (2020-12-28)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>OPENJPEG_INSTALL_DOC_DIR does not control a destination directory where HTML docs would be installed. #1309</li> <li>Heap-buffer-overflow in lib/openjp2/pi.c:312 #1302</li> <li>Heap-buffer-overflow in lib/openjp2/t2.c:973 #1299</li> <li>Heap-buffer-overflow in lib/openjp2/pi.c:623 #1293</li> <li>Global-buffer-overflow in lib/openjp2/dwt.c:1980 #1286</li> <li>Heap-buffer-overflow in lib/openjp2/tcd.c:2417 #1284</li> <li>Heap-buffer-overflow in lib/openjp2/mqc.c:499 #1283</li> <li>Openjpeg could not encode 32bit RGB float image #1281</li> <li>Openjpeg could not encode 32bit RGB float image #1280</li> <li>ISO/IEC 15444-1:2019 (E) compared with 'cio.h' #1277</li> <li>Test-suite failure due to hash mismatch #1264</li> <li>Heap use-after-free #1261</li> <li>Memory leak when failing to allocate object... #1259</li> <li>Memory leak of Tier 1 handle when OpenJPEG fails to set it as TLS... #1257</li> <li>Any plan to build release for CVE-2020-8112/CVE-2020-6851 #1247</li> <li>failing to convert 16-bit file: opj_t2_encode_packet(): only 5251 bytes remaining in output buffer. 5621 needed. #1243</li> <li>CMake+VS2017 Compile OK, thirdparty Compile OK, but thirdparty not install #1239</li> <li>New release to solve CVE-2019-6988 ? #1238</li> <li>Many tests fail to pass after the update of libtiff to version 4.1.0 #1233</li> <li>Another heap buffer overflow in libopenjp2 #1231</li> <li>Heap buffer overflow in libopenjp2 #1228</li> <li>Endianness of binary volume (JP3D) #1224</li> <li>New release to resolve CVE-2019-12973 #1222</li> <li>how to set the block size,like 128,256 ? #1216</li> <li>compress YUV files to motion jpeg2000 standard #1213</li> <li>Repair/update Java wrapper, and include in release #1208</li> <li>abc #1206</li> <li>Slow decoding #1202</li> <li>Installation question #1201</li> <li>Typo in test_decode_area - *ptilew is assigned instead of *ptileh #1195</li> <li>Creating a J2K file with one POC is broken #1191</li> <li>Make fails on Arch Linux #1174</li> <li>Heap buffer overflow in opj_t1_clbl_decode_processor() triggered with Ghostscript #1158</li> <li>opj_stream_get_number_byte_left: Assertion `p_stream-&gt;m_byte_offset &gt;= 0' failed. #1151</li> <li>The fuzzer ignores too many inputs #1079</li> <li>out of bounds read #1068</li> </ul> <p>Merged pull requests:</p> <ul> <li>Change defined WIN32 #1310 (Jamaika1)</li> <li>docs: fix simple typo, producted -&gt; produced #1308 (timgates42)</li> <li>Set ${OPENJPEG_INSTALL_DOC_DIR} to DESTINATION of HTMLs #1307 (lemniscati)</li> <li>Use INC_DIR for OPENJPEG_INCLUDE_DIRS (fixes uclouvain#1174) #1306 (matthew-sharp)</li> <li>pi.c: avoid out of bounds access with POC (fixes #1302) #1304 (rouault)</li> <li>Encoder: grow again buffer size #1303 (zodf0055980)</li> <li>opj_j2k_write_sod(): avoid potential heap buffer overflow (fixes #1299) (probably master only) #1301 (rouault)</li> <li>pi.c: avoid out of bounds access with POC (refs https://github.com/uclouvain/openjpeg/issues/1293#issuecomment-737122836) #1300 (rouault)</li> <li>opj_t2_encode_packet(): avoid out of bound access of #1297, but likely not the proper fix #1298 (rouault)</li> <li>opj_t2_encode_packet(): avoid out of bound access of #1294, but likely not the proper fix #1296 (rouault)</li> <li>opj_j2k_setup_encoder(): validate POC compno0 and compno1 (fixes #1293) #1295 (rouault)</li> <li>Encoder: avoid global buffer overflow on irreversible conversion when\u2026 #1292 (rouault)</li> <li>Decoding: deal with some SPOT6 images that have tiles with a single tile-part with TPsot == 0 and TNsot == 0, and with missing EOC #1291 (rouault)</li> <li>Free p_tcd_marker_info to avoid memory leak #1288 (zodf0055980)</li> <li>Encoder: grow again buffer size #1287 (zodf0055980)</li> <li>Encoder: avoid uint32 overflow when allocating memory for codestream buffer (fixes #1243) #1276 (rouault)</li> <li>Java compatibility from 1.5 to 1.6 #1263 (jiapei100)</li> <li>opj_decompress: fix double-free on input directory with mix of valid and invalid images #1262 (rouault)</li> <li>openjp2: Plug image leak when failing to allocate codestream index. #1260 (sebras)</li> <li>openjp2: Plug memory leak when setting data as TLS fails. #1258 (sebras)</li> <li>openjp2: Error out if failing to create Tier 1 handle. #1256 (sebras)</li> <li>Testing for invalid values of width, height, numcomps #1254 (szukw000)</li> <li>Single-threaded performance improvements in forward DWT for 5-3 and 9-7 (and other improvements) #1253 (rouault)</li> <li>Add support for multithreading in encoder #1248 (rouault)</li> <li>Add support for generation of PLT markers in encoder #1246 (rouault)</li> <li>Fix warnings about signed/unsigned casts in pi.c #1244 (rouault)</li> <li>opj_decompress: add sanity checks to avoid segfault in case of decoding error #1240 (rouault)</li> <li>ignore wrong icc #1236 (szukw000)</li> <li>Implement writing of IMF profiles #1235 (rouault)</li> <li>tests: add alternate checksums for libtiff 4.1 #1234 (rouault)</li> <li>opj_tcd_init_tile(): avoid integer overflow #1232 (rouault)</li> <li>tests/fuzzers: link fuzz binaries using $LIB_FUZZING_ENGINE. #1230 (Dor1s)</li> <li>opj_j2k_update_image_dimensions(): reject images whose coordinates are beyond INT_MAX (fixes #1228) #1229 (rouault)</li> <li>Fix resource leaks #1226 (dodys)</li> <li>abi-check.sh: fix false positive ABI error, and display output error log #1218 (rouault)</li> <li>pi.c: avoid integer overflow, resulting in later invalid access to memory in opj_t2_decode_packets() #1217 (rouault)</li> <li>Add check to validate SGcod/SPcoc/SPcod parameter values. #1211 (sebras)</li> <li>Fix buffer overflow reading an image file less than four characters #1196 (robert-ancell)</li> <li>compression: emit POC marker when only one single POC is requested (f\u2026 #1192 (rouault)</li> <li>Fix several potential vulnerabilities  #1185 (Young-X)</li> <li>openjp2/j2k: Report error if all wanted components are not decoded. #1164 (sebras)</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#v231-2019-04-02","title":"v2.3.1 (2019-04-02)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>v2.2.0 regression for decoding images where TNsot == 0 #1120</li> <li>Int overflow in jp3d #1162</li> <li>Heap buffer overflow in opj_j2k_update_image_data() triggered with Ghostscript #1157</li> <li>LINUX install doesn't work when building shared libraries is disabled #1155</li> <li>OPENJPEG null ptr dereference in openjpeg-2.3.0/src/bin/jp2/convert.c:2243 #1152</li> <li>How to drop certain subbands/layers in DWT #1147</li> <li>where is the MQ-Coder ouput stream in t2.c? #1146</li> <li>OpenJPEG 2.3 (and 2.2?) multi component image fails to decode with KDU v7.10 #1132</li> <li>Missing checks for header_info.height and header_info.width in function pnmtoimage in src/bin/jpwl/convert.c, which can lead to heap buffer overflow #1126</li> <li>Assertion Failure in jp2.c #1125</li> <li>Division-by-zero vulnerabilities in the function pi_next_pcrl, pi_next_cprl and pi_next_rpcl in src/lib/openjp3d/pi.c #1123</li> <li>Precinct switch (-c) doesn't right-shift last record to remaining resolution levels #1117</li> <li>Sample: encode J2K a data using streams??? #1114</li> <li>HIGH THROUGHPUT JPEG 2000 (HTJ2K) #1112</li> <li>How to build openjpeg for arm linux? #1108</li> <li>crash #1106</li> <li>JP2000 returning OPJ_CLRSPC_UNKNOWN color space #1103</li> <li>Compilation successful but install unsuccessful: Calling executables throws libraries missing error #1102</li> <li>fprintf format string requires 1 parameter but only 0 are given #1093</li> <li>fprintf format string requires 1 parameter but only 0 are given #1092</li> <li>sprintf buffer overflow #1088</li> <li>sprintf buffer overflow #1085</li> <li>Infinite loop when reading jp2 #1081</li> <li>missing format string parameter #1074</li> <li>Excessive Iteration in opj_t1_encode_cblks (src/lib/openjp2/t1.c) #1059</li> <li>Out-of-bound left shift in opj_j2k_setup_encoder (src/lib/openjp2/j2k.c) #1057</li> <li>Encode image on Unsplash #1054</li> <li>Integer overflow in opj_t1_encode_cblks (src/lib/openjp2/t1.c) #1053</li> <li>Signed Integer Overflow - 68065512 #1048</li> <li>Similar vulnerable functions related to CVE-2017-14041 #1044</li> <li>[ERROR] COD marker already read. No more than one COD marker per tile.  #1043</li> <li>failing to install latest version of openjpeg from source #1041</li> <li>Trouble compressing large raw image #1032</li> <li>Download and installed code from 2.3 archive. Installing 2.2? #1030</li> <li>missing fclose #1029</li> <li>NULL Pointer Access in function imagetopnm of convert.c(jp2):1289 #860</li> <li>NULL Pointer Access in function imagetopnm of convert.c:2226(jp2)  #859</li> <li>Heap Buffer Overflow in function imagetotga of convert.c(jp2):942 #858</li> </ul> <p>Merged pull requests:</p> <ul> <li>abi-check.sh: fix broken download URL #1188 (rouault)</li> <li>opj_t1_encode_cblks: fix UBSAN signed integer overflow #1187 (rouault)</li> <li>convertbmp: detect invalid file dimensions early (CVE-2018-6616) #1172 (hlef)</li> <li>color_apply_icc_profile: avoid potential heap buffer overflow #1170 (rouault)</li> <li>Fix multiple potential vulnerabilities and bugs #1168 (Young-X)</li> <li>Fix several memory and resource leaks #1163 (nforro)</li> <li>Fix some potential overflow issues #1161 (stweil)</li> <li>jp3d/jpwl convert: fix write stack buffer overflow #1160 (hlef)</li> <li>Int overflow fixed #1159 (ichlubna)</li> <li>Update knownfailures- files given current configurations #1149 (rouault)</li> <li>CVE-2018-5785: fix issues with zero bitmasks #1148 (hlef)</li> <li>openjp2/jp2: Fix two format strings #1143 (stweil)</li> <li>Changes in pnmtoimage if image data are missing #1141 (szukw000)</li> <li>Relative path to header files is hardcoded in OpenJPEGConfig.cmake.in file #1140 (bukatlib)</li> <li>Cast on uint ceildiv #1136 (reverson)</li> <li>Add -DBUILD_PKGCONFIG_FILES to install instructions #1133 (robe2)</li> <li>Fix some typos in code comments and documentation #1128 (stweil)</li> <li>Fix regression in reading files with TNsot == 0 (refs #1120) #1121 (rouault)</li> <li>Use local type declaration for POSIX standard type only for MS compiler #1119 (stweil)</li> <li>Fix Mac builds #1104 (rouault)</li> <li>jp3d: Replace sprintf() by snprintf() in volumetobin() #1101 (kbabioch)</li> <li>opj_mj2_extract: Rename output_location to output_prefix #1096 (kbabioch)</li> <li>mj2: Add missing variable to format string in fprintf() invocation in meta_out.c #1094 (kbabioch)</li> <li>Convert files to UTF-8 encoding #1090 (stweil)</li> <li>fix unchecked integer multiplication overflow #1080 (setharnold)</li> <li>Fixed typos #1062 (radarhere)</li> <li>Note that seek uses SEEK_SET behavior. #1055 (ideasman42)</li> <li>Some Doxygen tags are removed #1050 (szukw000)</li> <li>Fix resource leak (CID 179466) #1047 (stweil)</li> <li>Changed cmake version test to allow for cmake 2.8.11.x #1042 (radarhere)</li> <li>Add missing fclose() statement in error condition. #1037 (gfiumara)</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#v230-2017-10-04","title":"v2.3.0 (2017-10-04)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Sub-tile decoding: only decode precincts and codeblocks that intersect the window specified in opj_set_decode_area() #990 (rouault)</li> <li>Sub-tile decoding: only apply IDWT on areas that participate to the window of interest #1001 (rouault)</li> <li>Sub-tile decoding: memory use reduction and perf improvements #1010 (rouault)</li> <li>Add capability to decode only a subset of all components of an image. #1022 (rouault)</li> </ul> <p>Fixed bugs:</p> <ul> <li>Setting x offset of decode region to -1 causes opj_decompress to go into infinite loop #736</li> <li>Problem decoding multiple tiles with get_decoded_tile when cmap/pclr/cdef boxes are present in jp2 file #484</li> <li>set reduce_factor_may_fail #474</li> <li>opj_compress.exe, command line parser, infinite loop #469</li> <li>Various memory access issues found via fuzzing #448</li> <li>Multiple warnings when building OpenJPEG (trunk) #442</li> <li>Bulk fuzz-testing report #427</li> <li>remove all printf from openjpeg / use proper function pointer for logging #371</li> <li>minor changes, clean-up #349</li> <li>image-&gt;numcomps &gt; 4 #333</li> <li>Improve support for region of interest #39</li> <li>Public function to tell kernel type used (5x3 vs 9x7) #3</li> <li>elf binary in source package ?  #1026</li> <li>opj_cio_open #1025</li> <li>Building with Visual Studio 2015 #1023</li> <li>tcd.cpp&gt;:1617:33: error: assigning to 'OPJ_INT32 *' (aka 'int *') from incompatible type 'void *' #1021</li> <li>j2k.cpp &gt; comparison of address of 'p_j2k-&gt;m_cp.tcps[0].m_data' not equal to a null pointer is always true #1020</li> <li>Openjpeg 2.2.0 always build shared library even though -DBUILD_SHARED_LIBS:bool=off #1019</li> <li>missing fclose #1018</li> <li>Use opj_image_data_free instead of opj_free for image-&gt;comps[].data #1014</li> <li>malloc poison on some compilers - cross compiling #1013</li> <li>Add OPJ_VERSION_MAJOR, OPJ_VERSION_MINOR, OPJ_VERSION_MICRO macros in openjpeg.h #1011</li> <li>Encode: do not perform rate control for single-tile lossless #1009</li> <li>opj_set_decoded_resolution_factor(): bad interaction with opj_set_decode_area() and/or opj_decode() #1006</li> <li>memory allocation failure with .pgx file #999</li> <li>Unable to fuzz with raw image as input #998</li> <li>stack-based buffer overflow write in pgxtoimage (/convert.c) #997</li> <li>freeze with a crafted bmp #996</li> <li>invalid memory write in tgatoimage (convert.c) #995</li> <li>static build on Windows fails #994</li> <li>another heap-based buffer overflow in opj_t2_encode_packet (t2.c) #993</li> <li>heap-based buffer overflow in opj_t2_encode_packet (t2.c) #992</li> <li>heap-based buffer overflow in opj_write_bytes_LE (cio.c) (unfixed #985) #991</li> <li>heap overflow in opj_compress #988</li> <li>heap overflow in opj_decompress #987</li> <li>heap-based buffer overflow in opj_bio_byteout (bio.c) #986</li> <li>heap-based buffer overflow in opj_write_bytes_LE (cio.c) #985</li> <li>memory allocation failure in opj_aligned_alloc_n (opj_malloc.c) #983</li> <li>heap-base buffer overflow in opj_mqc_flush (mqc.c) #982</li> <li>Decode fails for JP2s with ICC profile #981</li> <li>Unit tests failing on Ubuntu 17.04 #916</li> <li>Encoder crashes on small images #901</li> <li>openjpeg-1.5.3 fails to compile #830</li> <li>opj_compress crops image (win) or creates a jp2 which cannot be decompressed (lin) #716</li> <li>-d flag is silently ignored when decoding a single tile #693</li> <li>transition away from dev-utils #628</li> <li>update instructions to build with Visual Studio and 64-Bit Visual C++ Toolset. #1028 (quangnh89)</li> <li>Add missing newline at end of file #1024 (stweil)</li> <li>merge master into coverity_scan to update coverity results #1008 (detonin)</li> <li>Use more const qualifiers #984 (stweil)</li> <li>Changes in converttif.c for PPC64 #980 (szukw000)</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#v220-2017-08-10","title":"v2.2.0 (2017-08-10)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Memory consumption reduction at decoding side #968 (rouault)</li> <li>T1 &amp; DWT multithreading decoding optimizations #786 (rouault)</li> <li>Tier1 decoder speed optimizations #783 (rouault)</li> <li>Inverse DWT 5x3: lift implementation / SSE accelerated version #953</li> <li>install static libraries #969 (jeroen)</li> <li>IDWT 5x3 single-pass lifting and SSE2/AVX2 implementation #957 (rouault)</li> <li>build both shared and static library #954 (jeroen)</li> <li>T1 flag optimizations (#172) #945 (rouault)</li> <li>CMake: add stronger warnings for openjp2 lib/bin by default, and error out on declaration-after-statement #936 (rouault)</li> <li>Quiet mode for opj_decompress via -quiet long parameter. #928 (RussellMcOrmond)</li> <li>Implement predictive termination check #800 (rouault)</li> </ul> <p>Fixed bugs:</p> <ul> <li>Several issues spotted by Google OSS Fuzz - see here</li> <li>Missing fclose #976</li> <li>Heap buffer overflow read in openjpeg imagetopnm #970</li> <li>opj_decompress opj_j2k_update_image_data() Segment falut #948</li> <li>Generic Crash in 1.5.0 #941</li> <li>Segmentation Faults #940</li> <li>Assertions thrown #939</li> <li>Floating Point Errors #938</li> <li>Division by zero crash #937</li> <li>malformed jp2 can cause heap-buffer-overflow  #909</li> <li>NULL dereference can cause by malformed file #908</li> <li>Out of bound read in opj_j2k_add_mct #907</li> <li>Check bpno_plus_one in opj_t1_decode_cblk #903</li> <li>Undefined-shift in opj_j2k_read_siz #902</li> <li>opj_compress v2.1.2 can create images opj_decompress cannot read #891</li> <li>Improper usage of opj_int_ceildiv can cause overflows #889</li> <li>Undefined shift in opj_get_all_encoding_parameters #885</li> <li>Denial of service (crash) due to use-after-free when decoding an illegal JPEG2000 image file v2.1.2 (2017-04 #880</li> <li>Denial of service (crash) when decoding an illegal JPEG2000 image file v2.1.2 (2017-03) #879</li> <li>bug png 2 j2k #868</li> <li>Inconsistent compression using cinema settings on folder of non-compliant image #864</li> <li>Openjpeg-2.1.2 Heap Buffer Overflow Vulnerability due to Insufficient check #862</li> <li>Heap Buffer Overflow in function pnmtoimage of convert.c #861</li> <li>CVE-2016-9112 FPE(Floating Point Exception) in lib/openjp2/pi.c:523 #855</li> <li>CVE-2016-5139, CVE-2016-5152, CVE-2016-5158, CVE-2016-5159 #854</li> <li>Undefined Reference error #853</li> <li>opj_compress with lossy compression results in strange pixel values #851</li> <li>CVE-2016-1626 and CVE-2016-1628 #850</li> <li>Out-of-Bounds Write in opj_mqc_byteout of mqc.c #835</li> <li>WARNING in tgt_create tree-&gt;numnodes == 0, no tree created. #794</li> <li>Potential overflow when precision is larger than 32 #781</li> <li>division-by-zero in function opj_pi_next_rpcl of pi.c (line 366) #780</li> <li>division-by-zero in function opj_pi_next_rpcl of pi.c (line 363) #779</li> <li>division-by-zero in function opj_pi_next_pcrl of pi.c (line 447) #778</li> <li>division-by-zero in function opj_pi_next_pcrl of pi.c (line 444) #777</li> <li>Encoding the following file with 32x32 tiling produces jp2 image with artifact #737</li> <li>division-by-zero (SIGFPE) error in opj_pi_next_cprl function (line 526 of pi.c) #732</li> <li>division-by-zero (SIGFPE) error in opj_pi_next_cprl function (line 523 of pi.c) #731</li> <li>OpenJpeg 2.1 and 1.4 fails to decompress this file correctly #721</li> <li>MQ Encode :uninitialized memory access when first pass does not output any bytes #709</li> <li>Out-of-bounds read in opj_j2k_update_image_data and opj_tgt_reset function #704</li> <li>Remove opj_aligned_malloc / opj_aligned_realloc / opj_aligned_free? #689</li> <li>There is an issue with rendering some type of jpeg file. Please ref the link. #672</li> <li>Null Dereference in tcd_malloc_decode_tile #657</li> <li>ETS-C1P0-p0_12.j2k-compare2ref &amp; NR-C1P0-p0_12.j2k-compare2base failing under windows #655</li> <li>Memory leak #631</li> <li>Test 481 reports error in valgrind memcheck #612</li> <li>reserved identifier violation #587</li> <li>Buffer overflow when compressing some 16 bits images of the test suite #539</li> <li>Heap-buffer-overflow in opj_dwt_decode_1 #480</li> <li>Automated fuzz testing #468</li> <li>Expected to find EPH marker  #425</li> <li>read: segment too long (6182) with max (35872) for codeblock 0 (p=19, b=2, r=5, c=1) #284</li> <li>building 64bit version has lots of warnings #244</li> <li>Wrong encoding of small tiles with high level number #239</li> <li>Errors raised in pi.c by VS11 analyzer  #190</li> <li>Undocumented optimization found in v2 branch of openjpeg #183</li> <li>T1 optimisations jpeg2000 #172</li> <li>Remove OPJ_NOSANITIZE in opj_bio_read() and opj_bio_write() (#761) #955 (rouault)</li> <li>Fix bypass pterm termall and lossless decomposition issue (#891, #892) #949 (rouault)</li> <li>Escape quotes to ensure README renders on GitHub correctly #914 (alexwlchan)</li> <li>Remove spurious .R macros from manpages #899 (jwilk)</li> <li>Remove warnings related to empty tag-trees. #893 (rouault)</li> </ul> <p>Maintenance-related tasks:</p> <ul> <li>Submit OpenJPEG to oss-fuzz #965</li> <li>Updates for Doxygen to suppress warnings #849</li> <li>Remove useless knownfailures (since LAZY encoding is fixed) #964 (rouault)</li> <li>Enable AVX2 at runtime on Travis-CI and AppVeyor #963 (rouault)</li> <li>Tests: test opj_compress in VSC mode (related to #172) #935 (rouault)</li> <li>Reformat: apply reformattin on .h files (#128) #926 (rouault)</li> <li>Add mechanisms to reformat and check code style, and reformat whole codebase (#128) #919 (rouault)</li> <li>Add profiling of CPU and memory usage (#912) #918 (rouault)</li> <li>Add performance benchmarking scripts #917 (rouault)</li> <li>Fix retrieval of jpylyzer in AppVeyor #915 (rouault)</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#v212-2016-09-28","title":"v2.1.2 (2016-09-28)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>null ptr dereference in convert.c:1331 #843</li> <li>Out-of-Bounds Read in function bmp24toimage of convertbmp.c #833</li> <li>Disable automatic compilation of t1_generate_luts in CMakeLists.txt #831</li> <li>CVE-2016-7163 Integer overflow in opj_pi_create_decode #826</li> <li>Security Advisory for OpenJPEG #810</li> <li>Add dashboard with static lib #804</li> <li>hidden visibility for the static library / building with -DOPJ_STATIC against shared lib #802</li> <li>Optimization when building library from source #799</li> <li>unsigned int16 on Solaris 11.2/sparc #796</li> <li>appveyor #793</li> <li>FFMpeg will not link to 2.1.1 release built as shared library #766</li> <li>API change since v2: opj_event_mgr_t not available #754</li> <li>openjpeg.h needs dependencies #673</li> <li>\"master\" does not build on ubuntu #658</li> <li>Package 'openjp2', required by 'libopenjpip', not found #594</li> </ul> <p>Merged pull requests:</p> <ul> <li>Fix PNM file reading #847 (mayeut)</li> <li>Fix some issues reported by Coverity Scan #846 (stweil)</li> <li>Fix potential out-of-bounds read (coverity)  #844 (stweil)</li> <li>Remove TODO for overflow check #842 (mayeut)</li> <li>Add overflow checks for opj_aligned_malloc #841 (mayeut)</li> <li>Flags in T1 shall be unsigned #840 (mayeut)</li> <li>Fix some warnings #838 (mayeut)</li> <li>Fix issue 833. #834 (trylab)</li> <li>Add overflow checks for opj_aligned_malloc #832 (mayeut)</li> <li>Add test for issue 820 #829 (mayeut)</li> <li>Add test for issue 826 #827 (mayeut)</li> <li>Fix coverity 113065 (CWE-484) #824 (mayeut)</li> <li>Add sanity check for tile coordinates #823 (mayeut)</li> <li>Add test for PR 818 #822 (mayeut)</li> <li>Update to libpng 1.6.25 #821 (mayeut)</li> <li>CVE-2016-8332: fix incrementing of \"l_tcp-&gt;m_nb_mcc_records\" in opj_j2k_read_mcc #820 (mayeut)</li> <li>Add overflow check in opj_tcd_init_tile #819 (mayeut)</li> <li>Fix leak &amp; invalid behavior of opj_jp2_read_ihdr #818 (mayeut)</li> <li>Add overflow check in opj_j2k_update_image_data #817 (mayeut)</li> <li>Change 'restrict' define to 'OPJ_RESTRICT' #816 (mayeut)</li> <li>Switch to clang 3.8 #814 (mayeut)</li> <li>Fix an integer overflow issue #809 (trylab)</li> <li>Update to lcms 2.8 #808 (mayeut)</li> <li>Update to libpng 1.6.24 #807 (mayeut)</li> <li>Reenable clang-3.9 build on travis #806 (mayeut)</li> <li>Bit fields type #805 (smuehlst)</li> <li>Add compilation test for standalone inclusion of openjpeg.h #798 (mayeut)</li> <li>jpwl: Remove non-portable data type u_int16_t (fix issue #796) #797 (stweil)</li> <li>Fix dependency for pkg-config (issue #594) #795 (stweil)</li> <li>Add .gitignore #787 (stweil)</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#v211-2016-07-05","title":"v2.1.1 (2016-07-05)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>opj_malloc replacement #625</li> <li>backport \"-p\" and \"-force-rgb\" options in 1.5 #606</li> <li>Use travis-ci matrix build #581</li> <li>Add Coverity Scan analysis #580</li> <li>Unnecessary rate distortion calculations  #479</li> <li>Add images from various security issues to test suite #415</li> <li>Coding speed for 9/7 on 32bits platforms (x86/ARM) can be improved with a quick fix #220</li> </ul> <p>Fixed bugs:</p> <ul> <li>Out-of-Bounds Access in function opj_tgt_reset of tgt.c #775</li> <li>Heap Buffer Overflow in function color_cmyk_to_rgb of color.c #774</li> <li>division-by-zero (SIGFPE) error in opj_tcd_init_tile function (line 730 of tcd.c) #733</li> <li>Out-Of-Bounds Read in sycc422_to_rgb function #726</li> <li>Heap Corruption in opj_free function #725</li> <li>Out-Of-Bounds Read in opj_tcd_free_tile function #724</li> <li>Cannot handle box of undefined size #653</li> <li>Compilation fails without platform-supplied aligned malloc #642</li> <li>HP compiler warns about redeclaration of static function #640</li> <li>Implementation-defined behavior of malloc causes different behavior on Linux and AIX #635</li> <li>Build on AIX fails because \"opj_includes.h\" is included after system headers #633</li> <li>Compiling with SSE2 on Linux 32-bit causes crashes in OpenJPEG #624</li> <li>Build on AIX fails because of \"restrict\" pointers #620</li> <li>bug in new tif conversion code #609</li> <li>bin/jp2/convert.c line 1085 Resource leak #607</li> <li>bin/jp2/convert.c  memory leak #601</li> <li>Resource leak in opj_j2k_create_cstr_index in case of failure #599</li> <li>Resource leak in opj_j2k_encode in case of failure #598</li> <li>Resource leak in opj_j2k_decode_one_tile in case of failure #597</li> <li>Resource Leak #573</li> <li>opj_compress fails to compress lossless on gcc/x86 (-m32) #571</li> <li>Use-after-free in opj_j2k_write_mco #563</li> <li>openjpeg-master-2015-07-30 failed to compile on LINUX #556</li> <li>PNG images are always read as RGB(A) images #536</li> <li>g4_colr.j2c not handled properly #532</li> <li>Bigendian: opj_compress + opj_decompress fails #518</li> <li>Suspicious code in j2k.c #517</li> <li>Decode times almost double(!!) on Visual Studio 2013, 2015 #505</li> <li>opj_data/input/nonregression/issue226.j2k #500</li> <li>opj_setup_encoder always returns true #497</li> <li>Double free in j2k_read_ppm_v3 parsing ((presumably invalid) image. #496</li> <li>Invalid write in opj_j2k_update_image_data #495</li> <li>Undefined printf format specifier %ud used in code #494</li> <li>Potential double free on malloc failure in opj_j2k_copy_default_tcp_and_create_tcp() #492</li> <li>Do not link with -ffast-math #488</li> <li>Heap-buffer-overflow in opj_dwt_decode #486</li> <li>opj_dump fails on Windows 7, 64 bits #482</li> <li>SIGSEGV in opj_j2k_update_image_data via pdfium_test #481</li> <li>Heap-buffer-overflow in opj_j2k_tcp_destroy #477</li> <li>Invalid image causes write past end of heap buffer #476</li> <li>Assertion `l_res-&gt;x0 &gt;= 0' fails when parsing invalid images  #475</li> <li>Bug on opj_write_bytes_BE function  #472</li> <li>Refactor j2k_read_ppm_v3 function #470</li> <li>compression: strange precinct dimensions #466</li> <li>(:- Console message in opj_decompress -:) #465</li> <li>opj_decompress fails to decompress any files #463</li> <li>bio-&gt;ct is unnecessarily set to zero in opj_bio_flush method #461</li> <li>Maximal unsigned short is 65535, not 65536 #460</li> <li>OpenJpeg fails to encode components with different precision properly #459</li> <li>component precision upscaling isn't correct in opj_decompress #458</li> <li>Multiple precision components won't get encoded to jp2 if 1 component is unsigned 1 bit #457</li> <li>Incorrect code in ../bin/jp2/convert.c,  function rawtoimage_common(...) #456</li> <li>[OpenJPEG-trunk] opj_stream_get_number_byte_left throws assert #455</li> <li>NR-DEC-kodak_2layers_lrcp.j2c-31-decode-md5 fails randomly when running tests in parallel #454</li> <li>compare_raw_files doesn't report an error on invalid arguments / missing input files #453</li> <li>Forward discrete wavelet transform: implement periodic symmetric extension at boundaries #452</li> <li>Bug in tiff reading method in convert.c #449</li> <li>Image in pdf don't display #447</li> <li>Multiple issues causing opj_decompress to segfault #446</li> <li>opj_compress: 40% of encode time is spent freeing data #445</li> <li>Multiple warnings when configuring OpenJPEG on MacOS with CMake 3.x (trunk) #443</li> <li>valgrind memleak found #437</li> <li>global-buffer-overflow src/lib/openjp2/t1.c:1146 opj_t1_getwmsedec #436</li> <li>Warning introduced on trunk r2923 &amp; r2924 #435</li> <li>heap-buffer-overflow in opj_t1_decode_cblks #432</li> <li>Heap-buffer-overflow in opj_tcd_init_decode_tile #431</li> <li>Heap-buffer-overflow in opj_j2k_tcp_destroy #430</li> <li>Heap-buffer-overflow in opj_jp2_apply_pclr #429</li> <li>issue412 revisited #428</li> <li>Image distorted (sides look cankered) #423</li> <li>openjpeg-2.x-trunk-r2918 is broken in color.c #422</li> <li>Heap-buffer-overflow in opj_tcd_init_decode_tile #420</li> <li>Heap-use-after-free in opj_t1_decode_cblks #418</li> <li>UNKNOWN in opj_read_bytes_LE #417</li> <li>Transparency problem #416</li> <li>Image with per channel alpha (cdef) does not decode properly #414</li> <li>OpenJPEG crashes with attached image #413</li> <li>Palette image with cdef fails to decompress #412</li> <li>Invalid member values from opj_read_header or opj_decode ? #411</li> <li>MD5 Checksum hangs under valgrind on MacOS X #410</li> <li>Heap-buffer-overflow in opj_tcd_get_decoded_tile_size #408</li> <li>C++ style comments in trunk/src/lib/openjp2/j2k.c #407</li> <li>Backport bugfixes from trunk to 2.1 branch #405</li> <li>Heap-buffer-overflow in parse_cmdline_encoder #403</li> <li>Heap-buffer-overflow in opj_v4dwt_interleave_h #400</li> <li>Heap-buffer-overflow in opj_dwt_decode #399</li> <li>Heap-use-after-free in opj_t1_decode_cblks #398</li> <li>Heap-buffer-overflow in opj_jp2_apply_cdef #397</li> <li>Heap-buffer-overflow in opj_t2_read_packet_header #396</li> <li>Heap-buffer-overflow in opj_t2_read_packet_header #395</li> <li>Heap-buffer-overflow in opj_dwt_decode_1 #394</li> <li>Heap-double-free in j2k_read_ppm_v3 #393</li> <li>Security hole in j2k.c #392</li> <li>Security: double-free in opj_tcd_code_block_dec_deallocate #391</li> <li>check for negative-size params in code #390</li> <li>Heap-buffer-overflow in opj_t2_read_packet_header #389</li> <li>Heap overflow in OpenJpeg 1.5.2 #388</li> <li>openjpip.so.6 file too short #387</li> <li>Corrupted JP3D file #386</li> <li>variable assigned to itself #383</li> <li>Null pointer dereferencing #382</li> <li>bad use of case statement #381</li> <li>Release 2.1 as a Ubuntu package #380</li> <li>Bug in libopenjpwl.pc #374</li> <li>inconsistent tile numbering in decode output message #370</li> <li>error in code block calculations #369</li> <li>r2872 fails to compile due to \"attempt to use poisoned malloc\" error in j2k.c #368</li> <li>OSX build gives libopenjp2.6.dylib with not-absolute install name id  #367</li> <li>opj_decompress gives error but successfully decompress in OPJ 2.1 #366</li> <li>pngtoimage() and imagetopng() have wrong byte order for 16-Bit image #365</li> <li>PDF crash in chrome - part2 (due to attachment limit) #364</li> <li>PDF crash in chrome - part1 #363</li> <li>PDF crash in chrome - part0 #362</li> <li>Compilation fails on Windows with mingw32 gcc4.8 #361</li> <li>security issue #360</li> <li>improve memory management #359</li> <li>how to compress a yuv420 raw data using opj_compress #357</li> <li>Some memory allocation are not checked #355</li> <li>Static library symbols shall be marked as hidden #354</li> <li>opj_compress rejects valid bmp files #353</li> <li>opj_compress crashes when number of resolutions is set to zero #352</li> <li>Compilation error under Visual Studio 2003 #351</li> <li>opj_compress description example error [Low priority] #350</li> <li>opj_write_bytes_BE is wrong in trunk #345</li> <li>PART1ONLY option in release.sh doesn't work properly #332</li> <li>openjpeg crash error #330</li> <li>openjpeg decompress error #329</li> <li>openjpeg decompress issue #326</li> <li>limited tif support #322</li> <li>asoc value of  65536 is allowed #321</li> <li>opj_skip_from_file error #314</li> <li>Heavy quota usage in openjpeg #309</li> <li>Verify -help actually match letter #307</li> <li>g3_colr.j2c not handled #288</li> <li>reopen/fix issue 165 #280</li> <li>kakadu conformance tests #279</li> <li>missing break after case statement in opj_dwt_decode_real  #274</li> <li>Run Coverity on trunk #270</li> <li>NR-ENC-random-issue-0005.tif-12-encode #259</li> <li>Use new add_test signature to handle cross compilation #258</li> <li>Loss decoding quality in 2.0.0 #254</li> <li>Decompress that worked in 1.5.1 fails in 2.0 #252</li> <li>Expected endianness with raw input is not documented leading to SEGFAULT #251</li> <li>OpenJPEG writes to stderr #246</li> <li>Inconsistent logging of tile index #245</li> <li>patch for openjpeg-trunk-r2347 and BIG_ENDIAN #242</li> <li>CMAP: MTYP == 0 (direct use) not handled properly #235</li> <li>Black Pixel #233</li> <li>opj_compress runtime error after fresh Linux install due to apparent failure to execute ldconfig #219</li> <li>openjp2 debug works, release build does not #217</li> <li>openjpeg-branch15-r2299 and openjpeg-trunk-r2299 fail to decode a JP2 file #212</li> <li>openjpeg-trunk issue with Win7 #201</li> <li>undefined reference to `opj_version' #200</li> <li>In tgt.c we used fprintf not the openjpeg message reporter #184</li> <li>Windows binaries not working under WinXP #176</li> <li>add ability to use intel ipp (performance primitive) within OpenJPEG #164</li> <li>Migration guide v2 #160</li> <li>Cannot decompress JPEG2000Aware3.18.7.3Win32_kdutranscode6.3.1.j2k #158</li> <li>Cannot decompress JPEG2000Aware3.18.7.3Win32.j2k #157</li> <li>openjpeg@googlegroups.com has disappeared #153</li> <li>OpenJPEG 1.5.0 crashes on a ridiculously big file... #151</li> <li>opj_image vs free #146</li> <li>Windows .dll file invalid #140</li> <li>Problem with second layer of a 2 layer coded LRCP (with precincts) #135</li> <li>version 1.4 crashes when opening PDF file with JPEG2000 images #133</li> <li>Setup a win64 dashboard #132</li> <li>J2KP4files/codestreams_profile0/p0_13.j2k question jpeg2000 #131</li> <li>Out of memory: Kill process 11204 (opj_server) score 917 or sacrifice child #123</li> <li>FILE* in opj API is unsafe #120</li> <li>third-party lib order #119</li> <li>openjpeg-1.5.0-Darwin-powerpc.dmg is huge ! #113</li> <li>misleading info in JP2 box lead to wrong number of components #110</li> <li>Image_to_j2k says that j2k files is generated but no file is on the HDD #109</li> <li>Error in openjpegV1.4 on compiling image_to_j2k: crash on reading bmp file #108</li> <li>Update to abi-compliance-checker 1.96 #106</li> <li>Decode error on the attached JPEG...works in KDU and with JASPER...please help! #101</li> <li>Mac binaries v1.4 is broken #95</li> <li>jp2_read_boxhdr() has size bug in version 1 #92</li> <li>Support for Java JAI Imageio #90</li> <li>encoding test failing #86</li> <li>source archive on demand #85</li> <li>CMakeLists.txt and Makefile.am for JPIP are buggy #84</li> <li>pclr-cmap-cdef #82</li> <li>Error when compiling openjpeg_v1_4_sources_r697 #79</li> <li>J2K codec issue on Windows Mobile  #77</li> <li>image_to_j2k.exe crashes on large .bmp file #75</li> <li>fatal error C1900 building the project on windows #65</li> <li>same option but different size #54</li> <li>Missing openjpegConfigure.h #38</li> <li>Not an issue in openjpeg, but ... #37</li> <li>OpenJPEG-1.3.0 pclr, cmap and cdef #27</li> <li>realloc maybe too big (t2.c) #26</li> <li>libopenjpeg/opj_malloc.h breaks on FreeBSD/Darwin systems #20</li> <li>image_to_j2k not outputting to win32 console properly #18</li> <li>[OpenJPEG] OpenJPEG_v13: tiled image part 2 #17</li> <li>JP2 Color Space modification by Matteo Italia #13</li> <li>Patch submission ( exotic video formats, and a few things )  #12</li> <li>16 bits lossy compression #10</li> <li>pnm file formats not accepting bitdepth greater than 8 bpp #8</li> <li>Heap corruption in j2k encoder #5</li> <li>JPWL crash in marker reallocation(+patch), segfault while decoding image with main header protection #4</li> <li>a couple of small errors in libopenjpeg detected by coverity #1</li> </ul> <p>Closed issues:</p> <ul> <li>Shared library build broken on ubuntu #728</li> <li>opj_includes.h shouldn't define <code>\\_\\_attribute\\_\\_</code> #727</li> <li>Possible website problems due to Jekyll upgrade #713</li> <li>Stable Release? #712</li> <li>Meta Issue : try to fix some of these critical bugs before thinking about optimizing the library #710</li> <li>Tiled encoding broken for images with non power of 2 dimensions #702</li> <li>install_name (still) not set on OS X #700</li> <li>Add section in wiki describing where one can get test images #699</li> <li>Make EvenManager into singleton #698</li> <li>Remove old branches from repo #696</li> <li>MQ Coder encode: Conditional jump or move depends on uninitialised value(s) #695</li> <li>Can we add these files to our test suite ? #688</li> <li>-t and -d command line flags for decode are not documented on OpenJPEG website #685</li> <li>Decoding at the precinct level #676</li> <li>Support unscaled 10 bit data for 2K cinema @ 48 FPS, as per DCI standard #671</li> <li>Use parallel jobs in ctest #664</li> <li>[Security]Multiple Memory error #663</li> <li>lossy encoding a 16 bit TIF file : severe artifacts in decompressed image #660</li> <li>opj_compress and opj_decompress : get_next_file method uses hard-coded unix path separator #630</li> <li>Uninitialized variable #629</li> <li>Use of enum variable for bit flags prevents compilation as C++ source #619</li> <li>Serious problem with quantization during lossy encoding #615</li> <li>Decompression does not work with sequential data source #613</li> <li>potential overflow in opj_tcd_tile_t #605</li> <li>Logical condition #596</li> <li>file9.jp2 does not dump correctly on 1.5 #595</li> <li>opj_compress man page is missing documentation of -jpip option #593</li> <li>opj_compress fails to compress lossless on gcc/x86 (-m32) in 1.5 branch #591</li> <li>Example: opj_compress -i image.j2k -o image.pgm #577</li> <li>Mismatching delete #575</li> <li>Compilation fails on Win7 #546</li> <li>NR-JP2-file5.jp2-compare2base fails with third party libcms #540</li> <li>CTest spits out an error at the end of the test run #516</li> <li>opj_uint_adds()  is questionable #515</li> <li>Might consider renaming this method: #491</li> <li>opj_compress run twice gives different fiile sizes for same file #490</li> <li>Android Support #483</li> <li>Add SSE2/SSE41 implementations for mct.c #451</li> <li>Reduce encoder code block memory usage for non 64x64 code block sizes #444</li> <li>valgrind \"Uninitialized Memory Read\" &amp; \"Uninitialized Memory Conditional\" found  #438</li> <li>No way to debug opj_tcd_init_encode_tile or opj_tcd_init_decode_tile #433</li> <li>Add option to call dsymutil on built binaries #409</li> <li>Allow opj_compress and opj_decompress to read/write images over stdin/stdout #379</li> <li>reduce memory significantly for single tile RGB encoding #375</li> <li>Switch code repo to github and start using pull request workflow #373</li> <li>This is a BigTIFF file.  This format not supported #125</li> <li>Add a test suite to check the convert functions #99</li> <li>Add build config to the dashboard to verify the autotools build #88</li> </ul> <p>Merged pull requests:</p> <ul> <li>Correct abi-check.sh for PR #791 (mayeut)</li> <li>Update tcd.c #790 (maddin200)</li> <li>Update lcms2 #773 (mayeut)</li> <li>Use lowercase for cmake commands consistently #769 (julienmalik)</li> <li>Ignore clang's summary warning #768 (julienmalik)</li> <li>Fix UBSan gcc warning for first arg to memset non null #767 (julienmalik)</li> <li>Update to libtiff-4.0.6 #764 (mayeut)</li> <li>Fix warnings #763 (mayeut)</li> <li>Check SSIZ is valid in opj_j2k_read_siz #762 (mayeut)</li> <li>Fix unsigned int overflow reported by UBSan #761 (mayeut)</li> <li>Fix unsigned int overflow reported by UBSan #759 (mayeut)</li> <li>Fix negative shift left reported by UBSan #758 (mayeut)</li> <li>Fix negative shift left reported by UBSan #757 (mayeut)</li> <li>Add clang 3.9 build to Travis matrix #753 (julienmalik)</li> <li>Fix implicit floating bool conversion #752 (julienmalik)</li> <li>Do not define __attribute__ in opj_includes.h #751 (mayeut)</li> <li>Allow to read/write 3/5/7/9/11/13/15 bpp TIF files #750 (mayeut)</li> <li>Fix heap-buffer-overflow in color_esycc_to_rgb #748 (mayeut)</li> <li>update libpng to from 1.6.17 to 1.6.21 #747 (julienmalik)</li> <li>Update cmake &amp; jpylyzer for travis builds #746 (julienmalik)</li> <li>Fix Out-Of-Bounds Read in sycc42x_to_rgb function #745 (mayeut)</li> <li>cppcheck fix for openjp2 #740 (julienmalik)</li> <li>Fix uninitialized variable reported by cppcheck #735 (julienmalik)</li> <li>Remove dead code in opj_dump #734 (julienmalik)</li> <li>issue #695 MQ Encode: ensure that bp pointer never points to uninitialized memory #708 (boxerab)</li> <li>Fix issue 135 #706 (mayeut)</li> <li>Fix implementation of opj_calloc #705 (stweil)</li> <li>[git/2.1 regression] Fix opj_write_tile() failure when numresolutions=1 #690 (rouault)</li> <li>Fix fatal crash on 64 bit Linux #687 (stweil)</li> <li>[libtiff] Add missing include statement for ssize_t #686 (mayeut)</li> <li>Fix duplicate article in comments #684 (stweil)</li> <li>Fix grammar in comment #679 (stweil)</li> <li>Remove whitespace and CR at line endings #678 (stweil)</li> <li>Fix typos #665 (jwilk)</li> <li>Add missing source for the JPIP library and executables (issue #658) #659 (stweil)</li> <li>Fix undefined size jp2 box handling #654 (mayeut)</li> <li>opj_decompress: Update error message #651 (stweil)</li> <li>Fix support of posix_memalloc for Linux #648 (stweil)</li> <li>Fix typo in comments #647 (stweil)</li> <li>Avoid pointer arithmetic with (void *) pointers #644 (smuehlst)</li> <li>Fix HP compiler warning about redeclaration of function (#640) #641 (smuehlst)</li> <li>Fix format strings and unneeded assignment #638 (stweil)</li> <li>Fix repository for JPEG2000 test data #637 (stweil)</li> <li>Update allocation functions #636 (mayeut)</li> <li>Fix OpenJPEG GitHub issue #633. #634 (smuehlst)</li> <li>travis-ci: Include add ons in matrix #632 (mayeut)</li> <li>Add Appveyor #627 (mayeut)</li> <li>Use Travis-ci to run ABI check #626 (mayeut)</li> <li>Fix warnings for C++ #623 (stweil)</li> <li>Fixed problem that C++ compilation failed because of enum variable. #622 (smuehlst)</li> <li>Added missing casts for return values of opj_malloc()/opj_calloc(). #618 (smuehlst)</li> <li>Add check for seek support before trying TPsot==TNsot workaround #617 (mayeut)</li> <li>Fix some typos found by codespell #610 (stweil)</li> <li>Correct leak in color_cielab_to_rgb #590 (mayeut)</li> <li>Add Travis-ci build matrix #584 (mayeut)</li> <li>Correct lossless issue on linux x86 #579 (mayeut)</li> <li>Travis-ci update #578 (mayeut)</li> <li>Correct CMake version requirements #572 (mayeut)</li> <li>Add tests for CMYK/esYCC/CIELab #567 (mayeut)</li> <li>Add support for CIELab, EYCC and CMYK #559 (szukw000)</li> <li>Remove printf/fprintf to stdout/stderr throughout openjp2 lib #558 (mayeut)</li> <li>better -ffast-math handling #555 (rdieter)</li> <li>Add jpylyzer tests for JP2 compression #552 (mayeut)</li> <li>Add COC/QCC in main header when needed #551 (mayeut)</li> <li>Use __emul under msvc x86 for fast 64 = 32 * 32 #550 (mayeut)</li> <li>Update convert for PNG output #549 (mayeut)</li> <li>Remove some warnings when building #548 (mayeut)</li> <li>Switch to libpng-1.6.17 #547 (mayeut)</li> <li>Add some missing static keywords #545 (mayeut)</li> <li>Switch to libcms2  mm2/Little-CMS@0e8234e090d6aab33f90e2eb0296f30aa0705e57 #544 (mayeut)</li> <li>Prevent overflow when coding 16 bits images #543 (mayeut)</li> <li>Switch to libcms2-2.6 #542 (mayeut)</li> <li>Update PNG support #538 (mayeut)</li> <li>Various Minor fixes #537 (mayeut)</li> <li>Update TIFF conversion to support more bit depth. #535 (mayeut)</li> <li>Add checks for odd looking cmap &amp; for cmap outside jp2h box #534 (mayeut)</li> <li>Refactor opj_j2k_read_ppm &amp; opj_j2k_read_ppt #533 (mayeut)</li> <li>Add option to force component splitting in imagetopnm #531 (mayeut)</li> <li>fix Suspicious code in j2k.c #517 #529 (renevanderark)</li> <li>Update zlib to version 1.2.8 #528 (mayeut)</li> <li>Fix opj_write_bytes_BE (#518) #521 (manisandro)</li> <li>Correctly decode files with incorrect tile-part header fields (TPsot==TNsot) #514 (mayeut)</li> <li>Fixed typos #510 (radarhere)</li> <li>Formatted the readme file #507 (htmfilho)</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version21-2014-04-29","title":"version.2.1 (2014-04-29)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version201-2014-04-22","title":"version.2.0.1 (2014-04-22)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version152-2014-03-28","title":"version.1.5.2 (2014-03-28)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version20-2014-03-28","title":"version.2.0 (2014-03-28)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version151-2012-09-13","title":"version.1.5.1 (2012-09-13)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version15-2012-02-07","title":"version.1.5 (2012-02-07)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version13-2011-07-03","title":"version.1.3 (2011-07-03)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version14-2011-07-03","title":"version.1.4 (2011-07-03)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version12-2007-06-04","title":"version.1.2 (2007-06-04)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/CHANGELOG/#version11-2007-01-31","title":"version.1.1 (2007-01-31)","text":"<p>List of fixed issues and enhancements unavailable, see NEWS or Full Changelog</p> <p>* This Change Log was automatically generated by github_changelog_generator</p>"},{"location":"opencv_docs/opencv/3rdparty/openjpeg/THANKS/","title":"OpenJPEG THANKS file","text":"<p>Many people have contributed to OpenJPEG by reporting problems, suggesting various improvements, or submitting actual code. Here is a list of these people. Help me keep it complete and exempt of errors.</p> <ul> <li>Giuseppe Baruffa</li> <li>Ben Boeckel</li> <li>Aaron Boxer</li> <li>David Burken</li> <li>Matthieu Darbois</li> <li>Rex Dieter</li> <li>Herve Drolon</li> <li>Antonin Descampe</li> <li>Francois-Olivier Devaux</li> <li>Parvatha Elangovan</li> <li>Jer\u00f4me Fimes</li> <li>Bob Friesenhahn</li> <li>Kaori Hagihara</li> <li>Luc Hermitte</li> <li>Luis Ibanez</li> <li>David Janssens</li> <li>Hans Johnson</li> <li>Callum Lerwick</li> <li>Ke Liu (Tencent's Xuanwu LAB)</li> <li>Sebastien Lugan</li> <li>Benoit Macq</li> <li>Mathieu Malaterre</li> <li>Julien Malik</li> <li>Arnaud Maye</li> <li>Vincent Nicolas</li> <li>Aleksander Nikolic (Cisco Talos)</li> <li>Glenn Pearson</li> <li>Even Rouault</li> <li>Dzonatas Sol</li> <li>Winfried Szukalski</li> <li>Vincent Torri</li> <li>Yannick Verschueren</li> <li>Peter Wimmer</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openvx/","title":"C++ wrappers for OpenVX-1.x C API","text":""},{"location":"opencv_docs/opencv/3rdparty/openvx/#core-ideas","title":"Core ideas:","text":"<ul> <li>lightweight - minimal overhead vs standard C API</li> <li>automatic references counting</li> <li>exceptions instead of return codes</li> <li>object-oriented design</li> <li>(NYI) helpers for user-defined kernels &amp; nodes</li> <li>C++ 11 friendly</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/openvx/#quick-start-sample","title":"Quick start sample","text":"<p>The following short sample gives basic knowledge on the wrappers usage:</p> <pre><code>#include \"ivx.hpp\"\n#include \"ivx_lib_debug.hpp\" // ivx::debug::*\n\nint main()\n{\n    vx_uint32 width = 640, height = 480;\n    try\n    {\n        ivx::Context context = ivx::Context::create();\n        ivx::Graph graph = ivx::Graph::create(context);\n        ivx::Image\n            gray = ivx::Image::create(context, width, height, VX_DF_IMAGE_U8),\n            gb   = ivx::Image::createVirtual(graph),\n            res  = ivx::Image::create(context, width, height, VX_DF_IMAGE_U8);\n\n        context.loadKernels(\"openvx-debug\");  // ivx::debug::*\n\n        ivx::debug::fReadImage(context, inputPath, gray);\n\n        ivx::Node::create(graph, VX_KERNEL_GAUSSIAN_3x3, gray, gb);\n        ivx::Node::create(\n            graph,\n            VX_KERNEL_THRESHOLD,\n            gb,\n            ivx::Threshold::createBinary(context, VX_TYPE_UINT8, 50),\n            res\n        );\n\n        graph.verify();\n        graph.process();\n\n        ivx::debug::fWriteImage(context, res, \"ovx-res-cpp.pgm\");\n    }\n    catch (const ivx::RuntimeError&amp; e)\n    {\n        printf(\"ErrorRuntime: code = %d(%x), message = %s\\n\", e.status(), e.status(), e.what());\n        return e.status();\n    }\n    catch (const ivx::WrapperError&amp; e)\n    {\n        printf(\"ErrorWrapper: message = %s\\n\", e.what());\n        return -1;\n    }\n    catch(const std::exception&amp; e)\n    {\n        printf(\"runtime_error: message = %s\\n\", e.what());\n        return -1;\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/openvx/#c-api-overview","title":"C++ API overview","text":"<p>The wrappers have header-only implementation that simplifies their integration to projects. All the API is inside <code>ivx</code> namespace (E.g. <code>class ivx::Graph</code>).</p> <p>While the C++ API is pretty much the same for underlying OpenVX version 1.0 and 1.1, there are alternative code branches for some features implementation  that are selected at compile time via <code>#ifdef</code> preprocessor directives. E.g. external ref-counting is implemented for 1.0 version and native OpenVX one is used (via  <code>vxRetainReference()</code> and <code>vxReleaseXYZ()</code>) for version 1.1.</p> <p>Also there are some C++ 11 features are used (e.g. rvalue ref-s) when their availability is detected at compile time.</p> <p>C++ exceptions are used for errors indication instead of return codes. There are two types of exceptions are defined:  <code>RuntimeError</code> is thrown when OpenVX C call returned unsuccessful result and <code>WrapperError</code> is thrown when a problem is occurred in the wrappers code. Both exception classes are derived from <code>std::exception</code> (actually from its inheritance).</p> <p>The so called OpenVX objects (e.g. <code>vx_image</code>) are represented as C++ classes in wrappers. All these classes use automatic ref-counting that allows development of exception-safe code. All these classes have <code>create()</code> or <code>createXYZ()</code> <code>static</code> methods for instances creation. (E.g. <code>Image::create()</code>, <code>Image::createVirtual()</code> and  <code>Image::createFromHandle()</code>) Most of the wrapped OpenVX functions are represented as methods of the corresponding C++ classes, but in most cases they still accept C \"object\" types (e.g. <code>vx_image</code> or <code>vx_context</code>) that allows mixing of C and C++ OpenVX API use. E.g.: <pre><code>class Image\n{\n    static Image create(vx_context context, vx_uint32 width, vx_uint32 height, vx_df_image format);\n    static Image createVirtual(vx_graph graph, vx_uint32 width = 0, vx_uint32 height = 0, vx_df_image format = VX_DF_IMAGE_VIRT);\n    // ...\n}\n</code></pre> All the classes instances can automatically be converted to the corresponding C \"object\" types.</p> <p>For more details please refer to C++ wrappers reference manual or directly to their source code.</p>"},{"location":"opencv_docs/opencv/3rdparty/openvx/hal/","title":"OpenVX-based HAL implementation.","text":"<p>It's built when OpenVX is available (<code>HAVE_OPENVX</code>). To build OpenCV with OpenVX support add the following cmake options: <code>-DOPENVX_ROOT=/path/to/prebuilt/openvx -DWITH_OPENVX=YES</code></p>"},{"location":"opencv_docs/opencv/3rdparty/protobuf/","title":"Index","text":"<p>Source code: https://github.com/protocolbuffers/protobuf Version: 3.19.1</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/","title":"Index","text":"CI Stable Develop GitHub Actions CodeFactor OSS-Fuzz Codecov"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#zlib-ng","title":"zlib-ng","text":"<p>zlib data compression library for the next generation systems</p> <p>Maintained by Hans Kristian Rosbach           aka Dead2 (zlib-ng \u00e0t circlestorm d\u00f3t org)</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#features","title":"Features","text":"<ul> <li>Zlib compatible API with support for dual-linking</li> <li>Modernized native API based on zlib API for ease of porting</li> <li>Modern C11 syntax and a clean code layout</li> <li>Deflate medium and quick algorithms based on Intel\u2019s zlib fork</li> <li>Support for CPU intrinsics when available</li> <li>Adler32 implementation using SSSE3, AVX2, AVX512, AVX512-VNNI, Neon, VMX &amp; VSX</li> <li>CRC32-B implementation using PCLMULQDQ, VPCLMULQDQ, ACLE, &amp; IBM Z</li> <li>Slide hash implementations using SSE2, AVX2, ARMv6, Neon, VMX &amp; VSX</li> <li>Compare256 implementations using SSE2, AVX2, Neon, POWER9 &amp; RVV</li> <li>Inflate chunk copying using SSE2, SSSE3, AVX, Neon &amp; VSX</li> <li>Support for hardware-accelerated deflate using IBM Z DFLTCC</li> <li>Unaligned memory read/writes and large bit buffer improvements</li> <li>Includes improvements from Cloudflare and Intel forks</li> <li>Configure, CMake, and NMake build system support</li> <li>Comprehensive set of CMake unit tests</li> <li>Code sanitizers, fuzzing, and coverage</li> <li>GitHub Actions continuous integration on Windows, macOS, and Linux</li> <li>Emulated CI for ARM, AARCH64, PPC, PPC64, RISCV, SPARC64, S390x using qemu</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#history","title":"History","text":"<p>The motivation for this fork was seeing several 3rd party contributions with new optimizations not getting implemented into the official zlib repository.</p> <p>Mark Adler has been maintaining zlib for a very long time, and he has done a great job and hopefully he will continue for a long time yet. The idea of zlib-ng is not to replace zlib, but to co-exist as a drop-in replacement with a lower threshold for code change.</p> <p>zlib has a long history and is incredibly portable, even supporting many systems that predate the Internet. That is great, but it can complicate further development and maintainability. The zlib code contains many workarounds for really old compilers or to accommodate systems with limitations such as operating in a 16-bit environment.</p> <p>Many of these workarounds are only maintenance burdens, some of them are pretty huge code-wise. With many workarounds cluttered throughout the code, it makes it harder for new programmers with an idea/interest for zlib to contribute.</p> <p>I decided to make a fork, merge all the Intel optimizations, some of the Cloudflare optimizations, plus a couple other smaller patches. Then started cleaning out workarounds, various dead code, all contrib and example code. The result is a better performing and easier to maintain zlib-ng.</p> <p>A lot of improvements have gone into zlib-ng since its start, and numerous people and companies have contributed both small and big improvements, or valuable testing.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#build","title":"Build","text":"<p><sup>Please read LICENSE.md, it is very simple and very liberal.</sup></p> <p>There are two ways to build zlib-ng:</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#cmake","title":"Cmake","text":"<p>To build zlib-ng using the cross-platform makefile generator cmake.</p> <pre><code>cmake .\ncmake --build . --config Release\nctest --verbose -C Release\n</code></pre> <p>Alternatively, you can use the cmake configuration GUI tool ccmake:</p> <pre><code>ccmake .\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#configure","title":"Configure","text":"<p>To build zlib-ng using the bash configure script:</p> <pre><code>./configure\nmake\nmake test\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#build-options","title":"Build Options","text":"CMake configure Description Default ZLIB_COMPAT --zlib-compat Compile with zlib compatible API OFF ZLIB_ENABLE_TESTS Build test binaries ON WITH_GZFILEOP --without-gzfileops Compile with support for gzFile related functions ON WITH_OPTIM --without-optimizations Build with optimisations ON WITH_NEW_STRATEGIES --without-new-strategies Use new strategies ON WITH_NATIVE_INSTRUCTIONS Compiles with full instruction set supported on this host (gcc/clang -march=native) OFF WITH_RUNTIME_CPU_DETECTION Compiles with runtime CPU detection ON WITH_SANITIZER Build with sanitizer (memory, address, undefined) OFF WITH_GTEST Build gtest_zlib ON WITH_FUZZERS Build test/fuzz OFF WITH_BENCHMARKS Build test/benchmarks OFF WITH_MAINTAINER_WARNINGS Build with project maintainer warnings OFF WITH_CODE_COVERAGE Enable code coverage reporting OFF"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#install","title":"Install","text":"<p>WARNING: We do not recommend manually installing unless you really know what you are doing, because this can potentially override the system default zlib library, and any incompatibility or wrong configuration of zlib-ng can make the whole system unusable, requiring recovery or reinstall. If you still want a manual install, we recommend using the /opt/ path prefix.</p> <p>For Linux distros, an alternative way to use zlib-ng (if compiled in zlib-compat mode) instead of zlib, is through the use of the LD_PRELOAD environment variable. If the program is dynamically linked with zlib, then the program will temporarily attempt to use zlib-ng instead, without risking system-wide instability.</p> <pre><code>LD_PRELOAD=/opt/zlib-ng/libz.so.1.2.13.zlib-ng /usr/bin/program\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#cmake_1","title":"Cmake","text":"<p>To install zlib-ng system-wide using cmake:</p> <p>```sh or powershell cmake --build . --target install <pre><code>### Configure\n\nTo install zlib-ng system-wide using the configure script:\n\n```sh\nmake install\n</code></pre></p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#cpack","title":"CPack","text":"<p>After building with cmake, an installation package can be created using cpack. By default a tgz package is created, but you can append <code>-G &lt;format&gt;</code> to each command to generate alternative packages types (TGZ, ZIP, RPM, DEB). To easily create a rpm or deb package, you would use <code>-G RPM</code> or <code>-G DEB</code> respectively.</p> <p>```sh or powershell cd build cpack --config CPackConfig.cmake cpack --config CPackSourceConfig.cmake <pre><code>### Vcpkg\n\nAlternatively, you can build and install zlib-ng using the [vcpkg](https://github.com/Microsoft/vcpkg/) dependency manager:\n\n```sh or powershell\ngit clone https://github.com/Microsoft/vcpkg.git\ncd vcpkg\n./bootstrap-vcpkg.sh # \"./bootstrap-vcpkg.bat\" for powershell\n./vcpkg integrate install\n./vcpkg install zlib-ng\n</code></pre></p> <p>The zlib-ng port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please create an issue or pull request on the vcpkg repository.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#contributing","title":"Contributing","text":"<p>Zlib-ng is aiming to be open to contributions, and we would be delighted to receive pull requests on github. Help with testing and reviewing pull requests etc is also very much appreciated.</p> <p>Please check the Wiki for more info: Contributing</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#acknowledgments","title":"Acknowledgments","text":"<p>Thanks go out to all the people and companies who have taken the time to contribute code reviews, testing and/or patches. Zlib-ng would not have been nearly as good without you.</p> <p>The deflate format used by zlib was defined by Phil Katz. The deflate and zlib specifications were written by L. Peter Deutsch.</p> <p>zlib was originally created by Jean-loup Gailly (compression) and Mark Adler (decompression).</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#advanced-build-options","title":"Advanced Build Options","text":"CMake configure Description Default FORCE_SSE2 --force-sse2 Skip runtime check for SSE2 instructions (Always on for x86_64) OFF (x86) WITH_AVX2 Build with AVX2 intrinsics ON WITH_AVX512 Build with AVX512 intrinsics ON WITH_AVX512VNNI Build with AVX512VNNI intrinsics ON WITH_SSE2 Build with SSE2 intrinsics ON WITH_SSSE3 Build with SSSE3 intrinsics ON WITH_SSE42 Build with SSE42 intrinsics ON WITH_PCLMULQDQ Build with PCLMULQDQ intrinsics ON WITH_VPCLMULQDQ --without-vpclmulqdq Build with VPCLMULQDQ intrinsics ON WITH_ACLE --without-acle Build with ACLE intrinsics ON WITH_NEON --without-neon Build with NEON intrinsics ON WITH_ARMV6 --without-armv6 Build with ARMv6 intrinsics ON WITH_ALTIVEC --without-altivec Build with AltiVec (VMX) intrinsics ON WITH_POWER8 --without-power8 Build with POWER8 optimisations ON WITH_RVV Build with RVV intrinsics ON WITH_CRC32_VX --without-crc32-vx Build with vectorized CRC32 on IBM Z ON WITH_DFLTCC_DEFLATE --with-dfltcc-deflate Build with DFLTCC intrinsics for compression on IBM Z OFF WITH_DFLTCC_INFLATE --with-dfltcc-inflate Build with DFLTCC intrinsics for decompression on IBM Z OFF WITH_UNALIGNED --without-unaligned Allow optimizations that use unaligned reads if safe on current arch ON WITH_INFLATE_STRICT Build with strict inflate distance checking OFF WITH_INFLATE_ALLOW_INVALID_DIST Build with zero fill for inflate invalid distances OFF INSTALL_UTILS Copy minigzip and minideflate during install OFF ZLIBNG_ENABLE_TESTS Test zlib-ng specific API ON"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/#related-projects","title":"Related Projects","text":"<ul> <li>Fork of the popular minizip                   https://github.com/zlib-ng/minizip-ng</li> <li>Python tool to benchmark minigzip/minideflate https://github.com/zlib-ng/deflatebench</li> <li>Python tool to benchmark pigz                 https://github.com/zlib-ng/pigzbench</li> <li>3rd party patches for zlib-ng compatibility   https://github.com/zlib-ng/patches</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/LICENSE/","title":"LICENSE","text":"<p>(C) 1995-2024 Jean-loup Gailly and Mark Adler</p> <p>This software is provided 'as-is', without any express or implied warranty. In no event will the authors be held liable for any damages arising from the use of this software.</p> <p>Permission is granted to anyone to use this software for any purpose, including commercial applications, and to alter it and redistribute it freely, subject to the following restrictions:</p> <ol> <li> <p>The origin of this software must not be misrepresented; you must not    claim that you wrote the original software. If you use this software    in a product, an acknowledgment in the product documentation would be    appreciated but is not required.</p> </li> <li> <p>Altered source versions must be plainly marked as such, and must not be    misrepresented as being the original software.</p> </li> <li> <p>This notice may not be removed or altered from any source distribution.</p> </li> </ol>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/riscv/","title":"Building RISC-V Target with Cmake","text":"<p>Warning Runtime rvv detection (using <code>hwcap</code>) requires linux kernel 6.5 or newer.</p> <p>When running on older kernels, we fall back to compile-time detection, potentially this can cause crashes if rvv is enabled at compile but not supported by the target cpu. Therefore if older kernel support is needed, rvv should be disabled if the target cpu does not support it.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/riscv/#prerequisite-build-risc-v-clang-toolchain-and-qemu","title":"Prerequisite: Build RISC-V Clang Toolchain and QEMU","text":"<p>If you don't have prebuilt clang and riscv64 qemu, you can refer to the script to get the source. Copy the script to the zlib-ng root directory, and run it to download the source and build them. Modify the content according to your conditions (e.g., toolchain version).</p> <pre><code>./prepare_riscv_toolchain_qemu.sh\n</code></pre> <p>After running script, clang &amp; qemu are built in <code>build-toolchain-qemu/riscv-clang/</code> &amp; <code>build-toolchain-qemu/riscv-qemu/</code>.</p> <p><code>build-toolchain-qemu/riscv-clang/</code> is your <code>TOOLCHAIN_PATH</code>. <code>build-toolchain-qemu/riscv-qemu/bin/qemu-riscv64</code> is your <code>QEMU_PATH</code>.</p> <p>You can also download the prebuilt toolchain &amp; qemu from the release page, and enjoy using them.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/riscv/#cross-compile-for-risc-v-target","title":"Cross-Compile for RISC-V Target","text":"<pre><code>cmake -G Ninja -B ./build-riscv \\\n  -D CMAKE_TOOLCHAIN_FILE=./cmake/toolchain-riscv.cmake \\\n  -D CMAKE_INSTALL_PREFIX=./build-riscv/install \\\n  -D TOOLCHAIN_PATH={TOOLCHAIN_PATH} \\\n  -D QEMU_PATH={QEMU_PATH} \\\n  .\n\ncmake --build ./build-riscv\n</code></pre> <p>Disable the option if there is no RVV support: <pre><code>-D WITH_RVV=OFF\n</code></pre></p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/riscv/#run-unittests-on-user-mode-qemu","title":"Run Unittests on User Mode QEMU","text":"<pre><code>cd ./build-riscv &amp;&amp; ctest --verbose\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/","title":"Introduction","text":"<p>This directory contains SystemZ deflate hardware acceleration support. It can be enabled using the following build commands:</p> <pre><code>$ ./configure --with-dfltcc-deflate --with-dfltcc-inflate\n$ make\n</code></pre> <p>or</p> <pre><code>$ cmake -DWITH_DFLTCC_DEFLATE=1 -DWITH_DFLTCC_INFLATE=1 .\n$ make\n</code></pre> <p>When built like this, zlib-ng would compress using hardware on level 1, and using software on all other levels. Decompression will always happen in hardware. In order to enable hardware compression for levels 1-6 (i.e. to make it used by default) one could add <code>-DDFLTCC_LEVEL_MASK=0x7e</code> to CFLAGS when building zlib-ng.</p> <p>SystemZ deflate hardware acceleration is available on IBM z15 and newer machines under the name  \"Integrated Accelerator for zEnterprise Data Compression\". The programming interface to it is a machine instruction called DEFLATE CONVERSION CALL (DFLTCC). It is documented in Chapter 26 of Principles of Operation. Both the code and the rest of this document refer to this feature simply as \"DFLTCC\".</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#performance","title":"Performance","text":"<p>Performance figures are published here. The compression speed-up can be as high as 110x and the decompression speed-up can be as high as 15x.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#limitations","title":"Limitations","text":"<p>Two DFLTCC compression calls with identical inputs are not guaranteed to produce identical outputs. Therefore care should be taken when using hardware compression when reproducible results are desired. In particular, zlib-ng-specific <code>zng_deflateSetParams</code> call allows setting <code>Z_DEFLATE_REPRODUCIBLE</code> parameter, which disables DFLTCC support for a particular stream.</p> <p>DFLTCC does not support every single zlib-ng feature, in particular:</p> <ul> <li><code>inflate(Z_BLOCK)</code> and <code>inflate(Z_TREES)</code></li> <li><code>inflateMark()</code></li> <li><code>inflatePrime()</code></li> <li><code>inflateSyncPoint()</code></li> </ul> <p>When used, these functions will either switch to software, or, in case this is not possible, gracefully fail.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#code-structure","title":"Code structure","text":"<p>All SystemZ-specific code lives in <code>arch/s390</code> directory and is integrated with the rest of zlib-ng using hook macros.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#hook-macros","title":"Hook macros","text":"<p>DFLTCC takes as arguments a parameter block, an input buffer, an output buffer, and a window. Parameter blocks are stored alongside zlib states; buffers are forwarded from the caller; and window - which must be 4k-aligned and is always 64k large, is managed using the <code>PAD_WINDOW()</code>, <code>WINDOW_PAD_SIZE</code>, <code>HINT_ALIGNED_WINDOW</code> and <code>DEFLATE_ADJUST_WINDOW_SIZE()</code> and <code>INFLATE_ADJUST_WINDOW_SIZE()</code> hooks.</p> <p>Software and hardware window formats do not match, therefore, <code>deflateSetDictionary()</code>, <code>deflateGetDictionary()</code>, <code>inflateSetDictionary()</code> and <code>inflateGetDictionary()</code> need special handling, which is triggered using <code>DEFLATE_SET_DICTIONARY_HOOK()</code>, <code>DEFLATE_GET_DICTIONARY_HOOK()</code>, <code>INFLATE_SET_DICTIONARY_HOOK()</code> and <code>INFLATE_GET_DICTIONARY_HOOK()</code> macros.</p> <p><code>deflateResetKeep()</code> and <code>inflateResetKeep()</code> update the DFLTCC parameter block using <code>DEFLATE_RESET_KEEP_HOOK()</code> and <code>INFLATE_RESET_KEEP_HOOK()</code> macros.</p> <p><code>INFLATE_PRIME_HOOK()</code>, <code>INFLATE_MARK_HOOK()</code> and <code>INFLATE_SYNC_POINT_HOOK()</code> macros make the respective unsupported calls gracefully fail.</p> <p><code>DEFLATE_PARAMS_HOOK()</code> implements switching between hardware and software compression mid-stream using <code>deflateParams()</code>. Switching normally entails flushing the current block, which might not be possible in low memory situations. <code>deflateParams()</code> uses <code>DEFLATE_DONE()</code> hook in order to detect and gracefully handle such situations.</p> <p>The algorithm implemented in hardware has different compression ratio than the one implemented in software. <code>DEFLATE_BOUND_ADJUST_COMPLEN()</code> and <code>DEFLATE_NEED_CONSERVATIVE_BOUND()</code> macros make <code>deflateBound()</code> return the correct results for the hardware implementation.</p> <p>Actual compression and decompression are handled by <code>DEFLATE_HOOK()</code> and <code>INFLATE_TYPEDO_HOOK()</code> macros. Since inflation with DFLTCC manages the window on its own, calling <code>updatewindow()</code> is suppressed using <code>INFLATE_NEED_UPDATEWINDOW()</code> macro.</p> <p>In addition to compression, DFLTCC computes CRC-32 and Adler-32 checksums, therefore, whenever it's used, software checksumming is suppressed using <code>DEFLATE_NEED_CHECKSUM()</code> and <code>INFLATE_NEED_CHECKSUM()</code> macros.</p> <p>While software always produces reproducible compression results, this is not the case for DFLTCC. Therefore, zlib-ng users are given the ability to specify whether or not reproducible compression results are required. While it is always possible to specify this setting before the compression begins, it is not always possible to do so in the middle of a deflate stream - the exact conditions for that are determined by <code>DEFLATE_CAN_SET_REPRODUCIBLE()</code> macro.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#systemz-specific-code","title":"SystemZ-specific code","text":"<p>When zlib-ng is built with DFLTCC, the hooks described above are converted to calls to functions, which are implemented in <code>arch/s390/dfltcc_*</code> files. The functions can be grouped in three broad categories:</p> <ul> <li>Base DFLTCC support, e.g. wrapping the machine instruction - <code>dfltcc()</code>.</li> <li>Translating between software and hardware data formats, e.g.   <code>dfltcc_deflate_set_dictionary()</code>.</li> <li>Translating between software and hardware state machines, e.g.   <code>dfltcc_deflate()</code> and <code>dfltcc_inflate()</code>.</li> </ul> <p>The functions from the first two categories are fairly simple, however, various quirks in both software and hardware state machines make the functions from the third category quite complicated.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#dfltcc_deflate-function","title":"<code>dfltcc_deflate()</code> function","text":"<p>This function is called by <code>deflate()</code> and has the following responsibilities:</p> <ul> <li>Checking whether DFLTCC can be used with the current stream. If this   is not the case, then it returns <code>0</code>, making <code>deflate()</code> use some   other function in order to compress in software. Otherwise it returns   <code>1</code>.</li> <li>Block management and Huffman table generation. DFLTCC ends blocks only   when explicitly instructed to do so by the software. Furthermore,   whether to use fixed or dynamic Huffman tables must also be determined   by the software. Since looking at data in order to gather statistics   would negate performance benefits, the following approach is used: the   first <code>DFLTCC_FIRST_FHT_BLOCK_SIZE</code> bytes are placed into a fixed   block, and every next <code>DFLTCC_BLOCK_SIZE</code> bytes are placed into   dynamic blocks.</li> <li>Writing EOBS. Block Closing Control bit in the parameter block   instructs DFLTCC to write EOBS, however, certain conditions need to be   met: input data length must be non-zero or Continuation Flag must be   set. To put this in simpler terms, DFLTCC will silently refuse to   write EOBS if this is the only thing that it is asked to do. Since the   code has to be able to emit EOBS in software anyway, in order to avoid   tricky corner cases Block Closing Control is never used. Whether to   write EOBS is instead controlled by <code>soft_bcc</code> variable.</li> <li>Triggering block post-processing. Depending on flush mode, <code>deflate()</code>   must perform various additional actions when a block or a stream ends.   <code>dfltcc_deflate()</code> informs <code>deflate()</code> about this using   <code>block_state *result</code> parameter.</li> <li>Converting software state fields into hardware parameter block fields,   and vice versa. For example, <code>wrap</code> and Check Value Type or <code>bi_valid</code>   and Sub-Byte Boundary. Certain fields cannot be translated and must   persist untouched in the parameter block between calls, for example,   Continuation Flag or Continuation State Buffer.</li> <li>Handling flush modes and low-memory situations. These aspects are   quite intertwined and pervasive. The general idea here is that the   code must not do anything in software - whether explicitly by e.g.   calling <code>send_eobs()</code>, or implicitly - by returning to <code>deflate()</code>   with certain return and <code>*result</code> values, when Continuation Flag is   set.</li> <li>Ending streams. When a new block is started and flush mode is   <code>Z_FINISH</code>, Block Header Final parameter block bit is used to mark   this block as final. However, sometimes an empty final block is   needed, and, unfortunately, just like with EOBS, DFLTCC will silently   refuse to do this. The general idea of DFLTCC implementation is to   rely as much as possible on the existing code. Here in order to do   this, the code pretends that it does not support DFLTCC, which makes   <code>deflate()</code> call a software compression function, which writes an   empty final block. Whether this is required is controlled by   <code>need_empty_block</code> variable.</li> <li>Error handling. This is simply converting   Operation-Ending-Supplemental Code to string. Errors can only happen   due to things like memory corruption, and therefore they don't affect   the <code>deflate()</code> return code.</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#dfltcc_inflate-function","title":"<code>dfltcc_inflate()</code> function","text":"<p>This function is called by <code>inflate()</code> from the <code>TYPEDO</code> state (that is, when all the metadata is parsed and the stream is positioned at the type bits of deflate block header) and it's responsible for the following:</p> <ul> <li>Falling back to software when flush mode is <code>Z_BLOCK</code> or <code>Z_TREES</code>.   Unfortunately, there is no way to ask DFLTCC to stop decompressing on   block or tree boundary.</li> <li><code>inflate()</code> decompression loop management. This is controlled using   the return value, which can be either <code>DFLTCC_INFLATE_BREAK</code> or   <code>DFLTCC_INFLATE_CONTINUE</code>.</li> <li>Converting software state fields into hardware parameter block fields,   and vice versa. For example, <code>whave</code> and History Length or <code>wnext</code> and   History Offset.</li> <li>Ending streams. This instructs <code>inflate()</code> to return <code>Z_STREAM_END</code>   and is controlled by <code>last</code> state field.</li> <li>Error handling. Like deflate, error handling comprises   Operation-Ending-Supplemental Code to string conversion. Unlike   deflate, errors may happen due to bad inputs, therefore they are   propagated to <code>inflate()</code> by setting <code>mode</code> field to <code>MEM</code> or <code>BAD</code>.</li> </ul>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#testing","title":"Testing","text":"<p>Given complexity of DFLTCC machine instruction, it is not clear whether QEMU TCG will ever support it. At the time of writing, one has to have access to an IBM z15+ VM or LPAR in order to test DFLTCC support. Since DFLTCC is a non-privileged instruction, neither special VM/LPAR configuration nor root are required.</p> <p>zlib-ng CI uses an IBM-provided z15 self-hosted builder for the DFLTCC testing. There is no official IBM Z GitHub Actions runner, so we build one inspired by <code>anup-kodlekere/gaplib</code>. Future updates to actions-runner might need an updated patch. The .net version number patch has been separated into a separate file to avoid a need for constantly changing the patch.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#configuring-the-builder","title":"Configuring the builder.","text":""},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#install-prerequisites","title":"Install prerequisites.","text":"<pre><code>sudo dnf install podman\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#add-actions-runner-service","title":"Add actions-runner service.","text":"<pre><code>sudo cp self-hosted-builder/actions-runner.service /etc/systemd/system/\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#create-a-config-file-needs-github-personal-access-token","title":"Create a config file, needs github personal access token.","text":"<pre><code># Create file /etc/actions-runner\nrepo=&lt;owner&gt;/&lt;name&gt;\naccess_token=&lt;ghp_***&gt;\n</code></pre> <p>Access token should have the repo scope, consult https://docs.github.com/en/rest/reference/actions#create-a-registration-token-for-a-repository for details.</p>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#autostart-actions-runner","title":"Autostart actions-runner.","text":"<pre><code>$ sudo systemctl enable --now actions-runner\n</code></pre>"},{"location":"opencv_docs/opencv/3rdparty/zlib-ng/arch/s390/#rebuilding-the-container","title":"Rebuilding the container","text":"<p>In order to update the <code>gaplib-actions-runner</code> podman container, e.g. to get the latest OS security fixes, follow these steps: <pre><code># Stop actions-runner service\nsudo systemctl stop actions-runner\n\n# Delete old container\nsudo podman container rm gaplib-actions-runner\n\n# Delete old image\nsudo podman image rm localhost/zlib-ng/actions-runner\n\n# Build image\nsudo podman build --squash -f Dockerfile.zlib-ng --tag zlib-ng/actions-runner --build-arg .\n\n# Build container\nsudo podman create --name=gaplib-actions-runner --env-file=/etc/actions-runner --init --interactive --volume=actions-runner-temp:/home/actions-runner zlib-ng/actions-runner\n\n# Start actions-runner service\nsudo systemctl start actions-runner\n</code></pre></p>"},{"location":"opencv_docs/opencv/apps/opencv_stitching_tool/","title":"Index","text":""},{"location":"opencv_docs/opencv/apps/opencv_stitching_tool/#moved-opencv_stitching_tool","title":"MOVED: opencv_stitching_tool","text":"<p>As the stitching package is now available on PyPI the tool and belonging package are now maintained here. The Tutorial is maintained here.</p>"},{"location":"opencv_docs/opencv/doc/opencv-logo/","title":"Opencv logo","text":"<p>OpenCV logo has been originally designed and contributed to OpenCV by Adi Shavit in 2006. The graphical part consists of three stylized letters O, C, V, colored in the primary R, G, B color components, used by humans and computers to perceive the world. It is shaped in a way to mimic the famous Kanizsa's triangle to emphasize that the prior knowledge and internal processing are at least as important as the actually acquired \"raw\" data.</p> <p>The restyled version of the logo has been designed and contributed by xperience.ai in July 2020 for the 20th anniversary of OpenCV.</p> <p>The logo uses Exo 2 font by Natanael Gama distributed under OFL license.</p> <p>Higher-resolution version of the logo, as well as SVG version of it, can be obtained at OpenCV Media Kit.</p> <p></p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers/","title":"Custom deep learning layers support {#tutorial_dnn_custom_layers}","text":"<p>@tableofcontents</p> <p>@prev_tutorial{tutorial_dnn_javascript} @next_tutorial{tutorial_dnn_OCR}</p> Original author Dmitry Kurtaev Compatibility OpenCV &gt;= 3.4.1"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers/#introduction","title":"Introduction","text":"<p>Deep learning is a fast-growing area. New approaches to building neural networks usually introduce new types of layers. These could be modifications of existing ones or implementation of outstanding research ideas.</p> <p>OpenCV allows importing and running networks from different deep learning frameworks. There is a number of the most popular layers. However, you can face a problem that your network cannot be imported using OpenCV because some layers of your network can be not implemented in the deep learning engine of OpenCV.</p> <p>The first solution is to create a feature request at https://github.com/opencv/opencv/issues mentioning details such as a source of a model and a type of new layer. The new layer could be implemented if the OpenCV community shares this need.</p> <p>The second way is to define a custom layer so that OpenCV's deep learning engine will know how to use it. This tutorial is dedicated to show you a process of deep learning model's import customization.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers/#define-a-custom-layer-in-c","title":"Define a custom layer in C++","text":"<p>Deep learning layer is a building block of network's pipeline. It has connections to input blobs and produces results to output blobs. There are trained weights and hyper-parameters. Layers' names, types, weights and hyper-parameters are stored in files are generated by native frameworks during training. If OpenCV encounters unknown layer type it throws an exception while trying to read a model:</p> <pre><code>Unspecified error: Can't create layer \"layer_name\" of type \"MyType\" in function getLayerInstance\n</code></pre> <p>To import the model correctly you have to derive a class from cv::dnn::Layer with the following methods:</p> <p>@snippet dnn/custom_layers.hpp A custom layer interface</p> <p>And register it before the import:</p> <p>@snippet dnn/custom_layers.hpp Register a custom layer</p> <p>@note <code>MyType</code> is a type of unimplemented layer from the thrown exception.</p> <p>Let's see what all the methods do:</p> <ul> <li>Constructor</li> </ul> <p>@snippet dnn/custom_layers.hpp MyLayer::MyLayer</p> <p>Retrieves hyper-parameters from cv::dnn::LayerParams. If your layer has trainable weights they will be already stored in the Layer's member cv::dnn::Layer::blobs.</p> <ul> <li>A static method <code>create</code></li> </ul> <p>@snippet dnn/custom_layers.hpp MyLayer::create</p> <p>This method should create an instance of you layer and return cv::Ptr with it.</p> <ul> <li>Output blobs' shape computation</li> </ul> <p>@snippet dnn/custom_layers.hpp MyLayer::getMemoryShapes</p> <p>Returns layer's output shapes depending on input shapes. You may request an extra memory using <code>internals</code>.</p> <ul> <li>Run a layer</li> </ul> <p>@snippet dnn/custom_layers.hpp MyLayer::forward</p> <p>Implement a layer's logic here. Compute outputs for given inputs.</p> <p>@note OpenCV manages memory allocated for layers. In the most cases the same memory can be reused between layers. So your <code>forward</code> implementation should not rely on that the second invocation of <code>forward</code> will have the same data at <code>outputs</code> and <code>internals</code>.</p> <ul> <li>Optional <code>finalize</code> method</li> </ul> <p>@snippet dnn/custom_layers.hpp MyLayer::finalize</p> <p>The chain of methods is the following: OpenCV deep learning engine calls <code>create</code> method once, then it calls <code>getMemoryShapes</code> for every created layer, then you can make some preparations depend on known input dimensions at cv::dnn::Layer::finalize. After network was initialized only <code>forward</code> method is called for every network's input.</p> <p>@note Varying input blobs' sizes such height, width or batch size make OpenCV reallocate all the internal memory. That leads to efficiency gaps. Try to initialize and deploy models using a fixed batch size and image's dimensions.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers/#example-custom-layer-from-caffe","title":"Example: custom layer from Caffe","text":"<p>Let's create a custom layer <code>Interp</code> from https://github.com/cdmh/deeplab-public. It's just a simple resize that takes an input blob of size <code>N x C x Hi x Wi</code> and returns an output blob of size <code>N x C x Ho x Wo</code> where <code>N</code> is a batch size, <code>C</code> is a number of channels, <code>Hi x Wi</code> and <code>Ho x Wo</code> are input and output <code>height x width</code> correspondingly. This layer has no trainable weights but it has hyper-parameters to specify an output size.</p> <p>In example, <pre><code>layer {\n  name: \"output\"\n  type: \"Interp\"\n  bottom: \"input\"\n  top: \"output\"\n  interp_param {\n    height: 9\n    width: 8\n  }\n}\n</code></pre></p> <p>This way our implementation can look like:</p> <p>@snippet dnn/custom_layers.hpp InterpLayer</p> <p>Next we need to register a new layer type and try to import the model.</p> <p>@snippet dnn/custom_layers.hpp Register InterpLayer</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers/#example-custom-layer-from-tensorflow","title":"Example: custom layer from TensorFlow","text":"<p>This is an example of how to import a network with tf.image.resize_bilinear operation. This is also a resize but with an implementation different from OpenCV's or <code>Interp</code> above.</p> <p>Let's create a single layer network: <pre><code>inp = tf.placeholder(tf.float32, [2, 3, 4, 5], 'input')\nresized = tf.image.resize_bilinear(inp, size=[9, 8], name='resize_bilinear')\n</code></pre> OpenCV sees that TensorFlow's graph in the following way:</p> <p><pre><code>node {\n  name: \"input\"\n  op: \"Placeholder\"\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: \"resize_bilinear/size\"\n  op: \"Const\"\n  attr {\n    key: \"dtype\"\n    value {\n      type: DT_INT32\n    }\n  }\n  attr {\n    key: \"value\"\n    value {\n      tensor {\n        dtype: DT_INT32\n        tensor_shape {\n          dim {\n            size: 2\n          }\n        }\n        tensor_content: \"\\t\\000\\000\\000\\010\\000\\000\\000\"\n      }\n    }\n  }\n}\nnode {\n  name: \"resize_bilinear\"\n  op: \"ResizeBilinear\"\n  input: \"input:0\"\n  input: \"resize_bilinear/size\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"align_corners\"\n    value {\n      b: false\n    }\n  }\n}\nlibrary {\n}\n</code></pre> Custom layers import from TensorFlow is designed to put all layer's <code>attr</code> into cv::dnn::LayerParams but input <code>Const</code> blobs into cv::dnn::Layer::blobs. In our case resize's output shape will be stored in layer's <code>blobs[0]</code>.</p> <p>@snippet dnn/custom_layers.hpp ResizeBilinearLayer</p> <p>Next we register a layer and try to import the model.</p> <p>@snippet dnn/custom_layers.hpp Register ResizeBilinearLayer</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_custom_layers/dnn_custom_layers/#define-a-custom-layer-in-python","title":"Define a custom layer in Python","text":"<p>The following example shows how to customize OpenCV's layers in Python.</p> <p>Let's consider Holistically-Nested Edge Detection deep learning model. That was trained with one and only difference comparing to a current version of Caffe framework. <code>Crop</code> layers that receive two input blobs and crop the first one to match spatial dimensions of the second one used to crop from the center. Nowadays Caffe's layer does it from the top-left corner. So using the latest version of Caffe or OpenCV you will get shifted results with filled borders.</p> <p>Next we're going to replace OpenCV's <code>Crop</code> layer that makes top-left cropping by a centric one.</p> <ul> <li>Create a class with <code>getMemoryShapes</code> and <code>forward</code> methods</li> </ul> <p>@snippet dnn/edge_detection.py CropLayer</p> <p>@note Both methods should return lists.</p> <ul> <li>Register a new layer.</li> </ul> <p>@snippet dnn/edge_detection.py Register</p> <p>That's it! We have replaced an implemented OpenCV's layer to a custom one. You may find a full script in the source code.</p> ![](js_tutorials/js_assets/lena.jpg) ![](images/lena_hed.jpg)"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial/","title":"Conversion of PyTorch Classification Models and Launch with OpenCV C++ {#pytorch_cls_c_tutorial_dnn_conversion}","text":"<p>@prev_tutorial{pytorch_cls_tutorial_dnn_conversion}</p> Original author Anastasia Murzova Compatibility OpenCV &gt;= 4.5"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial/#goals","title":"Goals","text":"<p>In this tutorial you will learn how to: * convert PyTorch classification models into ONNX format * run converted PyTorch model with OpenCV C/C++ API * provide model inference</p> <p>We will explore the above-listed points by the example of ResNet-50 architecture.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial/#introduction","title":"Introduction","text":"<p>Let's briefly view the key concepts involved in the pipeline of PyTorch models transition with OpenCV API. The initial step in conversion of PyTorch models into cv::dnn::Net is model transferring into ONNX format. ONNX aims at the interchangeability of the neural networks between various frameworks. There is a built-in function in PyTorch for ONNX conversion: <code>torch.onnx.export</code>. Further the obtained <code>.onnx</code> model is passed into cv::dnn::readNetFromONNX or cv::dnn::readNet.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial/#requirements","title":"Requirements","text":"<p>To be able to experiment with the below code you will need to install a set of libraries. We will use a virtual environment with python3.7+ for this:</p> <pre><code>virtualenv -p /usr/bin/python3.7 &lt;env_dir_path&gt;\nsource &lt;env_dir_path&gt;/bin/activate\n</code></pre> <p>For OpenCV-Python building from source, follow the corresponding instructions from the @ref tutorial_py_table_of_contents_setup.</p> <p>Before you start the installation of the libraries, you can customize the requirements.txt, excluding or including (for example, <code>opencv-python</code>) some dependencies. The below line initiates requirements installation into the previously activated virtual environment:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial/#practice","title":"Practice","text":"<p>In this part we are going to cover the following points: 1. create a classification model conversion pipeline 2. provide the inference, process prediction results</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial/#model-conversion-pipeline","title":"Model Conversion Pipeline","text":"<p>The code in this subchapter is located in the <code>samples/dnn/dnn_model_runner</code> module and can be executed with the line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50_onnx\n</code></pre> <p>The following code contains the description of the below-listed steps: 1. instantiate PyTorch model 2. convert PyTorch model into <code>.onnx</code></p> <pre><code># initialize PyTorch ResNet-50 model\noriginal_model = models.resnet50(pretrained=True)\n\n# get the path to the converted into ONNX PyTorch model\nfull_model_path = get_pytorch_onnx_model(original_model)\nprint(\"PyTorch ResNet-50 model was successfully converted: \", full_model_path)\n</code></pre> <p><code>get_pytorch_onnx_model(original_model)</code> function is based on <code>torch.onnx.export(...)</code> call:</p> <pre><code># define the directory for further converted model save\nonnx_model_path = \"models\"\n# define the name of further converted model\nonnx_model_name = \"resnet50.onnx\"\n\n# create directory for further converted model\nos.makedirs(onnx_model_path, exist_ok=True)\n\n# get full path to the converted model\nfull_model_path = os.path.join(onnx_model_path, onnx_model_name)\n\n# generate model input\ngenerated_input = Variable(\n    torch.randn(1, 3, 224, 224)\n)\n\n# model export into ONNX format\ntorch.onnx.export(\n    original_model,\n    generated_input,\n    full_model_path,\n    verbose=True,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    opset_version=11\n)\n</code></pre> <p>After the successful execution of the above code we will get the following output:</p> <pre><code>PyTorch ResNet-50 model was successfully converted: models/resnet50.onnx\n</code></pre> <p>The proposed in <code>dnn/samples</code> module <code>dnn_model_runner</code> allows us to reproduce the above conversion steps for the following PyTorch classification models: * alexnet * vgg11 * vgg13 * vgg16 * vgg19 * resnet18 * resnet34 * resnet50 * resnet101 * resnet152 * squeezenet1_0 * squeezenet1_1 * resnext50_32x4d * resnext101_32x8d * wide_resnet50_2 * wide_resnet101_2</p> <p>To obtain the converted model, the following line should be executed:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name &lt;pytorch_cls_model_name&gt; --evaluate False\n</code></pre> <p>For the ResNet-50 case the below line should be run:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name resnet50 --evaluate False\n</code></pre> <p>The default root directory for the converted model storage is defined in module <code>CommonConfig</code>:</p> <pre><code>@dataclass\nclass CommonConfig:\n    output_data_root_dir: str = \"dnn_model_runner/dnn_conversion\"\n</code></pre> <p>Thus, the converted ResNet-50 will be saved in <code>dnn_model_runner/dnn_conversion/models</code>.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_c_tutorial/#inference-pipeline","title":"Inference Pipeline","text":"<p>Now we can use <code>models/resnet50.onnx</code> for the inference pipeline using OpenCV C/C++ API. The implemented pipeline can be found in samples/dnn/classification.cpp. After the build of samples (<code>BUILD_EXAMPLES</code> flag value should be <code>ON</code>), the appropriate <code>example_dnn_classification</code> executable file will be provided.</p> <p>To provide model inference we will use the below squirrel photo (under CC0 license) corresponding to ImageNet class ID 335: <pre><code>fox squirrel, eastern fox squirrel, Sciurus niger\n</code></pre></p> <p></p> <p>For the label decoding of the obtained prediction, we also need <code>imagenet_classes.txt</code> file, which contains the full list of the ImageNet classes.</p> <p>In this tutorial we will run the inference process for the converted PyTorch ResNet-50 model from the build (<code>samples/build</code>) directory:</p> <pre><code>./dnn/example_dnn_classification --model=../dnn/models/resnet50.onnx --input=../data/squirrel_cls.jpg --width=224 --height=224 --rgb=true --scale=\"0.003921569\" --mean=\"123.675 116.28 103.53\" --std=\"0.229 0.224 0.225\" --crop=true --initial_width=256 --initial_height=256 --classes=../data/dnn/classification_classes_ILSVRC2012.txt\n</code></pre> <p>Let's explore <code>classification.cpp</code> key points step by step:</p> <ol> <li>read the model with cv::dnn::readNet, initialize the network:</li> </ol> <pre><code>Net net = readNet(model, config, framework);\n</code></pre> <p>The <code>model</code> parameter value is taken from <code>--model</code> key. In our case, it is <code>resnet50.onnx</code>.</p> <ul> <li>preprocess input image:</li> </ul> <pre><code>if (rszWidth != 0 &amp;&amp; rszHeight != 0)\n{\n    resize(frame, frame, Size(rszWidth, rszHeight));\n}\n\n// Create a 4D blob from a frame\nblobFromImage(frame, blob, scale, Size(inpWidth, inpHeight), mean, swapRB, crop);\n\n// Check std values.\nif (std.val[0] != 0.0 &amp;&amp; std.val[1] != 0.0 &amp;&amp; std.val[2] != 0.0)\n{\n    // Divide blob by std.\n    divide(blob, std, blob);\n}\n</code></pre> <p>In this step we use cv::dnn::blobFromImage function to prepare model input. We set <code>Size(rszWidth, rszHeight)</code> with  <code>--initial_width=256 --initial_height=256</code> for the initial image resize as it's described in PyTorch ResNet inference pipeline.</p> <p>It should be noted that firstly in cv::dnn::blobFromImage mean value is subtracted and only then pixel values are multiplied by scale. Thus, we use <code>--mean=\"123.675 116.28 103.53\"</code>, which is equivalent to <code>[0.485, 0.456, 0.406]</code> multiplied by <code>255.0</code> to reproduce the original image preprocessing order for PyTorch classification models:</p> <pre><code>img /= 255.0\nimg -= [0.485, 0.456, 0.406]\nimg /= [0.229, 0.224, 0.225]\n</code></pre> <ul> <li>make forward pass:</li> </ul> <pre><code>net.setInput(blob);\nMat prob = net.forward();\n</code></pre> <ul> <li>process the prediction:</li> </ul> <pre><code>Point classIdPoint;\ndouble confidence;\nminMaxLoc(prob.reshape(1, 1), 0, &amp;confidence, 0, &amp;classIdPoint);\nint classId = classIdPoint.x;\n</code></pre> <p>Here we choose the most likely object class. The <code>classId</code> result for our case is 335 - fox squirrel, eastern fox squirrel, Sciurus niger:</p> <p></p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/","title":"Conversion of PyTorch Classification Models and Launch with OpenCV Python {#pytorch_cls_tutorial_dnn_conversion}","text":"<p>@prev_tutorial{tutorial_dnn_OCR} @next_tutorial{pytorch_cls_c_tutorial_dnn_conversion}</p> Original author Anastasia Murzova Compatibility OpenCV &gt;= 4.5"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/#goals","title":"Goals","text":"<p>In this tutorial you will learn how to: * convert PyTorch classification models into ONNX format * run converted PyTorch model with OpenCV Python API * obtain an evaluation of the PyTorch and OpenCV DNN models.</p> <p>We will explore the above-listed points by the example of the ResNet-50 architecture.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/#introduction","title":"Introduction","text":"<p>Let's briefly view the key concepts involved in the pipeline of PyTorch models transition with OpenCV API. The initial step in conversion of PyTorch models into cv.dnn.Net is model transferring into ONNX format. ONNX aims at the interchangeability of the neural networks between various frameworks. There is a built-in function in PyTorch for ONNX conversion: <code>torch.onnx.export</code>. Further the obtained <code>.onnx</code> model is passed into cv.dnn.readNetFromONNX.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/#requirements","title":"Requirements","text":"<p>To be able to experiment with the below code you will need to install a set of libraries. We will use a virtual environment with python3.7+ for this:</p> <pre><code>virtualenv -p /usr/bin/python3.7 &lt;env_dir_path&gt;\nsource &lt;env_dir_path&gt;/bin/activate\n</code></pre> <p>For OpenCV-Python building from source, follow the corresponding instructions from the @ref tutorial_py_table_of_contents_setup.</p> <p>Before you start the installation of the libraries, you can customize the requirements.txt, excluding or including (for example, <code>opencv-python</code>) some dependencies. The below line initiates requirements installation into the previously activated virtual environment:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/#practice","title":"Practice","text":"<p>In this part we are going to cover the following points: 1. create a classification model conversion pipeline and provide the inference 2. evaluate and test classification models</p> <p>If you'd like merely to run evaluation or test model pipelines, the \"Model Conversion Pipeline\" part can be skipped.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/#model-conversion-pipeline","title":"Model Conversion Pipeline","text":"<p>The code in this subchapter is located in the <code>dnn_model_runner</code> module and can be executed with the line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_resnet50\n</code></pre> <p>The following code contains the description of the below-listed steps: 1. instantiate PyTorch model 2. convert PyTorch model into <code>.onnx</code> 3. read the transferred network with OpenCV API 4. prepare input data 5. provide inference</p> <pre><code># initialize PyTorch ResNet-50 model\noriginal_model = models.resnet50(pretrained=True)\n\n# get the path to the converted into ONNX PyTorch model\nfull_model_path = get_pytorch_onnx_model(original_model)\n\n# read converted .onnx model with OpenCV API\nopencv_net = cv2.dnn.readNetFromONNX(full_model_path)\nprint(\"OpenCV model was successfully read. Layer IDs: \\n\", opencv_net.getLayerNames())\n\n# get preprocessed image\ninput_img = get_preprocessed_img(\"../data/squirrel_cls.jpg\")\n\n# get ImageNet labels\nimagenet_labels = get_imagenet_labels(\"../data/dnn/classification_classes_ILSVRC2012.txt\")\n\n# obtain OpenCV DNN predictions\nget_opencv_dnn_prediction(opencv_net, input_img, imagenet_labels)\n\n# obtain original PyTorch ResNet50 predictions\nget_pytorch_dnn_prediction(original_model, input_img, imagenet_labels)\n</code></pre> <p>To provide model inference we will use the below squirrel photo (under CC0 license) corresponding to ImageNet class ID 335: <pre><code>fox squirrel, eastern fox squirrel, Sciurus niger\n</code></pre></p> <p></p> <p>For the label decoding of the obtained prediction, we also need <code>imagenet_classes.txt</code> file, which contains the full list of the ImageNet classes.</p> <p>Let's go deeper into each step by the example of pretrained PyTorch ResNet-50: *  instantiate PyTorch ResNet-50 model:</p> <pre><code># initialize PyTorch ResNet-50 model\noriginal_model = models.resnet50(pretrained=True)\n</code></pre> <ul> <li>convert PyTorch model into ONNX:</li> </ul> <pre><code># define the directory for further converted model save\nonnx_model_path = \"models\"\n# define the name of further converted model\nonnx_model_name = \"resnet50.onnx\"\n\n# create directory for further converted model\nos.makedirs(onnx_model_path, exist_ok=True)\n\n# get full path to the converted model\nfull_model_path = os.path.join(onnx_model_path, onnx_model_name)\n\n# generate model input\ngenerated_input = Variable(\n    torch.randn(1, 3, 224, 224)\n)\n\n# model export into ONNX format\ntorch.onnx.export(\n    original_model,\n    generated_input,\n    full_model_path,\n    verbose=True,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    opset_version=11\n)\n</code></pre> <p>After the successful execution of the above code, we will get <code>models/resnet50.onnx</code>.</p> <ul> <li>read the transferred network with cv.dnn.readNetFromONNX passing the obtained in the previous step ONNX model into it:</li> </ul> <pre><code># read converted .onnx model with OpenCV API\nopencv_net = cv2.dnn.readNetFromONNX(full_model_path)\n</code></pre> <ul> <li>prepare input data:</li> </ul> <pre><code># read the image\ninput_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\ninput_img = input_img.astype(np.float32)\n\ninput_img = cv2.resize(input_img, (256, 256))\n\n# define preprocess parameters\nmean = np.array([0.485, 0.456, 0.406]) * 255.0\nscale = 1 / 255.0\nstd = [0.229, 0.224, 0.225]\n\n# prepare input blob to fit the model input:\n# 1. subtract mean\n# 2. scale to set pixel values from 0 to 1\ninput_blob = cv2.dnn.blobFromImage(\n    image=input_img,\n    scalefactor=scale,\n    size=(224, 224),  # img target size\n    mean=mean,\n    swapRB=True,  # BGR -&gt; RGB\n    crop=True  # center crop\n)\n# 3. divide by std\ninput_blob[0] /= np.asarray(std, dtype=np.float32).reshape(3, 1, 1)\n</code></pre> <p>In this step we read the image and prepare model input with cv.dnn.blobFromImage function, which returns 4-dimensional blob. It should be noted that firstly in cv.dnn.blobFromImage mean value is subtracted and only then pixel values are multiplied by scale. Thus, <code>mean</code> is multiplied by <code>255.0</code> to reproduce the original image preprocessing order:</p> <pre><code>img /= 255.0\nimg -= [0.485, 0.456, 0.406]\nimg /= [0.229, 0.224, 0.225]\n</code></pre> <ul> <li>OpenCV cv.dnn.Net inference:</li> </ul> <pre><code># set OpenCV DNN input\nopencv_net.setInput(preproc_img)\n\n# OpenCV DNN inference\nout = opencv_net.forward()\nprint(\"OpenCV DNN prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get the predicted class ID\nimagenet_class_id = np.argmax(out)\n\n# get confidence\nconfidence = out[0][imagenet_class_id]\nprint(\"* class ID: {}, label: {}\".format(imagenet_class_id, imagenet_labels[imagenet_class_id]))\nprint(\"* confidence: {:.4f}\".format(confidence))\n</code></pre> <p>After the above code execution we will get the following output:</p> <pre><code>OpenCV DNN prediction:\n* shape:  (1, 1000)\n* class ID: 335, label: fox squirrel, eastern fox squirrel, Sciurus niger\n* confidence: 14.8308\n</code></pre> <ul> <li>PyTorch ResNet-50 model inference:</li> </ul> <pre><code>original_net.eval()\npreproc_img = torch.FloatTensor(preproc_img)\n\n# inference\nout = original_net(preproc_img)\nprint(\"\\nPyTorch model prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get the predicted class ID\nimagenet_class_id = torch.argmax(out, axis=1).item()\nprint(\"* class ID: {}, label: {}\".format(imagenet_class_id, imagenet_labels[imagenet_class_id]))\n\n# get confidence\nconfidence = out[0][imagenet_class_id]\nprint(\"* confidence: {:.4f}\".format(confidence.item()))\n</code></pre> <p>After the above code launching we will get the following output:</p> <pre><code>PyTorch model prediction:\n* shape:  torch.Size([1, 1000])\n* class ID: 335, label: fox squirrel, eastern fox squirrel, Sciurus niger\n* confidence: 14.8308\n</code></pre> <p>The inference results of the original ResNet-50 model and cv.dnn.Net are equal. For the extended evaluation of the models we can use <code>py_to_py_cls</code> of the <code>dnn_model_runner</code> module. This module part will be described in the next subchapter.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/#evaluation-of-the-models","title":"Evaluation of the Models","text":"<p>The proposed in <code>samples/dnn</code> <code>dnn_model_runner</code> module allows to run the full evaluation pipeline on the ImageNet dataset and test execution for the following PyTorch classification models: * alexnet * vgg11 * vgg13 * vgg16 * vgg19 * resnet18 * resnet34 * resnet50 * resnet101 * resnet152 * squeezenet1_0 * squeezenet1_1 * resnext50_32x4d * resnext101_32x8d * wide_resnet50_2 * wide_resnet101_2</p> <p>This list can be also extended with further appropriate evaluation pipeline configuration.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/#evaluation-mode","title":"Evaluation Mode","text":"<p>The below line represents running of the module in the evaluation mode:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name &lt;pytorch_cls_model_name&gt;\n</code></pre> <p>Chosen from the list classification model will be read into OpenCV cv.dnn.Net object. Evaluation results of PyTorch and OpenCV models (accuracy, inference time, L1) will be written into the log file. Inference time values will be also depicted in a chart to generalize the obtained model information.</p> <p>Necessary evaluation configurations are defined in the test_config.py and can be modified in accordance with actual paths of data location:</p> <pre><code>@dataclass\nclass TestClsConfig:\n    batch_size: int = 50\n    frame_size: int = 224\n    img_root_dir: str = \"./ILSVRC2012_img_val\"\n    # location of image-class matching\n    img_cls_file: str = \"./val.txt\"\n    bgr_to_rgb: bool = True\n</code></pre> <p>To initiate the evaluation of the PyTorch ResNet-50, run the following line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name resnet50\n</code></pre> <p>After script launch, the log file with evaluation data will be generated in <code>dnn_model_runner/dnn_conversion/logs</code>:</p> <pre><code>The model PyTorch resnet50 was successfully obtained and converted to OpenCV DNN resnet50\n===== Running evaluation of the model with the following params:\n    * val data location: ./ILSVRC2012_img_val\n    * log file location: dnn_model_runner/dnn_conversion/logs/PyTorch_resnet50_log.txt\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/pytorch_cls_model_conversion_tutorial/#test-mode","title":"Test Mode","text":"<p>The below line represents running of the module in the test mode, namely it provides the steps for the model inference:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name &lt;pytorch_cls_model_name&gt; --test True --default_img_preprocess &lt;True/False&gt; --evaluate False\n</code></pre> <p>Here <code>default_img_preprocess</code> key defines whether you'd like to parametrize the model test process with some particular values or use the default values, for example, <code>scale</code>, <code>mean</code> or <code>std</code>.</p> <p>Test configuration is represented in test_config.py <code>TestClsModuleConfig</code> class:</p> <pre><code>@dataclass\nclass TestClsModuleConfig:\n    cls_test_data_dir: str = \"../data\"\n    test_module_name: str = \"classification\"\n    test_module_path: str = \"classification.py\"\n    input_img: str = os.path.join(cls_test_data_dir, \"squirrel_cls.jpg\")\n    model: str = \"\"\n\n    frame_height: str = str(TestClsConfig.frame_size)\n    frame_width: str = str(TestClsConfig.frame_size)\n    scale: str = \"1.0\"\n    mean: List[str] = field(default_factory=lambda: [\"0.0\", \"0.0\", \"0.0\"])\n    std: List[str] = field(default_factory=list)\n    crop: str = \"False\"\n    rgb: str = \"True\"\n    rsz_height: str = \"\"\n    rsz_width: str = \"\"\n    classes: str = os.path.join(cls_test_data_dir, \"dnn\", \"classification_classes_ILSVRC2012.txt\")\n</code></pre> <p>The default image preprocessing options are defined in default_preprocess_config.py. For instance:</p> <pre><code>BASE_IMG_SCALE_FACTOR = 1 / 255.0\nPYTORCH_RSZ_HEIGHT = 256\nPYTORCH_RSZ_WIDTH = 256\n\npytorch_resize_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(BASE_IMG_SCALE_FACTOR),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"True\",\n    \"rgb\": \"True\",\n    \"rsz_height\": str(PYTORCH_RSZ_HEIGHT),\n    \"rsz_width\": str(PYTORCH_RSZ_WIDTH)\n}\n</code></pre> <p>The basis of the model testing is represented in samples/dnn/classification.py.  <code>classification.py</code> can be executed autonomously with provided converted model in <code>--input</code> and populated parameters for cv.dnn.blobFromImage.</p> <p>To reproduce from scratch the described in \"Model Conversion Pipeline\" OpenCV steps with <code>dnn_model_runner</code> execute the below line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.classification.py_to_py_cls --model_name resnet50 --test True --default_img_preprocess True --evaluate False\n</code></pre> <p>The network prediction is depicted in the top left corner of the output window:</p> <p></p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/","title":"Conversion of TensorFlow Classification Models and Launch with OpenCV Python {#tf_cls_tutorial_dnn_conversion}","text":"Original author Anastasia Murzova Compatibility OpenCV &gt;= 4.5"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/#goals","title":"Goals","text":"<p>In this tutorial you will learn how to: * obtain frozen graphs of TensorFlow (TF) classification models * run converted TensorFlow model with OpenCV Python API * obtain an evaluation of the TensorFlow and OpenCV DNN models</p> <p>We will explore the above-listed points by the example of MobileNet architecture.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/#introduction","title":"Introduction","text":"<p>Let's briefly view the key concepts involved in the pipeline of TensorFlow models transition with OpenCV API. The initial step in conversion of TensorFlow models into cv.dnn.Net is obtaining the frozen TF model graph. Frozen graph defines the combination of the model graph structure with kept values of the required variables, for example, weights. Usually the frozen graph is saved in protobuf (<code>.pb</code>) files. After the model <code>.pb</code> file was generated it can be read with cv.dnn.readNetFromTensorflow function.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/#requirements","title":"Requirements","text":"<p>To be able to experiment with the below code you will need to install a set of libraries. We will use a virtual environment with python3.7+ for this:</p> <pre><code>virtualenv -p /usr/bin/python3.7 &lt;env_dir_path&gt;\nsource &lt;env_dir_path&gt;/bin/activate\n</code></pre> <p>For OpenCV-Python building from source, follow the corresponding instructions from the @ref tutorial_py_table_of_contents_setup.</p> <p>Before you start the installation of the libraries, you can customize the requirements.txt, excluding or including (for example, <code>opencv-python</code>) some dependencies. The below line initiates requirements installation into the previously activated virtual environment:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/#practice","title":"Practice","text":"<p>In this part we are going to cover the following points: 1. create a TF classification model conversion pipeline and provide the inference 2. evaluate and test TF classification models</p> <p>If you'd like merely to run evaluation or test model pipelines, the \"Model Conversion Pipeline\" tutorial part can be skipped.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/#model-conversion-pipeline","title":"Model Conversion Pipeline","text":"<p>The code in this subchapter is located in the <code>dnn_model_runner</code> module and can be executed with the line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_mobilenet\n</code></pre> <p>The following code contains the description of the below-listed steps: 1. instantiate TF model 2. create TF frozen graph 3. read TF frozen graph with OpenCV API 4. prepare input data 5. provide inference</p> <pre><code># initialize TF MobileNet model\noriginal_tf_model = MobileNet(\n    include_top=True,\n    weights=\"imagenet\"\n)\n\n# get TF frozen graph path\nfull_pb_path = get_tf_model_proto(original_tf_model)\n\n# read frozen graph with OpenCV API\nopencv_net = cv2.dnn.readNetFromTensorflow(full_pb_path)\nprint(\"OpenCV model was successfully read. Model layers: \\n\", opencv_net.getLayerNames())\n\n# get preprocessed image\ninput_img = get_preprocessed_img(\"../data/squirrel_cls.jpg\")\n\n# get ImageNet labels\nimagenet_labels = get_imagenet_labels(\"../data/dnn/classification_classes_ILSVRC2012.txt\")\n\n# obtain OpenCV DNN predictions\nget_opencv_dnn_prediction(opencv_net, input_img, imagenet_labels)\n\n# obtain TF model predictions\nget_tf_dnn_prediction(original_tf_model, input_img, imagenet_labels)\n</code></pre> <p>To provide model inference we will use the below squirrel photo (under CC0 license) corresponding to ImageNet class ID 335: <pre><code>fox squirrel, eastern fox squirrel, Sciurus niger\n</code></pre></p> <p></p> <p>For the label decoding of the obtained prediction, we also need <code>imagenet_classes.txt</code> file, which contains the full list of the ImageNet classes.</p> <p>Let's go deeper into each step by the example of pretrained TF MobileNet: * instantiate TF model:</p> <pre><code># initialize TF MobileNet model\noriginal_tf_model = MobileNet(\n    include_top=True,\n    weights=\"imagenet\"\n)\n</code></pre> <ul> <li>create TF frozen graph</li> </ul> <pre><code># define the directory for .pb model\npb_model_path = \"models\"\n\n# define the name of .pb model\npb_model_name = \"mobilenet.pb\"\n\n# create directory for further converted model\nos.makedirs(pb_model_path, exist_ok=True)\n\n# get model TF graph\ntf_model_graph = tf.function(lambda x: tf_model(x))\n\n# get concrete function\ntf_model_graph = tf_model_graph.get_concrete_function(\n    tf.TensorSpec(tf_model.inputs[0].shape, tf_model.inputs[0].dtype))\n\n# obtain frozen concrete function\nfrozen_tf_func = convert_variables_to_constants_v2(tf_model_graph)\n# get frozen graph\nfrozen_tf_func.graph.as_graph_def()\n\n# save full tf model\ntf.io.write_graph(graph_or_graph_def=frozen_tf_func.graph,\n                  logdir=pb_model_path,\n                  name=pb_model_name,\n                  as_text=False)\n</code></pre> <p>After the successful execution of the above code, we will get a frozen graph in <code>models/mobilenet.pb</code>.</p> <ul> <li>read TF frozen graph with with cv.dnn.readNetFromTensorflow passing the obtained in the previous step <code>mobilenet.pb</code> into it:</li> </ul> <pre><code># get TF frozen graph path\nfull_pb_path = get_tf_model_proto(original_tf_model)\n</code></pre> <ul> <li>prepare input data with cv2.dnn.blobFromImage function:</li> </ul> <pre><code># read the image\ninput_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\ninput_img = input_img.astype(np.float32)\n\n# define preprocess parameters\nmean = np.array([1.0, 1.0, 1.0]) * 127.5\nscale = 1 / 127.5\n\n# prepare input blob to fit the model input:\n# 1. subtract mean\n# 2. scale to set pixel values from 0 to 1\ninput_blob = cv2.dnn.blobFromImage(\n    image=input_img,\n    scalefactor=scale,\n    size=(224, 224),  # img target size\n    mean=mean,\n    swapRB=True,  # BGR -&gt; RGB\n    crop=True  # center crop\n)\nprint(\"Input blob shape: {}\\n\".format(input_blob.shape))\n</code></pre> <p>Please, pay attention at the preprocessing order in the cv2.dnn.blobFromImage function. Firstly, the mean value is subtracted and only then pixel values are multiplied by the defined scale. Therefore, to reproduce the image preprocessing pipeline from the TF <code>mobilenet.preprocess_input</code> function, we multiply <code>mean</code> by <code>127.5</code>.</p> <p>As a result, 4-dimensional <code>input_blob</code> was obtained:</p> <p><code>Input blob shape: (1, 3, 224, 224)</code></p> <ul> <li>provide OpenCV cv.dnn.Net inference:</li> </ul> <pre><code># set OpenCV DNN input\nopencv_net.setInput(preproc_img)\n\n# OpenCV DNN inference\nout = opencv_net.forward()\nprint(\"OpenCV DNN prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get the predicted class ID\nimagenet_class_id = np.argmax(out)\n\n# get confidence\nconfidence = out[0][imagenet_class_id]\nprint(\"* class ID: {}, label: {}\".format(imagenet_class_id, imagenet_labels[imagenet_class_id]))\nprint(\"* confidence: {:.4f}\\n\".format(confidence))\n</code></pre> <p>After the above code execution we will get the following output:</p> <pre><code>OpenCV DNN prediction:\n* shape:  (1, 1000)\n* class ID: 335, label: fox squirrel, eastern fox squirrel, Sciurus niger\n* confidence: 0.9525\n</code></pre> <ul> <li>provide TF MobileNet inference:</li> </ul> <pre><code># inference\npreproc_img = preproc_img.transpose(0, 2, 3, 1)\nprint(\"TF input blob shape: {}\\n\".format(preproc_img.shape))\n\nout = original_net(preproc_img)\n\nprint(\"\\nTensorFlow model prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get the predicted class ID\nimagenet_class_id = np.argmax(out)\nprint(\"* class ID: {}, label: {}\".format(imagenet_class_id, imagenet_labels[imagenet_class_id]))\n\n# get confidence\nconfidence = out[0][imagenet_class_id]\nprint(\"* confidence: {:.4f}\".format(confidence))\n</code></pre> <p>To fit TF model input, <code>input_blob</code> was transposed:</p> <pre><code>TF input blob shape: (1, 224, 224, 3)\n</code></pre> <p>TF inference results are the following:</p> <pre><code>TensorFlow model prediction:\n* shape:  (1, 1000)\n* class ID: 335, label: fox squirrel, eastern fox squirrel, Sciurus niger\n* confidence: 0.9525\n</code></pre> <p>As it can be seen from the experiments OpenCV and TF inference results are equal.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/#evaluation-of-the-models","title":"Evaluation of the Models","text":"<p>The proposed in <code>dnn/samples</code> <code>dnn_model_runner</code> module allows to run the full evaluation pipeline on the ImageNet dataset and test execution for the following TensorFlow classification models: * vgg16 * vgg19 * resnet50 * resnet101 * resnet152 * densenet121 * densenet169 * densenet201 * inceptionresnetv2 * inceptionv3 * mobilenet * mobilenetv2 * nasnetlarge * nasnetmobile * xception</p> <p>This list can be also extended with further appropriate evaluation pipeline configuration.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/#evaluation-mode","title":"Evaluation Mode","text":"<p>To below line represents running of the module in the evaluation mode:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls --model_name &lt;tf_cls_model_name&gt;\n</code></pre> <p>Chosen from the list classification model will be read into OpenCV <code>cv.dnn_Net</code> object. Evaluation results of TF and OpenCV models (accuracy, inference time, L1) will be written into the log file. Inference time values will be also depicted in a chart to generalize the obtained model information.</p> <p>Necessary evaluation configurations are defined in the test_config.py and can be modified in accordance with actual paths of data location::</p> <pre><code>@dataclass\nclass TestClsConfig:\n    batch_size: int = 50\n    frame_size: int = 224\n    img_root_dir: str = \"./ILSVRC2012_img_val\"\n    # location of image-class matching\n    img_cls_file: str = \"./val.txt\"\n    bgr_to_rgb: bool = True\n</code></pre> <p>The values from <code>TestClsConfig</code> can be customized in accordance with chosen model.</p> <p>To initiate the evaluation of the TensorFlow MobileNet, run the following line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls --model_name mobilenet\n</code></pre> <p>After script launch, the log file with evaluation data will be generated in <code>dnn_model_runner/dnn_conversion/logs</code>:</p> <pre><code>===== Running evaluation of the model with the following params:\n    * val data location: ./ILSVRC2012_img_val\n    * log file location: dnn_model_runner/dnn_conversion/logs/TF_mobilenet_log.txt\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_classification/tf_cls_model_conversion_tutorial/#test-mode","title":"Test Mode","text":"<p>The below line represents running of the module in the test mode, namely it provides the steps for the model inference:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls --model_name &lt;tf_cls_model_name&gt; --test True --default_img_preprocess &lt;True/False&gt; --evaluate False\n</code></pre> <p>Here <code>default_img_preprocess</code> key defines whether you'd like to parametrize the model test process with some particular values or use the default values, for example, <code>scale</code>, <code>mean</code> or <code>std</code>.</p> <p>Test configuration is represented in test_config.py <code>TestClsModuleConfig</code> class:</p> <pre><code>@dataclass\nclass TestClsModuleConfig:\n    cls_test_data_dir: str = \"../data\"\n    test_module_name: str = \"classification\"\n    test_module_path: str = \"classification.py\"\n    input_img: str = os.path.join(cls_test_data_dir, \"squirrel_cls.jpg\")\n    model: str = \"\"\n\n    frame_height: str = str(TestClsConfig.frame_size)\n    frame_width: str = str(TestClsConfig.frame_size)\n    scale: str = \"1.0\"\n    mean: List[str] = field(default_factory=lambda: [\"0.0\", \"0.0\", \"0.0\"])\n    std: List[str] = field(default_factory=list)\n    crop: str = \"False\"\n    rgb: str = \"True\"\n    rsz_height: str = \"\"\n    rsz_width: str = \"\"\n    classes: str = os.path.join(cls_test_data_dir, \"dnn\", \"classification_classes_ILSVRC2012.txt\")\n</code></pre> <p>The default image preprocessing options are defined in <code>default_preprocess_config.py</code>. For instance, for MobileNet:</p> <pre><code>tf_input_blob = {\n    \"mean\": [\"127.5\", \"127.5\", \"127.5\"],\n    \"scale\": str(1 / 127.5),\n    \"std\": [],\n    \"crop\": \"True\",\n    \"rgb\": \"True\"\n}\n</code></pre> <p>The basis of the model testing is represented in samples/dnn/classification.py. <code>classification.py</code> can be executed autonomously with provided converted model in <code>--input</code> and populated parameters for cv.dnn.blobFromImage.</p> <p>To reproduce from scratch the described in \"Model Conversion Pipeline\" OpenCV steps with <code>dnn_model_runner</code> execute the below line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.classification.py_to_py_cls --model_name mobilenet --test True --default_img_preprocess True --evaluate False\n</code></pre> <p>The network prediction is depicted in the top left corner of the output window:</p> <p></p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial/","title":"Conversion of TensorFlow Detection Models and Launch with OpenCV Python {#tf_det_tutorial_dnn_conversion}","text":"Original author Anastasia Murzova Compatibility OpenCV &gt;= 4.5"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial/#goals","title":"Goals","text":"<p>In this tutorial you will learn how to: * obtain frozen graphs of TensorFlow (TF) detection models * run converted TensorFlow model with OpenCV Python API</p> <p>We will explore the above-listed points by the example of SSD MobileNetV1.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial/#introduction","title":"Introduction","text":"<p>Let's briefly view the key concepts involved in the pipeline of TensorFlow models transition with OpenCV API. The initial step in the conversion of TensorFlow models into cv.dnn.Net is obtaining the frozen TF model graph. A frozen graph defines the combination of the model graph structure with kept values of the required variables, for example, weights. The frozen graph is saved in protobuf (<code>.pb</code>) files. There are special functions for reading <code>.pb</code> graphs in OpenCV: cv.dnn.readNetFromTensorflow and cv.dnn.readNet.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial/#requirements","title":"Requirements","text":"<p>To be able to experiment with the below code you will need to install a set of libraries. We will use a virtual environment with python3.7+ for this:</p> <pre><code>virtualenv -p /usr/bin/python3.7 &lt;env_dir_path&gt;\nsource &lt;env_dir_path&gt;/bin/activate\n</code></pre> <p>For OpenCV-Python building from source, follow the corresponding instructions from the @ref tutorial_py_table_of_contents_setup.</p> <p>Before you start the installation of the libraries, you can customize the requirements.txt, excluding or including (for example, <code>opencv-python</code>) some dependencies. The below line initiates requirements installation into the previously activated virtual environment:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial/#practice","title":"Practice","text":"<p>In this part we are going to cover the following points: 1. create a TF classification model conversion pipeline and provide the inference 2. provide the inference, process prediction results</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_detection/tf_det_model_conversion_tutorial/#model-preparation","title":"Model Preparation","text":"<p>The code in this subchapter is located in the <code>samples/dnn/dnn_model_runner</code> module and can be executed with the below line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.detection.py_to_py_ssd_mobilenet\n</code></pre> <p>The following code contains the steps of the TF SSD MobileNetV1 model retrieval:</p> <pre><code>    tf_model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\n    graph_extraction_dir = \"./\"\n    frozen_graph_path = extract_tf_frozen_graph(tf_model_name, graph_extraction_dir)\n    print(\"Frozen graph path for {}: {}\".format(tf_model_name, frozen_graph_path))\n</code></pre> <p>In <code>extract_tf_frozen_graph</code> function we extract the provided in model archive <code>frozen_inference_graph.pb</code> for its further processing:</p> <pre><code># define model archive name\ntf_model_tar = model_name + '.tar.gz'\n# define link to retrieve model archive\nmodel_link = DETECTION_MODELS_URL + tf_model_tar\n\ntf_frozen_graph_name = 'frozen_inference_graph'\n\ntry:\n    urllib.request.urlretrieve(model_link, tf_model_tar)\nexcept Exception:\n    print(\"TF {} was not retrieved: {}\".format(model_name, model_link))\n    return\n\nprint(\"TF {} was retrieved.\".format(model_name))\n\ntf_model_tar = tarfile.open(tf_model_tar)\nfrozen_graph_path = \"\"\n\nfor model_tar_elem in tf_model_tar.getmembers():\n    if tf_frozen_graph_name in os.path.basename(model_tar_elem.name):\n        tf_model_tar.extract(model_tar_elem, extracted_model_path)\n        frozen_graph_path = os.path.join(extracted_model_path, model_tar_elem.name)\n        break\ntf_model_tar.close()\n</code></pre> <p>After the successful execution of the above code we will get the following output:</p> <pre><code>TF ssd_mobilenet_v1_coco_2017_11_17 was retrieved.\nFrozen graph path for ssd_mobilenet_v1_coco_2017_11_17: ./ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb\n</code></pre> <p>To provide model inference we will use the below double-decker bus photo (under Pexels license):</p> <p></p> <p>To initiate the test process we need to provide an appropriate model configuration. We will use <code>ssd_mobilenet_v1_coco.config</code> from TensorFlow Object Detection API. TensorFlow Object Detection API framework contains helpful mechanisms for object detection model manipulations.</p> <p>We will use this configuration to provide a text graph representation. To generate <code>.pbtxt</code> we will use the corresponding <code>samples/dnn/tf_text_graph_ssd.py</code> script:</p> <pre><code>python tf_text_graph_ssd.py --input ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb --config ssd_mobilenet_v1_coco_2017_11_17/ssd_mobilenet_v1_coco.config --output ssd_mobilenet_v1_coco_2017_11_17.pbtxt\n</code></pre> <p>After successful execution <code>ssd_mobilenet_v1_coco_2017_11_17.pbtxt</code> will be created.</p> <p>Before we run <code>object_detection.py</code>, let's have a look at the default values for the SSD MobileNetV1 test process configuration. They are located in <code>models.yml</code>:</p> <pre><code>ssd_tf:\n  model: \"ssd_mobilenet_v1_coco_2017_11_17.pb\"\n  config: \"ssd_mobilenet_v1_coco_2017_11_17.pbtxt\"\n  mean: [0, 0, 0]\n  scale: 1.0\n  width: 300\n  height: 300\n  rgb: true\n  classes: \"object_detection_classes_coco.txt\"\n  sample: \"object_detection\"\n</code></pre> <p>To fetch these values we need to provide frozen graph <code>ssd_mobilenet_v1_coco_2017_11_17.pb</code> model and text graph <code>ssd_mobilenet_v1_coco_2017_11_17.pbtxt</code>:</p> <pre><code>python object_detection.py ssd_tf --input ../data/pexels_double_decker_bus.jpg\n</code></pre> <p>This line is equivalent to:</p> <pre><code>python object_detection.py --model ssd_mobilenet_v1_coco_2017_11_17.pb --config  ssd_mobilenet_v1_coco_2017_11_17.pbtxt  --input ../data/pexels_double_decker_bus.jpg --width 300 --height 300 --classes ../data/dnn/object_detection_classes_coco.txt\n</code></pre> <p>The result is:</p> <p></p> <p>There are several helpful parameters, which can be also customized for result corrections: threshold (<code>--thr</code>) and non-maximum suppression (<code>--nms</code>) values.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial/","title":"Conversion of PyTorch Segmentation Models and Launch with OpenCV {#pytorch_segm_tutorial_dnn_conversion}","text":""},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial/#goals","title":"Goals","text":"<p>In this tutorial you will learn how to: * convert PyTorch segmentation models * run converted PyTorch model with OpenCV * obtain an evaluation of the PyTorch and OpenCV DNN models</p> <p>We will explore the above-listed points by the example of the FCN ResNet-50 architecture.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial/#introduction","title":"Introduction","text":"<p>The key points involved in the transition pipeline of the PyTorch classification and segmentation models with OpenCV API are equal. The first step is model transferring into ONNX format with PyTorch <code>torch.onnx.export</code> built-in function. Further the obtained <code>.onnx</code> model is passed into cv.dnn.readNetFromONNX, which returns cv.dnn.Net object ready for DNN manipulations.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial/#practice","title":"Practice","text":"<p>In this part we are going to cover the following points: 1. create a segmentation model conversion pipeline and provide the inference 2. evaluate and test segmentation models</p> <p>If you'd like merely to run evaluation or test model pipelines, the \"Model Conversion Pipeline\" part can be skipped.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial/#model-conversion-pipeline","title":"Model Conversion Pipeline","text":"<p>The code in this subchapter is located in the <code>dnn_model_runner</code> module and can be executed with the line:</p> <p><code>python -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_fcnresnet50</code></p> <p>The following code contains the description of the below-listed steps: 1. instantiate PyTorch model 2. convert PyTorch model into <code>.onnx</code> 3. read the transferred network with OpenCV API 4. prepare input data 5. provide inference 6. get colored masks from predictions 7. visualize results</p> <pre><code># initialize PyTorch FCN ResNet-50 model\noriginal_model = models.segmentation.fcn_resnet50(pretrained=True)\n\n# get the path to the converted into ONNX PyTorch model\nfull_model_path = get_pytorch_onnx_model(original_model)\n\n# read converted .onnx model with OpenCV API\nopencv_net = cv2.dnn.readNetFromONNX(full_model_path)\nprint(\"OpenCV model was successfully read. Layer IDs: \\n\", opencv_net.getLayerNames())\n\n# get preprocessed image\nimg, input_img = get_processed_imgs(\"test_data/sem_segm/2007_000033.jpg\")\n\n# obtain OpenCV DNN predictions\nopencv_prediction = get_opencv_dnn_prediction(opencv_net, input_img)\n\n# obtain original PyTorch ResNet50 predictions\npytorch_prediction = get_pytorch_dnn_prediction(original_model, input_img)\n\npascal_voc_classes, pascal_voc_colors = read_colors_info(\"test_data/sem_segm/pascal-classes.txt\")\n\n# obtain colored segmentation masks\nopencv_colored_mask = get_colored_mask(img.shape, opencv_prediction, pascal_voc_colors)\npytorch_colored_mask = get_colored_mask(img.shape, pytorch_prediction, pascal_voc_colors)\n\n# obtain palette of PASCAL VOC colors\ncolor_legend = get_legend(pascal_voc_classes, pascal_voc_colors)\n\ncv2.imshow('PyTorch Colored Mask', pytorch_colored_mask)\ncv2.imshow('OpenCV DNN Colored Mask', opencv_colored_mask)\ncv2.imshow('Color Legend', color_legend)\n\ncv2.waitKey(0)\n</code></pre> <p>To provide the model inference we will use the below picture from the PASCAL VOC validation dataset:</p> <p></p> <p>The target segmented result is:</p> <p></p> <p>For the PASCAL VOC colors decoding and its mapping with the predicted masks, we also need <code>pascal-classes.txt</code> file, which contains the full list of the PASCAL VOC classes and corresponding colors.</p> <p>Let's go deeper into each code step by the example of pretrained PyTorch FCN ResNet-50: *  instantiate PyTorch FCN ResNet-50 model:</p> <pre><code># initialize PyTorch FCN ResNet-50 model\noriginal_model = models.segmentation.fcn_resnet50(pretrained=True)\n</code></pre> <ul> <li>convert PyTorch model into ONNX format:</li> </ul> <pre><code># define the directory for further converted model save\nonnx_model_path = \"models\"\n# define the name of further converted model\nonnx_model_name = \"fcnresnet50.onnx\"\n\n# create directory for further converted model\nos.makedirs(onnx_model_path, exist_ok=True)\n\n# get full path to the converted model\nfull_model_path = os.path.join(onnx_model_path, onnx_model_name)\n\n# generate model input to build the graph\ngenerated_input = Variable(\n    torch.randn(1, 3, 500, 500)\n)\n\n# model export into ONNX format\ntorch.onnx.export(\n    original_model,\n    generated_input,\n    full_model_path,\n    verbose=True,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    opset_version=11\n)\n</code></pre> <p>The code from this step does not differ from the classification conversion case. Thus, after the successful execution of the above code, we will get <code>models/fcnresnet50.onnx</code>.</p> <ul> <li>read the transferred network with cv.dnn.readNetFromONNX passing the obtained in the previous step ONNX model into it:</li> </ul> <pre><code># read converted .onnx model with OpenCV API\nopencv_net = cv2.dnn.readNetFromONNX(full_model_path)\n</code></pre> <ul> <li>prepare input data:</li> </ul> <pre><code># read the image\ninput_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\ninput_img = input_img.astype(np.float32)\n\n# target image sizes\nimg_height = input_img.shape[0]\nimg_width = input_img.shape[1]\n\n# define preprocess parameters\nmean = np.array([0.485, 0.456, 0.406]) * 255.0\nscale = 1 / 255.0\nstd = [0.229, 0.224, 0.225]\n\n# prepare input blob to fit the model input:\n# 1. subtract mean\n# 2. scale to set pixel values from 0 to 1\ninput_blob = cv2.dnn.blobFromImage(\n    image=input_img,\n    scalefactor=scale,\n    size=(img_width, img_height),  # img target size\n    mean=mean,\n    swapRB=True,  # BGR -&gt; RGB\n    crop=False  # center crop\n)\n# 3. divide by std\ninput_blob[0] /= np.asarray(std, dtype=np.float32).reshape(3, 1, 1)\n</code></pre> <p>In this step we read the image and prepare model input with cv2.dnn.blobFromImage function, which returns 4-dimensional blob. It should be noted that firstly in <code>cv2.dnn.blobFromImage</code> mean value is subtracted and only then pixel values are scaled. Thus, <code>mean</code> is multiplied by <code>255.0</code> to reproduce the original image preprocessing order:</p> <pre><code>img /= 255.0\nimg -= [0.485, 0.456, 0.406]\nimg /= [0.229, 0.224, 0.225]\n</code></pre> <ul> <li>OpenCV <code>cv.dnn_Net</code> inference:</li> </ul> <pre><code># set OpenCV DNN input\nopencv_net.setInput(preproc_img)\n\n# OpenCV DNN inference\nout = opencv_net.forward()\nprint(\"OpenCV DNN segmentation prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get IDs of predicted classes\nout_predictions = np.argmax(out[0], axis=0)\n</code></pre> <p>After the above code execution we will get the following output:</p> <pre><code>OpenCV DNN segmentation prediction:\n* shape:  (1, 21, 500, 500)\n</code></pre> <p>Each prediction channel out of 21, where 21 represents the number of PASCAL VOC classes, contains probabilities, which indicate how likely the pixel corresponds to the PASCAL VOC class.</p> <ul> <li>PyTorch FCN ResNet-50 model inference:</li> </ul> <pre><code>original_net.eval()\npreproc_img = torch.FloatTensor(preproc_img)\n\nwith torch.no_grad():\n    # obtaining unnormalized probabilities for each class\n    out = original_net(preproc_img)['out']\n\nprint(\"\\nPyTorch segmentation model prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get IDs of predicted classes\nout_predictions = out[0].argmax(dim=0)\n</code></pre> <p>After the above code launching we will get the following output:</p> <pre><code>PyTorch segmentation model prediction:\n* shape:  torch.Size([1, 21, 366, 500])\n</code></pre> <p>PyTorch prediction also contains probabilities corresponding to each class prediction.</p> <ul> <li>get colored masks from predictions:</li> </ul> <pre><code># convert mask values into PASCAL VOC colors\nprocessed_mask = np.stack([colors[color_id] for color_id in segm_mask.flatten()])\n\n# reshape mask into 3-channel image\nprocessed_mask = processed_mask.reshape(mask_height, mask_width, 3)\nprocessed_mask = cv2.resize(processed_mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST).astype(\n    np.uint8)\n\n# convert colored mask from BGR to RGB for compatibility with PASCAL VOC colors\nprocessed_mask = cv2.cvtColor(processed_mask, cv2.COLOR_BGR2RGB)\n</code></pre> <p>In this step we map the probabilities from segmentation masks with appropriate colors of the predicted classes. Let's have a look at the results:</p> <p></p> <p>For the extended evaluation of the models, we can use <code>py_to_py_segm</code> script of the <code>dnn_model_runner</code> module. This module part will be described in the next subchapter.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial/#evaluation-of-the-models","title":"Evaluation of the Models","text":"<p>The proposed in <code>dnn/samples</code> <code>dnn_model_runner</code> module allows to run the full evaluation pipeline on the PASCAL VOC dataset and test execution for the following PyTorch segmentation models: * FCN ResNet-50 * FCN ResNet-101</p> <p>This list can be also extended with further appropriate evaluation pipeline configuration.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial/#evaluation-mode","title":"Evaluation Mode","text":"<p>The below line represents running of the module in the evaluation mode:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_segm --model_name &lt;pytorch_segm_model_name&gt;\n</code></pre> <p>Chosen from the list segmentation model will be read into OpenCV <code>cv.dnn_Net</code> object. Evaluation results of PyTorch and OpenCV models (pixel accuracy, mean IoU, inference time) will be written into the log file. Inference time values will be also depicted in a chart to generalize the obtained model information.</p> <p>Necessary evaluation configurations are defined in the <code>test_config.py</code>:</p> <pre><code>@dataclass\nclass TestSegmConfig:\n    frame_size: int = 500\n    img_root_dir: str = \"./VOC2012\"\n    img_dir: str = os.path.join(img_root_dir, \"JPEGImages/\")\n    img_segm_gt_dir: str = os.path.join(img_root_dir, \"SegmentationClass/\")\n    # reduced val: https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/data/pascal/seg11valid.txt\n    segm_val_file: str = os.path.join(img_root_dir, \"ImageSets/Segmentation/seg11valid.txt\")\n    colour_file_cls: str = os.path.join(img_root_dir, \"ImageSets/Segmentation/pascal-classes.txt\")\n</code></pre> <p>These values can be modified in accordance with chosen model pipeline.</p> <p>To initiate the evaluation of the PyTorch FCN ResNet-50, run the following line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_segm --model_name fcnresnet50\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/pytorch_sem_segm_model_conversion_tutorial/#test-mode","title":"Test Mode","text":"<p>The below line represents running of the module in the test mode, which provides the steps for the model inference:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_segm --model_name &lt;pytorch_segm_model_name&gt; --test True --default_img_preprocess &lt;True/False&gt; --evaluate False\n</code></pre> <p>Here <code>default_img_preprocess</code> key defines whether you'd like to parametrize the model test process with some particular values or use the default values, for example, <code>scale</code>, <code>mean</code> or <code>std</code>.</p> <p>Test configuration is represented in <code>test_config.py</code> <code>TestSegmModuleConfig</code> class:</p> <pre><code>@dataclass\nclass TestSegmModuleConfig:\n    segm_test_data_dir: str = \"test_data/sem_segm\"\n    test_module_name: str = \"segmentation\"\n    test_module_path: str = \"segmentation.py\"\n    input_img: str = os.path.join(segm_test_data_dir, \"2007_000033.jpg\")\n    model: str = \"\"\n\n    frame_height: str = str(TestSegmConfig.frame_size)\n    frame_width: str = str(TestSegmConfig.frame_size)\n    scale: float = 1.0\n    mean: List[float] = field(default_factory=lambda: [0.0, 0.0, 0.0])\n    std: List[float] = field(default_factory=list)\n    crop: bool = False\n    rgb: bool = True\n    classes: str = os.path.join(segm_test_data_dir, \"pascal-classes.txt\")\n</code></pre> <p>The default image preprocessing options are defined in <code>default_preprocess_config.py</code>:</p> <pre><code>pytorch_segm_input_blob = {\n    \"mean\": [\"123.675\", \"116.28\", \"103.53\"],\n    \"scale\": str(1 / 255.0),\n    \"std\": [\"0.229\", \"0.224\", \"0.225\"],\n    \"crop\": \"False\",\n    \"rgb\": \"True\"\n}\n</code></pre> <p>The basis of the model testing is represented in <code>samples/dnn/segmentation.py</code>.  <code>segmentation.py</code> can be executed autonomously with provided converted model in <code>--input</code> and populated parameters for <code>cv2.dnn.blobFromImage</code>.</p> <p>To reproduce from scratch the described in \"Model Conversion Pipeline\" OpenCV steps with <code>dnn_model_runner</code> execute the below line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.pytorch.segmentation.py_to_py_segm --model_name fcnresnet50 --test True --default_img_preprocess True --evaluate False\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial/","title":"Conversion of TensorFlow Segmentation Models and Launch with OpenCV {#tf_segm_tutorial_dnn_conversion}","text":""},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial/#goals","title":"Goals","text":"<p>In this tutorial you will learn how to: * convert TensorFlow (TF) segmentation models * run converted TensorFlow model with OpenCV * obtain an evaluation of the TensorFlow and OpenCV DNN models</p> <p>We will explore the above-listed points by the example of the DeepLab architecture.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial/#introduction","title":"Introduction","text":"<p>The key concepts involved in the transition pipeline of the TensorFlow classification and segmentation models with OpenCV API are almost equal excepting the phase of graph optimization. The initial step in conversion of TensorFlow models into cv.dnn.Net is obtaining the frozen TF model graph. Frozen graph defines the combination of the model graph structure with kept values of the required variables, for example, weights. Usually the frozen graph is saved in protobuf (<code>.pb</code>) files. To read the generated segmentation model <code>.pb</code> file with cv.dnn.readNetFromTensorflow, it is needed to modify the graph with TF graph transform tool.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial/#practice","title":"Practice","text":"<p>In this part we are going to cover the following points: 1. create a TF classification model conversion pipeline and provide the inference 2. evaluate and test TF classification models</p> <p>If you'd like merely to run evaluation or test model pipelines, the \"Model Conversion Pipeline\" tutorial part can be skipped.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial/#model-conversion-pipeline","title":"Model Conversion Pipeline","text":"<p>The code in this subchapter is located in the <code>dnn_model_runner</code> module and can be executed with the line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.segmentation.py_to_py_deeplab\n</code></pre> <p>TensorFlow segmentation models can be found in TensorFlow Research Models section, which contains the implementations of models on the basis of published research papers. We will retrieve the archive with the pre-trained TF DeepLabV3 from the below link:</p> <pre><code>http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz\n</code></pre> <p>The full frozen graph obtaining pipeline is described in <code>deeplab_retrievement.py</code>:</p> <pre><code>def get_deeplab_frozen_graph():\n    # define model path to download\n    models_url = 'http://download.tensorflow.org/models/'\n    mobilenetv2_voctrainval = 'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz'\n\n    # construct model link to download\n    model_link = models_url + mobilenetv2_voctrainval\n\n    try:\n        urllib.request.urlretrieve(model_link, mobilenetv2_voctrainval)\n    except Exception:\n        print(\"TF DeepLabV3 was not retrieved: {}\".format(model_link))\n        return\n\n    tf_model_tar = tarfile.open(mobilenetv2_voctrainval)\n\n    # iterate the obtained model archive\n    for model_tar_elem in tf_model_tar.getmembers():\n        # check whether the model archive contains frozen graph\n        if TF_FROZEN_GRAPH_NAME in os.path.basename(model_tar_elem.name):\n            # extract frozen graph\n            tf_model_tar.extract(model_tar_elem, FROZEN_GRAPH_PATH)\n\n    tf_model_tar.close()\n</code></pre> <p>After running this script:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.segmentation.deeplab_retrievement\n</code></pre> <p>we will get <code>frozen_inference_graph.pb</code> in <code>deeplab/deeplabv3_mnv2_pascal_trainval</code>.</p> <p>Before going to the network loading with OpenCV it is needed to optimize the extracted <code>frozen_inference_graph.pb</code>. To optimize the graph we use TF <code>TransformGraph</code> with default parameters:</p> <pre><code>DEFAULT_OPT_GRAPH_NAME = \"optimized_frozen_inference_graph.pb\"\nDEFAULT_INPUTS = \"sub_7\"\nDEFAULT_OUTPUTS = \"ResizeBilinear_3\"\nDEFAULT_TRANSFORMS = \"remove_nodes(op=Identity)\" \\\n                     \" merge_duplicate_nodes\" \\\n                     \" strip_unused_nodes\" \\\n                     \" fold_constants(ignore_errors=true)\" \\\n                     \" fold_batch_norms\" \\\n                     \" fold_old_batch_norms\"\n\n\ndef optimize_tf_graph(\n        in_graph,\n        out_graph=DEFAULT_OPT_GRAPH_NAME,\n        inputs=DEFAULT_INPUTS,\n        outputs=DEFAULT_OUTPUTS,\n        transforms=DEFAULT_TRANSFORMS,\n        is_manual=True,\n        was_optimized=True\n):\n    # ...\n\n    tf_opt_graph = TransformGraph(\n        tf_graph,\n        inputs,\n        outputs,\n        transforms\n    )\n</code></pre> <p>To run graph optimization process, execute the line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.segmentation.tf_graph_optimizer --in_graph deeplab/deeplabv3_mnv2_pascal_trainval/frozen_inference_graph.pb\n</code></pre> <p>As a result <code>deeplab/deeplabv3_mnv2_pascal_trainval</code> directory will contain <code>optimized_frozen_inference_graph.pb</code>.</p> <p>After we have obtained the model graphs, let's examine the below-listed steps: 1. read TF <code>frozen_inference_graph.pb</code> graph 2. read optimized TF frozen graph with OpenCV API 3. prepare input data 4. provide inference 5. get colored masks from predictions 6. visualize results</p> <pre><code># get TF model graph from the obtained frozen graph\ndeeplab_graph = read_deeplab_frozen_graph(deeplab_frozen_graph_path)\n\n# read DeepLab frozen graph with OpenCV API\nopencv_net = cv2.dnn.readNetFromTensorflow(opt_deeplab_frozen_graph_path)\nprint(\"OpenCV model was successfully read. Model layers: \\n\", opencv_net.getLayerNames())\n\n# get processed image\noriginal_img_shape, tf_input_blob, opencv_input_img = get_processed_imgs(\"test_data/sem_segm/2007_000033.jpg\")\n\n# obtain OpenCV DNN predictions\nopencv_prediction = get_opencv_dnn_prediction(opencv_net, opencv_input_img)\n\n# obtain TF model predictions\ntf_prediction = get_tf_dnn_prediction(deeplab_graph, tf_input_blob)\n\n# get PASCAL VOC classes and colors\npascal_voc_classes, pascal_voc_colors = read_colors_info(\"test_data/sem_segm/pascal-classes.txt\")\n\n# obtain colored segmentation masks\nopencv_colored_mask = get_colored_mask(original_img_shape, opencv_prediction, pascal_voc_colors)\ntf_colored_mask = get_tf_colored_mask(original_img_shape, tf_prediction, pascal_voc_colors)\n\n# obtain palette of PASCAL VOC colors\ncolor_legend = get_legend(pascal_voc_classes, pascal_voc_colors)\n\ncv2.imshow('TensorFlow Colored Mask', tf_colored_mask)\ncv2.imshow('OpenCV DNN Colored Mask', opencv_colored_mask)\n\ncv2.imshow('Color Legend', color_legend)\n</code></pre> <p>To provide the model inference we will use the below picture from the PASCAL VOC validation dataset:</p> <p></p> <p>The target segmented result is:</p> <p></p> <p>For the PASCAL VOC colors decoding and its mapping with the predicted masks, we also need <code>pascal-classes.txt</code> file, which contains the full list of the PASCAL VOC classes and corresponding colors.</p> <p>Let's go deeper into each step by the example of pretrained TF DeepLabV3 MobileNetV2:</p> <ul> <li>read TF <code>frozen_inference_graph.pb</code> graph :</li> </ul> <pre><code># init deeplab model graph\nmodel_graph = tf.Graph()\n\n# obtain\nwith tf.io.gfile.GFile(frozen_graph_path, 'rb') as graph_file:\n    tf_model_graph = GraphDef()\ntf_model_graph.ParseFromString(graph_file.read())\n\nwith model_graph.as_default():\n    tf.import_graph_def(tf_model_graph, name='')\n</code></pre> <ul> <li>read optimized TF frozen graph with OpenCV API:</li> </ul> <pre><code># read DeepLab frozen graph with OpenCV API\nopencv_net = cv2.dnn.readNetFromTensorflow(opt_deeplab_frozen_graph_path)\n</code></pre> <ul> <li>prepare input data with cv2.dnn.blobFromImage function:</li> </ul> <pre><code># read the image\ninput_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\ninput_img = input_img.astype(np.float32)\n\n# preprocess image for TF model input\ntf_preproc_img = cv2.resize(input_img, (513, 513))\ntf_preproc_img = cv2.cvtColor(tf_preproc_img, cv2.COLOR_BGR2RGB)\n\n# define preprocess parameters for OpenCV DNN\nmean = np.array([1.0, 1.0, 1.0]) * 127.5\nscale = 1 / 127.5\n\n# prepare input blob to fit the model input:\n# 1. subtract mean\n# 2. scale to set pixel values from 0 to 1\ninput_blob = cv2.dnn.blobFromImage(\n    image=input_img,\n    scalefactor=scale,\n    size=(513, 513),  # img target size\n    mean=mean,\n    swapRB=True,  # BGR -&gt; RGB\n    crop=False  # center crop\n)\n</code></pre> <p>Please, pay attention at the preprocessing order in the <code>cv2.dnn.blobFromImage</code> function. Firstly, the mean value is subtracted and only then pixel values are multiplied by the defined scale. Therefore, to reproduce TF image preprocessing pipeline, we multiply <code>mean</code> by <code>127.5</code>. Another important point is image preprocessing for TF DeepLab. To pass the image into TF model we need only to construct an appropriate shape, the rest image preprocessing is described in feature_extractor.py and will be invoked automatically.</p> <ul> <li>provide OpenCV <code>cv.dnn_Net</code> inference:</li> </ul> <pre><code># set OpenCV DNN input\nopencv_net.setInput(preproc_img)\n\n# OpenCV DNN inference\nout = opencv_net.forward()\nprint(\"OpenCV DNN segmentation prediction: \\n\")\nprint(\"* shape: \", out.shape)\n\n# get IDs of predicted classes\nout_predictions = np.argmax(out[0], axis=0)\n</code></pre> <p>After the above code execution we will get the following output:</p> <pre><code>OpenCV DNN segmentation prediction:\n* shape:  (1, 21, 513, 513)\n</code></pre> <p>Each prediction channel out of 21, where 21 represents the number of PASCAL VOC classes, contains probabilities, which indicate how likely the pixel corresponds to the PASCAL VOC class.</p> <ul> <li>provide TF model inference:</li> </ul> <pre><code>preproc_img = np.expand_dims(preproc_img, 0)\n\n# init TF session\ntf_session = Session(graph=model_graph)\n\ninput_tensor_name = \"ImageTensor:0\",\noutput_tensor_name = \"SemanticPredictions:0\"\n\n# run inference\nout = tf_session.run(\n    output_tensor_name,\n    feed_dict={input_tensor_name: [preproc_img]}\n)\n\nprint(\"TF segmentation model prediction: \\n\")\nprint(\"* shape: \", out.shape)\n</code></pre> <p>TF inference results are the following:</p> <pre><code>TF segmentation model prediction:\n* shape:  (1, 513, 513)\n</code></pre> <p>TensorFlow prediction contains the indexes of corresponding PASCAL VOC classes.</p> <ul> <li>transform OpenCV prediction into colored mask:</li> </ul> <pre><code>mask_height = segm_mask.shape[0]\nmask_width = segm_mask.shape[1]\n\nimg_height = original_img_shape[0]\nimg_width = original_img_shape[1]\n\n# convert mask values into PASCAL VOC colors\nprocessed_mask = np.stack([colors[color_id] for color_id in segm_mask.flatten()])\n\n# reshape mask into 3-channel image\nprocessed_mask = processed_mask.reshape(mask_height, mask_width, 3)\nprocessed_mask = cv2.resize(processed_mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST).astype(\n    np.uint8)\n\n# convert colored mask from BGR to RGB\nprocessed_mask = cv2.cvtColor(processed_mask, cv2.COLOR_BGR2RGB)\n</code></pre> <p>In this step we map the probabilities from segmentation masks with appropriate colors of the predicted classes. Let's have a look at the results:</p> <p></p> <p></p> <ul> <li>transform TF prediction into colored mask:</li> </ul> <pre><code>colors = np.array(colors)\nprocessed_mask = colors[segm_mask[0]]\n\nimg_height = original_img_shape[0]\nimg_width = original_img_shape[1]\n\nprocessed_mask = cv2.resize(processed_mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST).astype(\n    np.uint8)\n\n# convert colored mask from BGR to RGB for compatibility with PASCAL VOC colors\nprocessed_mask = cv2.cvtColor(processed_mask, cv2.COLOR_BGR2RGB)\n</code></pre> <p>The result is:</p> <p></p> <p>As a result, we get two equal segmentation masks.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial/#evaluation-of-the-models","title":"Evaluation of the Models","text":"<p>The proposed in <code>dnn/samples</code> <code>dnn_model_runner</code> module allows to run the full evaluation pipeline on the PASCAL VOC dataset and test execution for the DeepLab MobileNet model.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial/#evaluation-mode","title":"Evaluation Mode","text":"<p>To below line represents running of the module in the evaluation mode:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.segmentation.py_to_py_segm\n</code></pre> <p>The model will be read into OpenCV <code>cv.dnn_Net</code> object. Evaluation results of TF and OpenCV models  (pixel accuracy, mean IoU, inference time) will be written into the log file. Inference time values will be also depicted in a chart to generalize the obtained model information.</p> <p>Necessary evaluation configurations are defined in the <code>test_config.py</code>:</p> <pre><code>@dataclass\nclass TestSegmConfig:\n    frame_size: int = 500\n    img_root_dir: str = \"./VOC2012\"\n    img_dir: str = os.path.join(img_root_dir, \"JPEGImages/\")\n    img_segm_gt_dir: str = os.path.join(img_root_dir, \"SegmentationClass/\")\n    # reduced val: https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/data/pascal/seg11valid.txt\n    segm_val_file: str = os.path.join(img_root_dir, \"ImageSets/Segmentation/seg11valid.txt\")\n    colour_file_cls: str = os.path.join(img_root_dir, \"ImageSets/Segmentation/pascal-classes.txt\")\n</code></pre> <p>These values can be modified in accordance with chosen model pipeline.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/dnn/dnn_pytorch_tf_segmentation/tf_sem_segm_model_conversion_tutorial/#test-mode","title":"Test Mode","text":"<p>The below line represents running of the module in the test mode, which provides the steps for the model inference:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.segmentation.py_to_py_segm --test True --default_img_preprocess &lt;True/False&gt; --evaluate False\n</code></pre> <p>Here <code>default_img_preprocess</code> key defines whether you'd like to parametrize the model test process with some particular values or use the default values, for example, <code>scale</code>, <code>mean</code> or <code>std</code>.</p> <p>Test configuration is represented in <code>test_config.py</code> <code>TestSegmModuleConfig</code> class:</p> <pre><code>@dataclass\nclass TestSegmModuleConfig:\n    segm_test_data_dir: str = \"test_data/sem_segm\"\n    test_module_name: str = \"segmentation\"\n    test_module_path: str = \"segmentation.py\"\n    input_img: str = os.path.join(segm_test_data_dir, \"2007_000033.jpg\")\n    model: str = \"\"\n\n    frame_height: str = str(TestSegmConfig.frame_size)\n    frame_width: str = str(TestSegmConfig.frame_size)\n    scale: float = 1.0\n    mean: List[float] = field(default_factory=lambda: [0.0, 0.0, 0.0])\n    std: List[float] = field(default_factory=list)\n    crop: bool = False\n    rgb: bool = True\n    classes: str = os.path.join(segm_test_data_dir, \"pascal-classes.txt\")\n</code></pre> <p>The default image preprocessing options are defined in <code>default_preprocess_config.py</code>:</p> <pre><code>tf_segm_input_blob = {\n    \"scale\": str(1 / 127.5),\n    \"mean\": [\"127.5\", \"127.5\", \"127.5\"],\n    \"std\": [],\n    \"crop\": \"False\",\n    \"rgb\": \"True\"\n}\n</code></pre> <p>The basis of the model testing is represented in <code>samples/dnn/segmentation.py</code>.  <code>segmentation.py</code> can be executed autonomously with provided converted model in <code>--input</code> and populated parameters for <code>cv2.dnn.blobFromImage</code>.</p> <p>To reproduce from scratch the described in \"Model Conversion Pipeline\" OpenCV steps with <code>dnn_model_runner</code> execute the below line:</p> <pre><code>python -m dnn_model_runner.dnn_conversion.tf.segmentation.py_to_py_segm --test True --default_img_preprocess True --evaluate False\n</code></pre>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/","title":"Extract horizontal and vertical lines by using morphological operations {#tutorial_morph_lines_detection}","text":"<p>@tableofcontents</p> <p>@prev_tutorial{tutorial_hitOrMiss} @next_tutorial{tutorial_pyramids}</p> Original author Theodore Tsesmelis Compatibility OpenCV &gt;= 3.0"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#goal","title":"Goal","text":"<p>In this tutorial you will learn how to:</p> <ul> <li> <p>Apply two very common morphology operators (i.e. Dilation and Erosion), with the creation of custom kernels, in order to extract straight lines on the horizontal and vertical axes. For this purpose, you will use the following OpenCV functions:</p> <ul> <li>erode()</li> <li>dilate()</li> <li>getStructuringElement()</li> </ul> <p>in an example where your goal will be to extract the music notes from a music sheet.</p> </li> </ul>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#theory","title":"Theory","text":""},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#morphology-operations","title":"Morphology Operations","text":"<p>Morphology is a set of image processing operations that process images based on predefined structuring elements known also as kernels. The value of each pixel in the output image is based on a comparison of the corresponding pixel in the input image with its neighbors. By choosing the size and shape of the kernel, you can construct a morphological operation that is sensitive to specific shapes regarding the input image.</p> <p>Two of the most basic morphological operations are dilation and erosion. Dilation adds pixels to the boundaries of the object in an image, while erosion does exactly the opposite. The amount of pixels added or removed, respectively depends on the size and shape of the structuring element used to process the image. In general the rules followed from these two operations have as follows:</p> <ul> <li> <p>Dilation: The value of the output pixel is the maximum value of all the pixels that fall within the structuring element's size and shape. For example in a binary image, if any of the pixels of the input image falling within the range of the kernel is set to the value 1, the corresponding pixel of the output image will be set to 1 as well. The latter applies to any type of image (e.g. grayscale, bgr, etc).</p> <p></p> <p></p> </li> <li> <p>Erosion: The vice versa applies for the erosion operation. The value of the output pixel is the minimum value of all the pixels that fall within the structuring element's size and shape. Look the at the example figures below:</p> <p></p> <p></p> </li> </ul>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#structuring-elements","title":"Structuring Elements","text":"<p>As it can be seen above and in general in any morphological operation the structuring element used to probe the input image, is the most important part.</p> <p>A structuring element is a matrix consisting of only 0's and 1's that can have any arbitrary shape and size. Typically are much smaller than the image being processed, while the pixels with values of 1 define the neighborhood. The center pixel of the structuring element, called the origin, identifies the pixel of interest -- the pixel being processed.</p> <p>For example, the following illustrates a diamond-shaped structuring element of 7x7 size.</p> <p></p> <p>A structuring element can have many common shapes, such as lines, diamonds, disks, periodic lines, and circles and sizes. You typically choose a structuring element the same size and shape as the objects you want to process/extract in the input image. For example, to find lines in an image, create a linear structuring element as you will see later.</p>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#code","title":"Code","text":"<p>This tutorial code's is shown lines below.</p> <p>@add_toggle_cpp You can also download it from here. @include samples/cpp/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.cpp @end_toggle</p> <p>@add_toggle_java You can also download it from here. @include samples/java/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.java @end_toggle</p> <p>@add_toggle_python You can also download it from here. @include samples/python/tutorial_code/imgProc/morph_lines_detection/morph_lines_detection.py @end_toggle</p>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#explanation-result","title":"Explanation / Result","text":"<p>Get image from here .</p>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#load-image","title":"Load Image","text":"<p>@add_toggle_cpp @snippet samples/cpp/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.cpp load_image @end_toggle</p> <p>@add_toggle_java @snippet samples/java/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.java load_image @end_toggle</p> <p>@add_toggle_python @snippet samples/python/tutorial_code/imgProc/morph_lines_detection/morph_lines_detection.py load_image @end_toggle</p> <p></p>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#grayscale","title":"Grayscale","text":"<p>@add_toggle_cpp @snippet samples/cpp/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.cpp gray @end_toggle</p> <p>@add_toggle_java @snippet samples/java/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.java gray @end_toggle</p> <p>@add_toggle_python @snippet samples/python/tutorial_code/imgProc/morph_lines_detection/morph_lines_detection.py gray @end_toggle</p> <p></p>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#grayscale-to-binary-image","title":"Grayscale to Binary image","text":"<p>@add_toggle_cpp @snippet samples/cpp/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.cpp bin @end_toggle</p> <p>@add_toggle_java @snippet samples/java/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.java bin @end_toggle</p> <p>@add_toggle_python @snippet samples/python/tutorial_code/imgProc/morph_lines_detection/morph_lines_detection.py bin @end_toggle</p> <p></p>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#output-images","title":"Output images","text":"<p>Now we are ready to apply morphological operations in order to extract the horizontal and vertical lines and as a consequence to separate the music notes from the music sheet, but first let's initialize the output images that we will use for that reason:</p> <p>@add_toggle_cpp @snippet samples/cpp/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.cpp init @end_toggle</p> <p>@add_toggle_java @snippet samples/java/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.java init @end_toggle</p> <p>@add_toggle_python @snippet samples/python/tutorial_code/imgProc/morph_lines_detection/morph_lines_detection.py init @end_toggle</p>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#structure-elements","title":"Structure elements","text":"<p>As we specified in the theory in order to extract the object that we desire, we need to create the corresponding structure element. Since  we want to extract the horizontal lines, a corresponding structure element for that purpose will have the following shape:  and in the source code this is represented by the following code snippet:</p> <p>@add_toggle_cpp @snippet samples/cpp/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.cpp horiz @end_toggle</p> <p>@add_toggle_java @snippet samples/java/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.java horiz @end_toggle</p> <p>@add_toggle_python @snippet samples/python/tutorial_code/imgProc/morph_lines_detection/morph_lines_detection.py horiz @end_toggle</p> <p></p> <p>The same applies for the vertical lines, with the corresponding structure element:  and again this is represented as follows:</p> <p>@add_toggle_cpp @snippet samples/cpp/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.cpp vert @end_toggle</p> <p>@add_toggle_java @snippet samples/java/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.java vert @end_toggle</p> <p>@add_toggle_python @snippet samples/python/tutorial_code/imgProc/morph_lines_detection/morph_lines_detection.py vert @end_toggle</p> <p></p>"},{"location":"opencv_docs/opencv/doc/tutorials/imgproc/morph_lines_detection/morph_lines_detection/#refine-edges-result","title":"Refine edges / Result","text":"<p>As you can see we are almost there. However, at that point you will notice that the edges of the notes are a bit rough. For that reason we need to refine the edges in order to obtain a smoother result:</p> <p>@add_toggle_cpp @snippet samples/cpp/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.cpp smooth @end_toggle</p> <p>@add_toggle_java @snippet samples/java/tutorial_code/ImgProc/morph_lines_detection/Morphology_3.java smooth @end_toggle</p> <p>@add_toggle_python @snippet samples/python/tutorial_code/imgProc/morph_lines_detection/morph_lines_detection.py smooth @end_toggle</p> <p></p>"},{"location":"opencv_docs/opencv/modules/dnn/src/webnn/","title":"Index","text":""},{"location":"opencv_docs/opencv/modules/dnn/src/webnn/#build-instructions","title":"Build Instructions","text":""},{"location":"opencv_docs/opencv/modules/dnn/src/webnn/#build-webnn-native-and-set-the-environment-variable","title":"Build WebNN-native and set the environment variable","text":"<p>Refer to WebNN's build instructions to complete the build of WebNN-native.</p> <p>Set environment variable <code>WEBNN_NATIVE_DIR</code> to enable native DNN_BACKEND_WEBNN build: <code>export WEBNN_NATIVE_DIR=${PATH_TO_WebNN}</code>. Please let <code>WEBNN_NATIVE_DIR</code> points the output directory of webnn-native build (e.g. webnn-native/out/Release).</p>"},{"location":"opencv_docs/opencv/modules/dnn/src/webnn/#test-native-dnn_backend_webnn-backend","title":"Test native DNN_BACKEND_WEBNN backend","text":"<p>Add -DWITH_WEBNN=ON to the cmake command to build the WebNN module such as: <code>cmake -DWITH_WEBNN=ON ../opencv</code> (according to the @ref tutorial_linux_install)</p>"},{"location":"opencv_docs/opencv/modules/gapi/doc/10-hld-overview/","title":"High-level design overview {#gapi_hld}","text":"<ul> <li>High-level design overview {#gapi_hld}</li> <li>G-API High-level design overview</li> <li>API layer {#gapi_api_layer}</li> <li>Graph compiler layer {#gapi_compiler}</li> <li>Backends layer {#gapi_backends}</li> <li>Graph execution {#gapi_compiled}</li> </ul>"},{"location":"opencv_docs/opencv/modules/gapi/doc/10-hld-overview/#g-api-high-level-design-overview","title":"G-API High-level design overview","text":"<p>G-API is a heterogeneous framework and provides an unified API to program image processing pipelines with a number of supported backends.</p> <p>The key design idea is to keep pipeline code itself platform-neutral while specifying which kernels to use and which devices to utilize using extra parameters at graph compile (configuration) time. This requirement has led to the following architecture:</p> <p></p> <p>There are three layers in this architecture: * API Layer -- this is the top layer, which implements G-API   public interface, its building blocks and semantics.   When user constructs a pipeline with G-API, he interacts with this   layer directly, and the entities the user operates on (like cv::GMat   or cv::GComputation) are provided by this layer. * Graph Compiler Layer -- this is the intermediate layer which   unrolls user computation into a graph and then applies a number of   transformations to it (e.g. optimizations). This layer is built atop   of ADE Framework. * Backends Layer -- this is the lowest level layer, which lists a   number of Backends. In contrast with the above two layers,   backends are highly coupled with low-level platform details, with   every backend standing for every platform. A backend operates on a   processed graph (coming from the graph compiler) and executes this   graph optimally for a specific platform or device.</p>"},{"location":"opencv_docs/opencv/modules/gapi/doc/10-hld-overview/#api-layer-gapi_api_layer","title":"API layer {#gapi_api_layer}","text":"<p>API layer is what user interacts with when defining and using a pipeline (a Computation in G-API terms). API layer defines a set of G-API dynamic objects which can be used as inputs, outputs, and intermediate data objects within a graph: * cv::GMat * cv::GScalar * cv::GArray (template class)</p> <p>API layer specifies a list of Operations which are defined on these data objects -- so called kernels. See G-API core and imgproc namespaces for details on which operations G-API provides by default.</p> <p>G-API is not limited to these operations only -- users can define their own kernels easily using a special macro G_TYPED_KERNEL().</p> <p>API layer is also responsible for marshalling and storing operation parameters on pipeline creation. In addition to the aforementioned G-API dynamic objects, operations may also accept arbitrary parameters (more on this here), so API layer captures its values and stores internally upon the moment of execution.</p> <p>Finally, cv::GComputation and cv::GCompiled are the remaining important components of API layer. The former wraps a series of G-API expressions into an object (graph), and the latter is a product of graph compilation (see this chapter for details).</p>"},{"location":"opencv_docs/opencv/modules/gapi/doc/10-hld-overview/#graph-compiler-layer-gapi_compiler","title":"Graph compiler layer {#gapi_compiler}","text":"<p>Every G-API computation is compiled before it executes. Compilation process is triggered in two ways: * implicitly, when cv::GComputation::apply() is used. In this case,   graph compilation is then immediately followed by execution. * explicitly, when cv::GComputation::compile() is used. In this case,   a cv::GCompiled object is returned which then can be invoked as a   C++ functor.</p> <p>The first way is recommended for cases when input data format is not known in advance -- e.g. when it comes from an arbitrary input file. The second way is recommended for deployment (production) scenarios where input data characteristics are usually predefined.</p> <p>Graph compilation process is built atop of ADE Framework. Initially, a bipartite graph is generated from expressions captured by API layer. This graph contains nodes of two types: Data and Operations. Graph always starts and ends with a Data node(s), with Operations nodes in-between. Every Operation node has inputs and outputs, both are Data nodes.</p> <p>After the initial graph is generated, it is actually processed by a number of graph transformations, called passes. ADE Framework acts as a compiler pass management engine, and passes are written specifically for G-API.</p> <p>There are different passes which check graph validity, refine details on operations and data, organize nodes into clusters (\"Islands\") based on affinity or user-specified regioning[TBD], and more. Backends also are able to inject backend-specific passes into the compilation process, see more on this in the dedicated chapter.</p> <p>Result of graph compilation is a compiled object, represented by class cv::GCompiled. A new cv::GCompiled object is always created regardless if there was an explicit or implicit compilation request (see above). Actual graph execution happens within cv::GCompiled and is determined by backends which participated in the graph compilation.</p> <p>@sa cv::GComputation::apply(), cv::GComputation::compile(), cv::GCompiled</p>"},{"location":"opencv_docs/opencv/modules/gapi/doc/10-hld-overview/#backends-layer-gapi_backends","title":"Backends layer {#gapi_backends}","text":"<p>The above diagram lists two backends, OpenCV and Fluid. OpenCV is so-called \"reference backend\", which implements G-API operations using plain old OpenCV functions. This backend is useful for prototyping on a familiar development system. Fluid is a plugin for cache-efficient execution on CPU -- it implements a different execution policy and operates with its own, special kernels. Fluid backend allows to achieve less memory footprint and better memory locality when running on CPU.</p> <p>There may be more backends available, e.g. Halide, OpenCL, etc. -- G-API provides an uniform internal API to develop backends so any enthusiast or a company are free to scale G-API on a new platform or accelerator. In terms of OpenCV infrastructure, every new backend is a new distinct OpenCV module, which extends G-API when build as a part of OpenCV.</p>"},{"location":"opencv_docs/opencv/modules/gapi/doc/10-hld-overview/#graph-execution-gapi_compiled","title":"Graph execution {#gapi_compiled}","text":"<p>The way graph executed is defined by backends selected for compilation. In fact, every backend builds its own execution script as the final stage of graph compilation process, when an executable (compiled) object is being generated. For example, in OpenCV backend, this script is just a topologically-sorted sequence of OpenCV functions to call; for Fluid backend, it is a similar thing -- a topologically sorted list of Agents processing lines of input on every iteration.</p> <p>Graph execution is triggered in two ways: * via cv::GComputation::apply(), with graph compiled in-place exactly   for the given input data; * via cv::GCompiled::operator()(), when the graph has been precompiled.</p> <p>Both methods are polimorphic and take a variadic number of arguments, with validity checks performed in runtime. If a number, shapes, and formats of passed data objects differ from expected, a runtime exception is thrown. G-API also provides typed wrappers to move these checks to the compile time -- see <code>cv::GComputationT&lt;&gt;</code>.</p> <p>G-API graph execution is declared stateless -- it means that a compiled functor (cv::GCompiled) acts like a pure C++ function and provides the same result for the same set of input arguments.</p> <p>Both execution methods take \\f$N+M\\f$ parameters, where \\f$N\\f$ is a number of inputs, and \\f$M\\f$ is a number of outputs on which a cv::GComputation is defined. Note that while G-API types (cv::GMat, etc) are used in definition, the execution methods accept OpenCV's traditional data types (like cv::Mat) which hold actual data -- see table in parameter marshalling.</p> <p>@sa @ref gapi_impl, @ref gapi_kernel_api</p>"},{"location":"opencv_docs/opencv/modules/gapi/doc/slides/","title":"G-API Overview","text":"<p>This is the latest overview slide deck on G-API.</p>"},{"location":"opencv_docs/opencv/modules/gapi/doc/slides/#prerequisites","title":"Prerequisites","text":"<ul> <li>Emacs v24 or higher;</li> <li>Org-mode 8.2.10;</li> <li><code>pdflatex</code>;</li> <li><code>texlive-latex-recommended</code> (Beamer package);</li> <li><code>texlive-font-utils</code> (<code>epstopdf</code>);</li> <li><code>wget</code> (for <code>get_sty.sh</code>).</li> </ul>"},{"location":"opencv_docs/opencv/modules/gapi/doc/slides/#building","title":"Building","text":"<ol> <li>Download and build the Metropolis theme with the script:</li> </ol> <pre><code>$ ./get_sty.sh\n</code></pre> <ol> <li>Now open <code>gapi_overview.org</code> with Emacs and press <code>C-c C-e l P</code>.</li> </ol>"},{"location":"opencv_docs/opencv/modules/gapi/src/api/","title":"Index","text":"<p>This directory contains implementation of G-API frontend (public API classes).</p>"},{"location":"opencv_docs/opencv/modules/gapi/src/backends/","title":"Index","text":"<p>This directory contains various G-API backends, which provide scheduling logic and kernel implementations for specific targets.</p>"},{"location":"opencv_docs/opencv/modules/gapi/src/compiler/","title":"Index","text":"<p>This directory contains G-API graph compiler logic.</p>"},{"location":"opencv_docs/opencv/modules/js/perf/","title":"OpenCV.js Performance Test","text":""},{"location":"opencv_docs/opencv/modules/js/perf/#nodejs-version","title":"Node.js Version","text":""},{"location":"opencv_docs/opencv/modules/js/perf/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>node.js, npm: Make sure you have installed these beforehand with the system package manager.</p> </li> <li> <p>Benchmark.js: Make sure you have installed Benchmark.js by npm before use. Please run <code>npm install</code> in the directory <code>&lt;build_dir&gt;/bin/perf</code>.</p> </li> </ol>"},{"location":"opencv_docs/opencv/modules/js/perf/#how-to-use","title":"How to Use","text":"<p>For example, if you want to test the performance of cvtColor, please run <code>perf_cvtcolor.js</code> by node in terminal:</p> <pre><code>node perf_cvtcolor.js\n</code></pre> <p>All tests of cvtColor will be run by above command.</p> <p>If you just want to run one specific case, please use <code>--test_param_filter=\"()\"</code> flag, like:</p> <pre><code>node perf_cvtcolor.js --test_param_filter=\"(1920x1080, COLOR_BGR2GRAY)\"\n</code></pre>"},{"location":"opencv_docs/opencv/modules/js/perf/#browser-version","title":"Browser Version","text":""},{"location":"opencv_docs/opencv/modules/js/perf/#how-to-use_1","title":"How to Use","text":"<p>To run performance tests, please launch a local web server in /bin folder. For example, node http-server which serves on localhost:8080. <p>Navigate the web browser to the kernel page you want to test, like http://localhost:8080/perf/imgproc/cvtcolor.html.</p> <p>You can input the parameter, and then click the <code>Run</code> button to run the specific case, or it will run all the cases.</p>"},{"location":"opencv_docs/opencv/modules/objc/doc/","title":"Index","text":""},{"location":"opencv_docs/opencv/modules/objc/doc/#about","title":"About","text":"<p>This is the documentation for the Objective-C/Swift OpenCV wrapper</p> <p>To get started: add the OpenCV framework to your project and add the following to your source</p>"},{"location":"opencv_docs/opencv/modules/objc/doc/#objective-c","title":"Objective-C","text":"<pre><code>#import &lt;OpenCV/OpenCV.h&gt;\n</code></pre>"},{"location":"opencv_docs/opencv/modules/objc/doc/#swift","title":"Swift","text":"<pre><code>import OpenCV\n</code></pre>"},{"location":"opencv_docs/opencv/platforms/android/aar-template/","title":"Index","text":""},{"location":"opencv_docs/opencv/platforms/android/aar-template/#scripts-for-creating-an-aar-package-and-a-local-maven-repository-with-opencv-libraries-for-android","title":"Scripts for creating an AAR package and a local Maven repository with OpenCV libraries for Android","text":""},{"location":"opencv_docs/opencv/platforms/android/aar-template/#how-to-run-the-scripts","title":"How to run the scripts","text":"<ol> <li>Set JAVA_HOME and ANDROID_HOME environment variables. For example: <pre><code>export JAVA_HOME=~/Android Studio/jbr\nexport ANDROID_HOME=~/Android/SDK\n</code></pre></li> <li>Download OpenCV SDK for Android</li> <li>Run build script for version with Java and a shared C++ library: <pre><code>python build_java_shared_aar.py \"~/opencv-4.7.0-android-sdk/OpenCV-android-sdk\"\n</code></pre></li> <li>Run build script for version with static C++ libraries: <pre><code>python build_static_aar.py \"~/opencv-4.7.0-android-sdk/OpenCV-android-sdk\"\n</code></pre> The AAR libraries and the local Maven repository will be created in the outputs directory</li> </ol>"},{"location":"opencv_docs/opencv/platforms/android/aar-template/#technical-details","title":"Technical details","text":"<p>The scripts consist of 5 steps: 1. Preparing Android AAR library project template 2. Adding Java code to the project. Adding C++ public headers for shared version to the project. 3. Compiling the project to build an AAR package 4. Adding C++ binary libraries to the AAR package. Adding C++ public headers for static version to the AAR package. 5. Creating Maven repository with the AAR package</p> <p>There are a few minor limitations: 1. Due to the AAR design the Java + shared C++ AAR package contains duplicates of C++ binary libraries, but the final user's Android application contains only one library instance. 2. The compile definitions from cmake configs are skipped, but it shouldn't affect the library because the script uses precompiled C++ binaries from SDK.</p>"},{"location":"opencv_docs/opencv/platforms/apple/readme/","title":"Building for Apple Platforms","text":"<p>build_xcframework.py creates an xcframework supporting a variety of Apple platforms.</p> <p>You'll need the following to run these steps: - MacOS 10.15 or later - Python 3.6 or later - CMake 3.18.5/3.19.0 or later (make sure the <code>cmake</code> command is available on your PATH) - Xcode 12.2 or later (and its command line tools)</p> <p>You can then run build_xcframework.py, as below: <pre><code>cd ~/&lt;my_working_directory&gt;\npython opencv/platforms/apple/build_xcframework.py --out ./build_xcframework\n</code></pre></p> <p>Grab a coffee, because you'll be here for a while. By default this builds OpenCV for 8 architectures across 4 platforms:</p> <ul> <li>iOS (<code>--iphoneos_archs</code>): arm64, armv7</li> <li>iOS Simulator (<code>--iphonesimulator_archs</code>): x86_64, arm64</li> <li>macOS (<code>--macos_archs</code>): x86_64, arm64</li> <li>Mac Catalyst (<code>--catalyst_archs</code>): x86_64, arm64</li> </ul> <p>If everything's fine, you will eventually get <code>opencv2.xcframework</code> in the output directory.</p> <p>The script has some configuration options to exclude platforms and architectures you don't want to build for. Use the <code>--help</code> flag for more information.</p>"},{"location":"opencv_docs/opencv/platforms/apple/readme/#how-it-works","title":"How it Works","text":"<p>This script generates a fat <code>.framework</code> for each platform you specify, and stitches them together into a <code>.xcframework</code>. This file can be used to support the same architecture on different platforms, which fat <code>.framework</code>s don't allow. To build the intermediate <code>.framework</code>s, <code>build_xcframework.py</code> leverages the <code>build_framework.py</code> scripts in the ios and osx platform folders.</p>"},{"location":"opencv_docs/opencv/platforms/apple/readme/#passthrough-arguments","title":"Passthrough Arguments","text":"<p>Any arguments that aren't recognized by <code>build_xcframework.py</code> will be passed to the platform-specific <code>build_framework.py</code> scripts. The <code>--without</code> flag mentioned in the examples is an example of this in action. For more info, see the <code>--help</code> info for those scripts.</p>"},{"location":"opencv_docs/opencv/platforms/apple/readme/#examples","title":"Examples","text":"<p>You may override the defaults by specifying a value for any of the <code>*_archs</code> flags. For example, if you want to build for arm64 on every platform, you can do this:</p> <pre><code>python build_xcframework.py --out somedir --iphoneos_archs arm64 --iphonesimulator_archs arm64 --macos_archs arm64 --catalyst_archs arm64\n</code></pre> <p>If you want to build only for certain platforms, you can supply the <code>--build_only_specified_archs</code> flag, which makes the script build only the archs you directly ask for. For example, to build only for Catalyst, you can do this:</p> <pre><code>python build_xcframework.py --out somedir --catalyst_archs x86_64,arm64 --build_only_specified_archs\n</code></pre> <p>You can also build without OpenCV functionality you don't need. You can do this by using the <code>--without</code> flag, which you use once per item you want to go without. For example, if you wanted to compile without <code>video</code> or <code>objc</code>, you'd can do this:</p> <p><pre><code>python build_xcframework.py --out somedir --without video --without objc\n</code></pre> (if you have issues with this, try using <code>=</code>, e.g. <code>--without=video --without=objc</code>, and file an issue on GitHub.)</p>"},{"location":"opencv_docs/opencv/platforms/js/","title":"Building OpenCV.js by Emscripten","text":"<p>Download and install Emscripten.</p> <p>Execute <code>build_js.py</code> script: <pre><code>emcmake python &lt;opencv_src_dir&gt;/platforms/js/build_js.py &lt;build_dir&gt;\n</code></pre></p> <p>If everything is fine, a few minutes later you will get <code>&lt;build_dir&gt;/bin/opencv.js</code>. You can add this into your web pages.</p> <p>Find out more build options by <code>-h</code> switch.</p> <p>For detailed build tutorial, check out <code>&lt;opencv_src_dir&gt;/doc/js_tutorials/js_setup/js_setup/js_setup.markdown</code>.</p>"},{"location":"opencv_docs/opencv/platforms/maven/","title":"Using Maven to build OpenCV","text":"<p>This page describes the how to build OpenCV using Apache Maven. The Maven build is simply a wrapper around the existing CMake process but has the additional aims of creating Java OSGi-compatible bundles with included native support and also allow the build to be carried out on RaspberryPi (ARM) architecture. There is nothing preventing using the POM on x86 Linux however.</p> <p>The following assumes building on Debian-based Linux platform.</p>"},{"location":"opencv_docs/opencv/platforms/maven/#1-overview","title":"1 - Overview","text":"<p>The Maven build process aims to:   1. Provide a simpler route to build OpenCV and Java bundles.   2. Automatically check the required native dependencies.   3. Make the Java libraries OSGi compatible.   4. Include the native OpenCV native library inside the Java bundle.   5. Integration testing of the bundle within an OSGi environment.   6. Allow the build to function on x86, x86_64 or amd architectures, Debian-based Linux platform.</p>"},{"location":"opencv_docs/opencv/platforms/maven/#2-preparing-the-build-environment","title":"2 - Preparing The Build environment","text":"<p>To build using the Maven build process both <code>Maven</code> and and up-to-date <code>JDK</code> (Java Development Kit) need to be installed. If you know you already have these installed then continue to <code>Environment Variable</code> otherwise the easiest solution is to install them using the aptitude package manager:</p> <p><code>sudo aptitude install maven default-jdk</code></p> <p>Note that installing via <code>aptitude</code> you are unlikely to get the latest version of Maven or JDK although if you are not developing Java code this shouldn't matter for this build process.</p>"},{"location":"opencv_docs/opencv/platforms/maven/#3-starting-the-build","title":"3 - Starting the build","text":""},{"location":"opencv_docs/opencv/platforms/maven/#31-environment-variables","title":"3.1 - Environment variables","text":"<p>Applicability: All processors.</p> <p>The following environment variables must be set otherwise the build will fail and halt:</p> <ul> <li><code>$JAVA_HOME</code> (the absolute path to the JDK root directory)</li> <li><code>$ANT_HOME</code> (the absolute path to the Ant root directory)</li> </ul> <p>It is recommended that advantage is taken of multiple processor cores to reduce build time. This can be done by setting a MAKEFLAGS environment variable specifying the number of parallel builds e.g.:</p> <ul> <li><code>$MAKEFLAGS=\"-j8\"</code></li> </ul> <p>However if this flag is not set the build will NOT fail. On a RaspberryPi 2 typical build times are 5 hours with <code>-j1</code> (which is the default if <code>$MAKEFLAGS</code> is not specified) and a little over 2 hours with <code>-j4</code>.</p> <p>All of the above environment variables can be set on an ad-hoc basis using 'export'.</p>"},{"location":"opencv_docs/opencv/platforms/maven/#32-build-directory","title":"3.2 - Build Directory","text":"<p>Applicability: All processors</p> <p>By default the following build directories are created.</p> <p><code>&lt;OpenCV_root_dir&gt;/build</code></p> <p><code>&lt;OpenCV_root_dir&gt;/build/maven/opencv/target</code></p> <p><code>&lt;OpenCV_root_dir&gt;/build/maven/opencv-it/target</code></p> <p>Under <code>build</code> are the standard OpenCV artifacts. Under <code>build/maven/opencv/target</code> can be found the OSGi compatible Java bundle. When deploying the bundle into an OSGi framework e.g. Apache Karaf, loading of the native library is automatically taken care of. An integration testing module is created under the <code>opencv-it</code> directory and is only of use during the build but is disabled by fault. The standard Java library as created by the CMake process is also available as specified in the existing OpenCV documentation.</p> <p>The Maven build is initiated from the directory contain the <code>pom.xml</code> file.</p>"},{"location":"opencv_docs/opencv/platforms/maven/#33-x86-or-x86_64-architecture","title":"3.3 - x86 or x86_64 Architecture:","text":"<p>Generally all that is required is the standard Maven command:</p> <p><code>mvn clean install</code></p> <p>One of the first things the build will do is check the required native dependencies. The Maven build indicates the status of the required dependencies and will fail at this point if any are missing. Install using the package manager e.g. aptitude or apt-get, and restart the build with the above command.</p> <p>Once the build successfully completes the OSGi compatible artifacts are available as described above in 'Build Directory'.</p>"},{"location":"opencv_docs/opencv/platforms/maven/#34-arm-32-bit-architecture-raspbian-distribution","title":"3.4 - ARM 32-bit Architecture - Raspbian Distribution","text":"<p>Similar to the x86 architecture the native dependencies are first checked so install any that are missing, however at the time of writing there are no official <code>libtbb2</code> and <code>libtbb-dev</code> packages in Raspbian. Version 4.4.3 of Intel's Thread Building Blocks library are available here as a Raspbian-compatible Debian packages.</p> <p>PLEASE NOTE THESE ARE NOT OFFICIAL RASPBIAN PACKAGES. INSTALL AT YOUR OWN RISK.</p> <p>The build can be started with the following command:</p> <p><code>mvn clean install</code></p> <p>Upon a successful build the libraries will be available as described above in 'Build Directory'.</p>"},{"location":"opencv_docs/opencv/platforms/maven/#35-cmake","title":"3.5 CMake","text":"<p>Applicability: x86 processors</p> <p>The CMake Maven plugin is configured to use the native CMake package (recommended) i.e. it will NOT download the latest CMake binary. Should you require CMake download then include the following Maven commandline switch when building:</p> <p><code>-Ddownload.cmake=true</code></p> <p>#### 3.6 Integration Tests  Applicability: All processors</p> <p>OSGi integration tests can be run as part of the build by including the following commandline switch to Maven:</p> <p><code>-Pintegration</code></p>"},{"location":"opencv_docs/opencv/platforms/maven/#40-maintainer-notes","title":"4.0 Maintainer Notes","text":"<p>This section is relevant to those maintaining the Maven platform build. If you just want to build the library then you do not need to refer to this section.</p>"},{"location":"opencv_docs/opencv/platforms/maven/#41-updating-pom-version-to-match-core-version","title":"4.1 Updating POM Version to Match Core Version","text":"<p>Maven requires the version to be hard-coded in the POM or in otherwords it cannot be changed at runtime. When the core C/C++ code version changes it is easy to forget to update the Maven version. The POM utilises the enforcer plugin to ensure the POM and Core versions match causing the build to fail if they do not.</p> <p>Should the POM version require updating then this can be done utilising the Maven 'versions' plugin and this will apply the correct version to all POMs within the project. Execute the following Maven command from the root directory of the Maven project:</p> <p><code>mvn versions:set -DnewVersion=$(. ./opencv/scripts/functions &amp;&amp; cd ./opencv/scripts &amp;&amp; extract_version &amp;&amp; echo $REPLY)</code></p>"},{"location":"opencv_docs/opencv/platforms/wince/readme/","title":"Building OpenCV from Source for Windows Embedded Compact (WINCE/WEC)","text":""},{"location":"opencv_docs/opencv/platforms/wince/readme/#requirements","title":"Requirements","text":"<p>CMake 3.1.0 or higher Windows Embedded Compact SDK</p>"},{"location":"opencv_docs/opencv/platforms/wince/readme/#configuring","title":"Configuring","text":"<p>To configure CMake for Windows Embedded, specify Visual Studio 2013 as generator and the name of your installed SDK:</p> <p><code>cmake -G \"Visual Studio 12 2013\" -A \"MySDK WEC2013\" -DCMAKE_TOOLCHAIN_FILE:FILEPATH=../platforms/wince/arm-wince.toolchain.cmake</code></p> <p>If you are building for a headless WINCE, specify <code>-DBUILD_HEADLESS=ON</code> when configuring. This will remove the <code>commctrl.lib</code> dependency.</p> <p>If you are building for anything else than WINCE800, you need to specify that in the configuration step. Example:</p> <pre><code>-DCMAKE_SYSTEM_VERSION=7.0 -DCMAKE_GENERATOR_TOOLSET=CE700 -DCMAKE_SYSTEM_PROCESSOR=arm-v4\n</code></pre> <p>For headless WEC2013, this configuration may not be limited to but is known to work:</p> <pre><code>-DBUILD_EXAMPLES=OFF `\n-DBUILD_opencv_apps=OFF `\n-DBUILD_opencv_calib3d=OFF `\n-DBUILD_opencv_highgui=OFF `\n-DBUILD_opencv_features2d=OFF `\n-DBUILD_opencv_flann=OFF `\n-DBUILD_opencv_ml=OFF `\n-DBUILD_opencv_objdetect=OFF `\n-DBUILD_opencv_photo=OFF `\n-DBUILD_opencv_shape=OFF `\n-DBUILD_opencv_stitching=OFF `\n-DBUILD_opencv_superres=OFF `\n-DBUILD_opencv_ts=OFF `\n-DBUILD_opencv_video=OFF `\n-DBUILD_opencv_videoio=OFF `\n-DBUILD_opencv_videostab=OFF `\n-DBUILD_opencv_dnn=OFF `\n-DBUILD_opencv_java=OFF `\n-DBUILD_opencv_python2=OFF `\n-DBUILD_opencv_python3=OFF `\n-DBUILD_opencv_java_bindings_generator=OFF `\n-DBUILD_opencv_python_bindings_generator=OFF `\n-DBUILD_TIFF=OFF `\n-DCV_TRACE=OFF `\n-DWITH_OPENCL=OFF `\n-DHAVE_OPENCL=OFF `\n-DWITH_QT=OFF `\n-DWITH_GTK=OFF `\n-DWITH_QUIRC=OFF `\n-DWITH_JASPER=OFF `\n-DWITH_WEBP=OFF `\n-DWITH_PROTOBUF=OFF `\n-DBUILD_SHARED_LIBS=OFF `\n-DWITH_OPENEXR=OFF `\n-DWITH_TIFF=OFF `\n</code></pre>"},{"location":"opencv_docs/opencv/platforms/wince/readme/#configuring-to-build-as-shared","title":"Configuring to build as shared","text":"<p>Building OpenCV as shared libraries is as easy as appending <pre><code>-DBUILD_SHARED_LIBS=ON `\n-DBUILD_ZLIB=ON\n</code></pre> to the build configuration.</p>"},{"location":"opencv_docs/opencv/platforms/wince/readme/#building","title":"Building","text":"<p>You are required to build using Unicode: <code>cmake --build . -- /p:CharacterSet=Unicode</code></p>"},{"location":"opencv_docs/opencv/samples/dnn/","title":"OpenCV deep learning module samples","text":""},{"location":"opencv_docs/opencv/samples/dnn/#model-zoo","title":"Model Zoo","text":"<p>Check a wiki for a list of tested models.</p> <p>If OpenCV is built with Intel's Inference Engine support you can use Intel's pre-trained models.</p> <p>There are different preprocessing parameters such mean subtraction or scale factors for different models. You may check the most popular models and their parameters at models.yml configuration file. It might be also used for aliasing samples parameters. In example,</p> <pre><code>python object_detection.py opencv_fd --model /path/to/caffemodel --config /path/to/prototxt\n</code></pre> <p>Check <code>-h</code> option to know which values are used by default:</p> <pre><code>python object_detection.py opencv_fd -h\n</code></pre>"},{"location":"opencv_docs/opencv/samples/dnn/#sample-models","title":"Sample models","text":"<p>You can download sample models using <code>download_models.py</code>. For example, the following command will download network weights for OpenCV Face Detector model and store them in FaceDetector folder:</p> <pre><code>python download_models.py --save_dir FaceDetector opencv_fd\n</code></pre> <p>You can use default configuration files adopted for OpenCV from here.</p> <p>You also can use the script to download necessary files from your code. Assume you have the following code inside <code>your_script.py</code>:</p> <pre><code>from download_models import downloadFile\n\nfilepath1 = downloadFile(\"https://drive.google.com/uc?export=download&amp;id=0B3gersZ2cHIxRm5PMWRoTkdHdHc\", None, filename=\"MobileNetSSD_deploy.caffemodel\", save_dir=\"save_dir_1\")\nfilepath2 = downloadFile(\"https://drive.google.com/uc?export=download&amp;id=0B3gersZ2cHIxRm5PMWRoTkdHdHc\", \"994d30a8afaa9e754d17d2373b2d62a7dfbaaf7a\", filename=\"MobileNetSSD_deploy.caffemodel\")\nprint(filepath1)\nprint(filepath2)\n# Your code\n</code></pre> <p>By running the following commands, you will get MobileNetSSD_deploy.caffemodel file: <pre><code>export OPENCV_DOWNLOAD_DATA_PATH=download_folder\npython your_script.py\n</code></pre></p> <p>Note that you can provide a directory using save_dir parameter or via OPENCV_SAVE_DIR environment variable.</p>"},{"location":"opencv_docs/opencv/samples/dnn/#face-detection","title":"Face detection","text":"<p>An origin model with single precision floating point weights has been quantized using TensorFlow framework. To achieve the best accuracy run the model on BGR images resized to <code>300x300</code> applying mean subtraction of values <code>(104, 177, 123)</code> for each blue, green and red channels correspondingly.</p> <p>The following are accuracy metrics obtained using COCO object detection evaluation tool on FDDB dataset (see script) applying resize to <code>300x300</code> and keeping an origin images' sizes. <pre><code>AP - Average Precision                            | FP32/FP16 | UINT8          | FP32/FP16 | UINT8          |\nAR - Average Recall                               | 300x300   | 300x300        | any size  | any size       |\n--------------------------------------------------|-----------|----------------|-----------|----------------|\nAP @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] | 0.408     | 0.408          | 0.378     | 0.328 (-0.050) |\nAP @[ IoU=0.50      | area=   all | maxDets=100 ] | 0.849     | 0.849          | 0.797     | 0.790 (-0.007) |\nAP @[ IoU=0.75      | area=   all | maxDets=100 ] | 0.251     | 0.251          | 0.208     | 0.140 (-0.068) |\nAP @[ IoU=0.50:0.95 | area= small | maxDets=100 ] | 0.050     | 0.051 (+0.001) | 0.107     | 0.070 (-0.037) |\nAP @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] | 0.381     | 0.379 (-0.002) | 0.380     | 0.368 (-0.012) |\nAP @[ IoU=0.50:0.95 | area= large | maxDets=100 ] | 0.455     | 0.455          | 0.412     | 0.337 (-0.075) |\nAR @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] | 0.299     | 0.299          | 0.279     | 0.246 (-0.033) |\nAR @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] | 0.482     | 0.482          | 0.476     | 0.436 (-0.040) |\nAR @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] | 0.496     | 0.496          | 0.491     | 0.451 (-0.040) |\nAR @[ IoU=0.50:0.95 | area= small | maxDets=100 ] | 0.189     | 0.193 (+0.004) | 0.284     | 0.232 (-0.052) |\nAR @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] | 0.481     | 0.480 (-0.001) | 0.470     | 0.458 (-0.012) |\nAR @[ IoU=0.50:0.95 | area= large | maxDets=100 ] | 0.528     | 0.528          | 0.520     | 0.462 (-0.058) |\n</code></pre></p>"},{"location":"opencv_docs/opencv/samples/dnn/#references","title":"References","text":"<ul> <li>Models downloading script</li> <li>Configuration files adopted for OpenCV</li> <li>How to import models from TensorFlow Object Detection API</li> <li>Names of classes from different datasets</li> </ul>"},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/","title":"Run PaddlePaddle model using OpenCV","text":"<p>These two demonstrations show how to inference PaddlePaddle model using OpenCV.</p>"},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#environment-setup","title":"Environment Setup","text":"<pre><code>pip install paddlepaddle-gpu\npip install paddlehub\npip install paddle2onnx\n</code></pre>"},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#1-run-paddlepaddle-resnet50-using-opencv","title":"1. Run PaddlePaddle ResNet50 using OpenCV","text":""},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#run-paddlepaddle-model-demo","title":"Run PaddlePaddle model demo","text":"<p>Run the code sample as follows:</p> <pre><code>python paddle_resnet50.py\n</code></pre> <p>There are three parts to the process:</p> <ol> <li>Export PaddlePaddle ResNet50 model to onnx format.</li> <li>Use <code>cv2.dnn.readNetFromONNX</code> to load the model file.</li> <li>Preprocess image file and do the inference.</li> </ol>"},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#2-run-paddleseg-portrait-segmentation-using-opencv","title":"2. Run PaddleSeg Portrait Segmentation using OpenCV","text":""},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#convert-to-onnx-model","title":"Convert to ONNX Model","text":""},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#1-get-paddle-inference-model","title":"1. Get Paddle Inference model","text":"<p>For more details, please refer to PaddleSeg.</p> <pre><code>wget https://x2paddle.bj.bcebos.com/inference/models/humanseg_hrnet18_small_v1.zip\nunzip humanseg_hrnet18_small_v1.zip\n</code></pre> <p>Notes:</p> <ul> <li>The exported model must have a fixed input shape, as dynamic is not supported at this moment.</li> </ul>"},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#2-convert-to-onnx-model-using-paddle2onnx","title":"2. Convert to ONNX model using paddle2onnx","text":"<p>To convert the model, use the following command:</p> <pre><code>paddle2onnx --model_dir humanseg_hrnet18_small_v1 \\\n            --model_filename model.pdmodel \\\n            --params_filename model.pdiparams \\\n            --opset_version 11 \\\n            --save_file humanseg_hrnet18_tiny.onnx\n</code></pre> <p>The converted model can be found in the current directory by the name <code>humanseg_hrnet18_tiny.onnx</code> .</p>"},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#run-paddleseg-portrait-segmentation-demo","title":"Run PaddleSeg Portrait Segmentation demo","text":"<p>Run the code sample as follows:</p> <pre><code>python paddle_humanseg.py\n</code></pre> <p>There are three parts to the process:</p> <ol> <li>Use <code>cv2.dnn.readNetFromONNX</code> to load the model file.</li> <li>Preprocess image file and do inference.</li> <li>Postprocess image file and visualize.</li> </ol> <p>The resulting file can be found at <code>data/result_test_human.jpg</code> .</p>"},{"location":"opencv_docs/opencv/samples/dnn/dnn_model_runner/dnn_conversion/paddlepaddle/#portrait-segmentation-visualization","title":"Portrait segmentation visualization","text":""},{"location":"opencv_docs/opencv/samples/hal/","title":"Custom HAL samples","text":"<p>Samples in this folder are intended to demonstrate functionality replacement mechanism in the OpenCV library.</p> <p>The c_hal is the example of pure C replacement library with all functions returning error. It can be used to verify error handling in the function switching code.</p> <p>The slow_hal contains naive C++ implementations of the element-wise logical array operations (and, or, xor, not) making them twice slower than the default.</p>"},{"location":"opencv_docs/opencv/samples/hal/#build-custom-hal-replacement-library","title":"Build custom HAL replacement library","text":"<ol> <li>Create folder for build (for example <code>&lt;home-dir&gt;/my-hal-build</code>)</li> <li>Go to the created folder and run cmake: <code>cmake &lt;opencv-src&gt;/samples/hal/slow_hal</code></li> <li>Run make</li> </ol> <p>After build you will find static library in the build folder: <code>libslow_hal.a</code></p>"},{"location":"opencv_docs/opencv/samples/hal/#build-opencv-with-hal-replacement","title":"Build OpenCV with HAL replacement","text":"<ol> <li>Create folder for build (for example <code>&lt;home-dir&gt;/my-opencv-build</code>)</li> <li>Go to the created folder and run cmake:     <pre><code>cmake \\\n    -DOpenCV_HAL_DIR=\"&lt;home-dir&gt;/my-hal-build/\" \\\n    &lt;opencv-src&gt;\n</code></pre></li> <li>Run make (or <code>make opencv_perf_core</code> to build the demonstration test executable only)</li> <li>After build you can run the tests and verify that some functions works slower:     <pre><code>./bin/opencv_perf_core --gtest_filter=*bitwise_and*\n</code></pre></li> </ol>"},{"location":"opencv_docs/opencv/samples/semihosting/","title":"Arm semihosting","text":"<p>This folder contain a toolchain file and a couple of examples for building OpenCV based applications that can run in an Arm semihosting setup.</p> <p>OpenCV can be compiled to target a semihosting platform as follows:</p> <pre><code>cmake ../opencv/ \\\n    -DCMAKE_TOOLCHAIN_FILE=../opencv/platforms/semihosting/aarch64-semihosting.toolchain.cmake \\\n    -DSEMIHOSTING_TOOLCHAIN_PATH=/path/to/baremetal-toolchain/bin/ \\\n    -DBUILD_EXAMPLES=ON -GNinja\n</code></pre> <p>A barematel toolchain for targeting aarch64 semihosting can be found here, under <code>aarch64-none-elf</code>.</p> <p>The code of the examples in the <code>norm</code> and <code>histogram</code> folders can be executed with qemu in Linux userspace:</p> <pre><code>    qemu-aarch64 ./bin/example_semihosting_histogram\n    qemu-aarch64 ./bin/example_semihosting_norm\n</code></pre>"},{"location":"source_docs/","title":"source checking","text":""},{"location":"source_docs/testcases/tests/","title":"Tests","text":"<p>1) testcase 1 2) testcase 2</p>"}]}